module &__llvm_hsail_module:1:0:$full:$large:$near;

decl function &amp_barrier()(arg_u32 %n);

prog kernel &ZZ17Stencil_Hcc_Shfl8RN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__819__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	align(4) spill_u8 %__spillStack[944];
	// BB#0:
	laneid_u32	$s1;
	cvt_s64_s32	$d0, $s1;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s0, $d1;
	shr_u64	$d0, $d0, 32;
	cvt_u32_u64	$s2, $d0;
	shr_s32	$s2, $s2, 2;
	add_u32	$s5, $s2, $s0;
	shr_s32	$s2, $s1, 3;
	workitemabsid_u32	$s0, 1;
	shl_u32	$s0, $s0, 3;
	and_b32	$s0, $s0, -64;
	add_u32	$s29, $s2, $s0;
	and_b32	$s6, $s29, -8;
	ld_kernarg_align(4)_width(all)_u32	$s24, [%__arg_p9];
	shl_u32	$s4, $s24, 1;
	ld_kernarg_align(4)_width(all)_u32	$s26, [%__arg_p11];
	ld_kernarg_align(4)_width(all)_u32	$s27, [%__arg_p10];
	add_u32	$s7, $s5, $s6;
	add_u32	$s3, $s7, 64;
	add_u32	$s0, $s4, $s27;
	add_u32	$s8, $s4, $s26;
	workitemabsid_u32	$s16, 0;
	mul_u32	$s4, $s5, 10;
	and_b32	$s9, $s16, -8;
	sub_u32	$s4, $s1, $s4;
	add_u32	$s4, $s4, $s9;
	cmp_lt_b1_s32	$c0, $s3, $s8;
	cmp_lt_b1_s32	$c1, $s4, $s0;
	and_b1	$c0, $c1, $c0;
	add_u32	$s8, $s1, 2;
	cvt_s64_s32	$d0, $s8;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	add_u32	$s10, $s1, 6;
	cvt_s64_s32	$d3, $s10;
	cvt_u32_u64	$s11, $d1;
	shr_u64	$d0, $d0, 32;
	cvt_u32_u64	$s12, $d0;
	shr_s32	$s12, $s12, 2;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	add_u32	$s11, $s12, $s11;
	mul_u64	$d3, $d3, 0x66666667;
	add_u32	$s12, $s6, $s11;
	shr_u64	$d4, $d3, 32;
	add_u32	$s13, $s1, 8;
	cvt_s64_s32	$d5, $s13;
	cvt_u32_u64	$s14, $d4;
	shr_u64	$d3, $d3, 63;
	add_u32	$s15, $s1, 4;
	cvt_s64_s32	$d4, $s15;
	add_u32	$s17, $s12, 51;
	mul_u64	$d4, $d4, 0x66666667;
	cvt_u32_u64	$s18, $d3;
	add_u32	$s5, $s6, $s5;
	shr_s32	$s14, $s14, 2;
	mul_u64	$d3, $d5, 0x66666667;
	shr_u64	$d5, $d3, 63;
	cvt_u32_u64	$s19, $d5;
	shr_u64	$d3, $d3, 32;
	cvt_u32_u64	$s20, $d3;
	shr_s32	$s20, $s20, 2;
	add_u32	$s19, $s20, $s19;
	add_u32	$s14, $s14, $s18;
	or_b32	$s18, $s6, 6;
	shr_u64	$d3, $d4, 63;
	mul_u32	$s5, $s5, $s0;
	add_u32	$s5, $s5, $s4;
	mul_u32	$s17, $s17, $s0;
	add_u32	$s20, $s6, $s14;
	add_u32	$s21, $s6, $s19;
	mul_u32	$s11, $s11, 10;
	cvt_u32_u64	$s22, $d3;
	shr_u64	$d3, $d4, 32;
	cvt_u32_u64	$s23, $d3;
	shr_s32	$s23, $s23, 2;
	add_u32	$s22, $s23, $s22;
	add_u32	$s23, $s21, 12;
	add_u32	$s18, $s22, $s18;
	add_u32	$s25, $s20, 57;
	sub_u32	$s8, $s8, $s11;
	add_u32	$s8, $s8, $s9;
	add_u32	$s11, $s12, 19;
	add_u32	$s12, $s17, $s8;
	cvt_s64_s32	$d3, $s5;
	add_u32	$s5, $s21, 44;
	mul_u32	$s17, $s25, $s0;
	mul_u32	$s14, $s14, 10;
	sub_u32	$s10, $s10, $s14;
	add_u32	$s10, $s10, $s9;
	add_u32	$s14, $s17, $s10;
	mul_u32	$s17, $s18, $s0;
	mul_u32	$s18, $s23, $s0;
	mul_u32	$s11, $s11, $s0;
	mul_u32	$s21, $s22, 10;
	sub_u32	$s15, $s15, $s21;
	add_u32	$s15, $s15, $s9;
	mul_u32	$s19, $s19, 10;
	sub_u32	$s13, $s13, $s19;
	add_u32	$s9, $s13, $s9;
	add_u32	$s8, $s11, $s8;
	add_u32	$s11, $s18, $s9;
	add_u32	$s13, $s17, $s15;
	cvt_s64_s32	$d4, $s12;
	cvt_s64_s32	$d5, $s14;
	add_u32	$s7, $s7, 32;
	mul_u32	$s7, $s7, $s0;
	add_u32	$s7, $s7, $s4;
	cvt_s64_s32	$d6, $s7;
	shl_u64	$d6, $d6, 2;
	shl_u64	$d3, $d3, 2;
	shl_u64	$d5, $d5, 2;
	shl_u64	$d4, $d4, 2;
	cvt_s64_s32	$d7, $s13;
	cvt_s64_s32	$d8, $s11;
	cvt_s64_s32	$d9, $s8;
	add_u32	$s7, $s20, 25;
	mul_u32	$s7, $s7, $s0;
	add_u32	$s7, $s7, $s10;
	cvt_s64_s32	$d10, $s7;
	add_u32	$s6, $s6, $s22;
	shl_u64	$d10, $d10, 2;
	shl_u64	$d9, $d9, 2;
	shl_u64	$d8, $d8, 2;
	shl_u64	$d7, $d7, 2;
	add_u64	$d4, $d2, $d4;
	add_u64	$d5, $d2, $d5;
	add_u64	$d3, $d2, $d3;
	add_u64	$d6, $d2, $d6;
	add_u32	$s6, $s6, 38;
	mul_u32	$s6, $s6, $s0;
	add_u32	$s6, $s6, $s15;
	cvt_s64_s32	$d11, $s6;
	shl_u64	$d11, $d11, 2;
	add_u64	$d11, $d2, $d11;
	mul_u32	$s6, $s5, $s0;
	ld_global_align(4)_u32	$s5, [$d6];
	ld_global_align(4)_u32	$s10, [$d3];
	ld_global_align(4)_u32	$s35, [$d5];
	ld_global_align(4)_u32	$s36, [$d4];
	add_u64	$d3, $d2, $d7;
	add_u64	$d4, $d2, $d8;
	add_u64	$d5, $d2, $d9;
	add_u64	$d6, $d2, $d10;
	add_u32	$s6, $s6, $s9;
	cvt_s64_s32	$d7, $s6;
	shl_u64	$d7, $d7, 2;
	add_u64	$d7, $d2, $d7;
	ld_global_align(4)_u32	$s44, [$d7];
	cmp_ne_b1_b1	$c0, $c0, 1;
	ld_global_align(4)_u32	$s6, [$d11];
	ld_global_align(4)_u32	$s9, [$d6];
	ld_global_align(4)_u32	$s51, [$d5];
	ld_global_align(4)_u32	$s52, [$d4];
	ld_global_align(4)_u32	$s54, [$d3];
	// implicit-def: S46
	cbr_b1	$c0, @BB0_2;
	// BB#1:
	mul_u32	$s3, $s3, $s0;
	add_u32	$s3, $s3, $s4;
	cvt_s64_s32	$d3, $s3;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s46, [$d2];

@BB0_2:
	shl_u32	$s2, $s2, 1;
	add_u32	$s4, $s2, $s1;
	and_b32	$s7, $s4, 63;
	ld_global_align(4)_f32	$s3, [$d1];
	st_spill_align(4)_u32	$s3, [%__spillStack][8];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s10, $s7, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][24];
	// 4-byte Folded Spill
	add_u32	$s8, $s1, $s2;
	add_u32	$s2, $s8, 16;
	ld_global_align(4)_f32	$s3, [$d1];
	st_spill_align(4)_u32	$s3, [%__spillStack][36];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s54, $s7, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][44];
	// 4-byte Folded Spill
	and_b32	$s11, $s2, 63;
	add_u32	$s63, $s8, 32;
	add_u32	$s3, $s8, 48;
	ld_global_align(4)_f32	$s12, [$d1];
	st_spill_align(4)_u32	$s12, [%__spillStack][16];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s54, $s11, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][684];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1];
	st_spill_align(4)_u32	$s12, [%__spillStack][28];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s52, $s11, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][688];
	// 4-byte Folded Spill
	and_b32	$s12, $s3, 63;
	and_b32	$s13, $s63, 63;
	ld_global_align(4)_f32	$s14, [$d1];
	st_spill_align(4)_u32	$s14, [%__spillStack];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s52, $s13, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][4];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1];
	st_spill_align(4)_u32	$s14, [%__spillStack][12];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s51, $s13, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][560];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1];
	st_spill_align(4)_u32	$s14, [%__spillStack][404];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s51, $s12, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][412];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1];
	st_spill_align(4)_u32	$s14, [%__spillStack][416];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s9, $s12, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][424];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1];
	st_spill_align(4)_u32	$s14, [%__spillStack][316];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s5, $s7, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][320];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1];
	st_spill_align(4)_u32	$s14, [%__spillStack][324];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s6, $s7, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][328];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1];
	st_spill_align(4)_u32	$s7, [%__spillStack][200];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s6, $s11, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][204];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1];
	st_spill_align(4)_u32	$s7, [%__spillStack][208];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s44, $s11, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][212];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1];
	st_spill_align(4)_u32	$s7, [%__spillStack][116];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s44, $s13, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][120];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1];
	st_spill_align(4)_u32	$s7, [%__spillStack][124];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s36, $s13, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][136];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1];
	st_spill_align(4)_u32	$s7, [%__spillStack][48];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s36, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][52];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1];
	st_spill_align(4)_u32	$s7, [%__spillStack][56];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s35, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][60];
	// 4-byte Folded Spill
	add_u32	$s7, $s8, 17;
	and_b32	$s7, $s7, 63;
	add_u32	$s11, $s4, 1;
	and_b32	$s11, $s11, 63;
	ld_global_align(4)_f32	$s12, [$d1+4];
	st_spill_align(4)_u32	$s12, [%__spillStack][860];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s10, $s11, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][868];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+4];
	st_spill_align(4)_u32	$s12, [%__spillStack][864];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s54, $s11, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][872];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+4];
	st_spill_align(4)_u32	$s12, [%__spillStack][716];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s54, $s7, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][732];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+4];
	st_spill_align(4)_u32	$s12, [%__spillStack][724];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s52, $s7, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][736];
	// 4-byte Folded Spill
	add_u32	$s12, $s8, 49;
	and_b32	$s12, $s12, 63;
	add_u32	$s13, $s8, 33;
	and_b32	$s13, $s13, 63;
	ld_global_align(4)_f32	$s14, [$d1+4];
	st_spill_align(4)_u32	$s14, [%__spillStack][588];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s52, $s13, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][604];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+4];
	st_spill_align(4)_u32	$s14, [%__spillStack][592];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s51, $s13, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][608];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+4];
	st_spill_align(4)_u32	$s14, [%__spillStack][480];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s51, $s12, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][496];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+4];
	st_spill_align(4)_u32	$s14, [%__spillStack][492];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s9, $s12, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][508];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+4];
	st_spill_align(4)_u32	$s14, [%__spillStack][372];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s5, $s11, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][388];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+4];
	st_spill_align(4)_u32	$s14, [%__spillStack][376];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s6, $s11, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][392];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+4];
	st_spill_align(4)_u32	$s11, [%__spillStack][252];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s6, $s7, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][260];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+4];
	st_spill_align(4)_u32	$s11, [%__spillStack][256];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s44, $s7, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][264];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+4];
	st_spill_align(4)_u32	$s7, [%__spillStack][140];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s44, $s13, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][152];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+4];
	st_spill_align(4)_u32	$s7, [%__spillStack][144];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s36, $s13, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][156];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+4];
	st_spill_align(4)_u32	$s7, [%__spillStack][64];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s36, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][72];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+4];
	st_spill_align(4)_u32	$s7, [%__spillStack][68];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s35, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][76];
	// 4-byte Folded Spill
	add_u32	$s7, $s4, 2;
	and_b32	$s7, $s7, 63;
	ld_global_align(4)_f32	$s11, [$d1+8];
	st_spill_align(4)_u32	$s11, [%__spillStack][892];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s10, $s7, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][908];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+8];
	st_spill_align(4)_u32	$s11, [%__spillStack][900];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s54, $s7, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][912];
	// 4-byte Folded Spill
	add_u32	$s11, $s8, 50;
	and_b32	$s11, $s11, 63;
	add_u32	$s12, $s8, 34;
	and_b32	$s12, $s12, 63;
	add_u32	$s13, $s8, 18;
	and_b32	$s13, $s13, 63;
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][768];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s54, $s13, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][784];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][780];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s52, $s13, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][788];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][640];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s52, $s12, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][648];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][644];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s51, $s12, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][660];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][532];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s51, $s11, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][540];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][536];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s9, $s11, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][556];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][408];
	// 4-byte Folded Spill
	activelanepermute_b32	$s14, $s5, $s7, 0, 0;
	st_spill_align(4)_u32	$s14, [%__spillStack][428];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s14, [$d1+8];
	st_spill_align(4)_u32	$s14, [%__spillStack][420];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s6, $s7, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][440];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+8];
	st_spill_align(4)_u32	$s7, [%__spillStack][292];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s6, $s13, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][300];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+8];
	st_spill_align(4)_u32	$s7, [%__spillStack][296];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s44, $s13, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][312];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+8];
	st_spill_align(4)_u32	$s7, [%__spillStack][180];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s44, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][188];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+8];
	st_spill_align(4)_u32	$s7, [%__spillStack][184];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s36, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][192];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+8];
	st_spill_align(4)_u32	$s7, [%__spillStack][80];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s36, $s11, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][88];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+8];
	st_spill_align(4)_u32	$s7, [%__spillStack][84];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s35, $s11, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][92];
	// 4-byte Folded Spill
	add_u32	$s7, $s4, 10;
	and_b32	$s12, $s7, 63;
	ld_global_align(4)_f32	$s7, [$d1+12];
	st_spill_align(4)_u32	$s7, [%__spillStack][932];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s10, $s12, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][940];
	// 4-byte Folded Spill
	add_u32	$s7, $s8, 42;
	and_b32	$s18, $s7, 63;
	add_u32	$s7, $s8, 26;
	and_b32	$s19, $s7, 63;
	ld_global_align(4)_f32	$s7, [$d1+12];
	st_spill_align(4)_u32	$s7, [%__spillStack][936];
	// 4-byte Folded Spill
	activelanepermute_b32	$s17, $s54, $s12, 0, 0;
	ld_global_align(4)_f32	$s7, [$d1+12];
	st_spill_align(4)_u32	$s7, [%__spillStack][808];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s54, $s19, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][820];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+12];
	st_spill_align(4)_u32	$s7, [%__spillStack][816];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s52, $s19, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][824];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+12];
	st_spill_align(4)_u32	$s7, [%__spillStack][680];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s52, $s18, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][704];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+12];
	st_spill_align(4)_u32	$s7, [%__spillStack][692];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s51, $s18, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][708];
	// 4-byte Folded Spill
	add_u32	$s7, $s8, 58;
	and_b32	$s20, $s7, 63;
	activelanepermute_b32	$s8, $s51, $s20, 0, 0;
	activelanepermute_b32	$s7, $s9, $s20, 0, 0;
	activelanepermute_b32	$s11, $s5, $s20, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+12];
	st_spill_align(4)_u32	$s13, [%__spillStack][472];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s5, $s12, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][500];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+12];
	st_spill_align(4)_u32	$s13, [%__spillStack][476];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s12, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][504];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][332];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][340];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][336];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][344];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][216];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s18, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][236];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][228];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s18, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][240];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][100];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s20, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][104];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][108];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s35, $s20, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][128];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+12];
	st_spill_align(4)_u32	$s12, [%__spillStack][112];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s46, $s20, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][132];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 5;
	cbr_b1	$c0, @BB0_4;
	// BB#3:
	ld_global_align(4)_f32	$s7, [$d1+12];
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s7, $s7, $s8;
	br	@BB0_5;

@BB0_4:
	ld_global_align(4)_f32	$s8, [$d1+12];
	ld_global_align(4)_f32	$s12, [$d1+12];
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s12, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s8, $s7;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c0, $s1, 56;
	cmov_b32	$s7, $c0, $s7, $s11;
	mov_b32	$s7, $s7;

@BB0_5:
	st_spill_align(4)_u32	$s7, [%__spillStack][636];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c1, $s1, 56;
	ld_global_align(4)_f32	$s23, [$d1+16];
	add_u32	$s7, $s4, 11;
	and_b32	$s12, $s7, 63;
	activelanepermute_b32	$s28, $s10, $s12, 0, 0;
	ld_global_align(4)_f32	$s25, [$d1+16];
	activelanepermute_b32	$s30, $s54, $s12, 0, 0;
	ld_global_align(4)_f32	$s7, [$d1+16];
	st_spill_align(4)_u32	$s7, [%__spillStack][884];
	// 4-byte Folded Spill
	add_u32	$s7, $s2, 11;
	add_u32	$s8, $s63, 11;
	and_b32	$s18, $s8, 63;
	and_b32	$s19, $s7, 63;
	activelanepermute_b32	$s7, $s54, $s19, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][896];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+16];
	st_spill_align(4)_u32	$s7, [%__spillStack][888];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s52, $s19, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][904];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+16];
	st_spill_align(4)_u32	$s7, [%__spillStack][756];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s52, $s18, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][772];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+16];
	st_spill_align(4)_u32	$s7, [%__spillStack][760];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s51, $s18, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][776];
	// 4-byte Folded Spill
	add_u32	$s7, $s3, 11;
	and_b32	$s20, $s7, 63;
	activelanepermute_b32	$s8, $s51, $s20, 0, 0;
	activelanepermute_b32	$s7, $s9, $s20, 0, 0;
	activelanepermute_b32	$s11, $s5, $s20, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+16];
	st_spill_align(4)_u32	$s13, [%__spillStack][572];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s5, $s12, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][580];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+16];
	st_spill_align(4)_u32	$s13, [%__spillStack][576];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s12, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][584];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][396];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][432];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][400];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][436];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][284];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s18, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][304];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][288];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s18, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][308];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][148];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s20, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][160];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][164];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s35, $s20, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][172];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+16];
	st_spill_align(4)_u32	$s12, [%__spillStack][168];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s46, $s20, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][176];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 4;
	cbr_b1	$c0, @BB0_7;
	// BB#6:
	ld_global_align(4)_f32	$s7, [$d1+16];
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s7, $s7, $s8;
	br	@BB0_8;

@BB0_7:
	ld_global_align(4)_f32	$s8, [$d1+16];
	ld_global_align(4)_f32	$s12, [$d1+16];
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s12, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s8, $s7;
	mov_b32	$s7, $s7;
	cmov_b32	$s7, $c1, $s7, $s11;
	mov_b32	$s7, $s7;

@BB0_8:
	st_spill_align(4)_u32	$s7, [%__spillStack][712];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s38, [$d1+20];
	add_u32	$s7, $s4, 12;
	and_b32	$s12, $s7, 63;
	activelanepermute_b32	$s40, $s10, $s12, 0, 0;
	ld_global_align(4)_f32	$s39, [$d1+20];
	activelanepermute_b32	$s41, $s54, $s12, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+20];
	add_u32	$s7, $s2, 12;
	add_u32	$s8, $s63, 12;
	and_b32	$s19, $s8, 63;
	and_b32	$s31, $s7, 63;
	activelanepermute_b32	$s21, $s54, $s31, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+20];
	activelanepermute_b32	$s22, $s52, $s31, 0, 0;
	ld_global_align(4)_f32	$s7, [$d1+20];
	st_spill_align(4)_u32	$s7, [%__spillStack][828];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s52, $s19, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][844];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+20];
	st_spill_align(4)_u32	$s7, [%__spillStack][832];
	// 4-byte Folded Spill
	activelanepermute_b32	$s7, $s51, $s19, 0, 0;
	st_spill_align(4)_u32	$s7, [%__spillStack][848];
	// 4-byte Folded Spill
	add_u32	$s7, $s3, 12;
	and_b32	$s32, $s7, 63;
	activelanepermute_b32	$s8, $s51, $s32, 0, 0;
	activelanepermute_b32	$s7, $s9, $s32, 0, 0;
	activelanepermute_b32	$s11, $s5, $s32, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+20];
	st_spill_align(4)_u32	$s13, [%__spillStack][652];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s5, $s12, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][664];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+20];
	st_spill_align(4)_u32	$s13, [%__spillStack][656];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s12, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][668];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][512];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s31, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][520];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][516];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s31, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][524];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][360];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][380];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][368];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][384];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][196];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s32, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][220];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][224];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s35, $s32, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][244];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+20];
	st_spill_align(4)_u32	$s12, [%__spillStack][232];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s46, $s32, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][248];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 3;
	cbr_b1	$c0, @BB0_10;
	// BB#9:
	ld_global_align(4)_f32	$s7, [$d1+20];
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s7, $s7, $s8;
	br	@BB0_11;

@BB0_10:
	ld_global_align(4)_f32	$s8, [$d1+20];
	ld_global_align(4)_f32	$s12, [$d1+20];
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s12, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s8, $s7;
	mov_b32	$s7, $s7;
	cmov_b32	$s7, $c1, $s7, $s11;
	mov_b32	$s7, $s7;

@BB0_11:
	st_spill_align(4)_u32	$s7, [%__spillStack][792];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s48, [$d1+24];
	add_u32	$s7, $s4, 20;
	and_b32	$s12, $s7, 63;
	activelanepermute_b32	$s50, $s10, $s12, 0, 0;
	ld_global_align(4)_f32	$s49, [$d1+24];
	activelanepermute_b32	$s53, $s54, $s12, 0, 0;
	ld_global_align(4)_f32	$s31, [$d1+24];
	add_u32	$s7, $s2, 20;
	and_b32	$s19, $s7, 63;
	activelanepermute_b32	$s33, $s54, $s19, 0, 0;
	ld_global_align(4)_f32	$s32, [$d1+24];
	activelanepermute_b32	$s34, $s52, $s19, 0, 0;
	add_u32	$s7, $s3, 20;
	add_u32	$s8, $s63, 20;
	and_b32	$s37, $s8, 63;
	and_b32	$s42, $s7, 63;
	activelanepermute_b32	$s8, $s52, $s37, 0, 0;
	activelanepermute_b32	$s7, $s51, $s37, 0, 0;
	activelanepermute_b32	$s11, $s9, $s37, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+24];
	st_spill_align(4)_u32	$s13, [%__spillStack][836];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s9, $s42, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][852];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+24];
	st_spill_align(4)_u32	$s13, [%__spillStack][840];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s5, $s42, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][856];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+24];
	st_spill_align(4)_u32	$s13, [%__spillStack][720];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s5, $s12, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][740];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+24];
	st_spill_align(4)_u32	$s13, [%__spillStack][728];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s12, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][744];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][596];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s6, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][612];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][600];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s19, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][620];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][452];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s37, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][460];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][464];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s36, $s37, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][484];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][468];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s35, $s37, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][488];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][268];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s35, $s42, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][276];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+24];
	st_spill_align(4)_u32	$s12, [%__spillStack][272];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s46, $s42, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][280];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 9;
	cbr_b1	$c0, @BB0_13;
	// BB#12:
	ld_global_align(4)_f32	$s7, [$d1+24];
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s19, $s7, $s8;
	br	@BB0_14;

@BB0_13:
	ld_global_align(4)_f32	$s8, [$d1+24];
	ld_global_align(4)_f32	$s12, [$d1+24];
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s12, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s8, $s7;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c0, $s1, 62;
	cmov_b32	$s7, $c0, $s7, $s11;
	mov_b32	$s19, $s7;

@BB0_14:
	ld_global_align(4)_f32	$s56, [$d1+28];
	add_u32	$s7, $s4, 21;
	and_b32	$s11, $s7, 63;
	activelanepermute_b32	$s58, $s10, $s11, 0, 0;
	ld_global_align(4)_f32	$s57, [$d1+28];
	activelanepermute_b32	$s60, $s54, $s11, 0, 0;
	ld_global_align(4)_f32	$s42, [$d1+28];
	add_u32	$s7, $s2, 21;
	and_b32	$s55, $s7, 63;
	activelanepermute_b32	$s45, $s54, $s55, 0, 0;
	ld_global_align(4)_f32	$s43, [$d1+28];
	activelanepermute_b32	$s47, $s52, $s55, 0, 0;
	add_u32	$s7, $s3, 21;
	add_u32	$s8, $s63, 21;
	and_b32	$s59, $s8, 63;
	and_b32	$s61, $s7, 63;
	activelanepermute_b32	$s8, $s52, $s59, 0, 0;
	activelanepermute_b32	$s7, $s51, $s59, 0, 0;
	activelanepermute_b32	$s37, $s9, $s59, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+28];
	st_spill_align(4)_u32	$s12, [%__spillStack][916];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s9, $s61, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][924];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+28];
	st_spill_align(4)_u32	$s12, [%__spillStack][920];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s5, $s61, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][928];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+28];
	st_spill_align(4)_u32	$s12, [%__spillStack][796];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s5, $s11, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][804];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+28];
	st_spill_align(4)_u32	$s12, [%__spillStack][800];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s6, $s11, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][812];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][672];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s6, $s55, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][696];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][676];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s44, $s55, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][700];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][528];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s44, $s59, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][544];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][548];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s36, $s59, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][564];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][552];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s35, $s59, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][568];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][348];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s35, $s61, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][356];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+28];
	st_spill_align(4)_u32	$s11, [%__spillStack][352];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s46, $s61, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][364];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 8;
	cbr_b1	$c0, @BB0_16;
	// BB#15:
	ld_global_align(4)_f32	$s7, [$d1+28];
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s37, $s7, $s8;
	br	@BB0_17;

@BB0_16:
	ld_global_align(4)_f32	$s8, [$d1+28];
	ld_global_align(4)_f32	$s11, [$d1+28];
	mov_b32	$s37, $s37;
	mul_ftz_f32	$s11, $s11, $s37;
	mov_b32	$s11, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s8, $s7;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c0, $s1, 61;
	cmov_b32	$s7, $c0, $s7, $s11;
	mov_b32	$s37, $s7;

@BB0_17:
	add_u32	$s16, $s16, $s24;
	add_u32	$s59, $s29, $s24;
	ld_global_align(4)_f32	$s61, [$d1+32];
	add_u32	$s4, $s4, 22;
	and_b32	$s12, $s4, 63;
	activelanepermute_b32	$s7, $s10, $s12, 0, 0;
	ld_global_align(4)_f32	$s62, [$d1+32];
	activelanepermute_b32	$s8, $s54, $s12, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+32];
	add_u32	$s2, $s2, 22;
	and_b32	$s13, $s2, 63;
	activelanepermute_b32	$s29, $s54, $s13, 0, 0;
	ld_global_align(4)_f32	$s54, [$d1+32];
	activelanepermute_b32	$s55, $s52, $s13, 0, 0;
	add_u32	$s2, $s3, 22;
	add_u32	$s3, $s63, 22;
	and_b32	$s14, $s3, 63;
	and_b32	$s15, $s2, 63;
	activelanepermute_b32	$s52, $s52, $s14, 0, 0;
	activelanepermute_b32	$s51, $s51, $s14, 0, 0;
	activelanepermute_b32	$s11, $s9, $s14, 0, 0;
	ld_global_align(4)_f32	$s2, [$d1+32];
	activelanepermute_b32	$s4, $s9, $s15, 0, 0;
	ld_global_align(4)_f32	$s3, [$d1+32];
	activelanepermute_b32	$s63, $s5, $s15, 0, 0;
	ld_global_align(4)_f32	$s9, [$d1+32];
	st_spill_align(4)_u32	$s9, [%__spillStack][876];
	// 4-byte Folded Spill
	activelanepermute_b32	$s9, $s5, $s12, 0, 0;
	ld_global_align(4)_f32	$s5, [$d1+32];
	st_spill_align(4)_u32	$s5, [%__spillStack][880];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s6, $s12, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+32];
	st_spill_align(4)_u32	$s12, [%__spillStack][748];
	// 4-byte Folded Spill
	activelanepermute_b32	$s6, $s6, $s13, 0, 0;
	st_spill_align(4)_u32	$s6, [%__spillStack][764];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s6, [$d1+32];
	st_spill_align(4)_u32	$s6, [%__spillStack][752];
	// 4-byte Folded Spill
	activelanepermute_b32	$s6, $s44, $s13, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+32];
	st_spill_align(4)_u32	$s12, [%__spillStack][616];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s44, $s14, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][624];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+32];
	st_spill_align(4)_u32	$s12, [%__spillStack][628];
	// 4-byte Folded Spill
	activelanepermute_b32	$s36, $s36, $s14, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+32];
	st_spill_align(4)_u32	$s12, [%__spillStack][632];
	// 4-byte Folded Spill
	activelanepermute_b32	$s44, $s35, $s14, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+32];
	st_spill_align(4)_u32	$s12, [%__spillStack][444];
	// 4-byte Folded Spill
	activelanepermute_b32	$s12, $s35, $s15, 0, 0;
	st_spill_align(4)_u32	$s12, [%__spillStack][456];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+32];
	st_spill_align(4)_u32	$s12, [%__spillStack][448];
	// 4-byte Folded Spill
	activelanepermute_b32	$s46, $s46, $s15, 0, 0;
	cmp_gt_b1_s32	$c0, $s1, 7;
	cbr_b1	$c0, @BB0_19;
	// BB#18:
	cvt_u32_b1	$s13, $c1;
	st_spill_align(4)_u32	$s13, [%__spillStack][96];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s11, [$d1+32];
	mov_b32	$s12, $s52;
	mul_ftz_f32	$s35, $s11, $s12;
	br	@BB0_20;

@BB0_19:
	cvt_u32_b1	$s14, $c1;
	st_spill_align(4)_u32	$s14, [%__spillStack][96];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s12, [$d1+32];
	ld_global_align(4)_f32	$s13, [$d1+32];
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s13, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s13, $s51;
	mul_ftz_f32	$s12, $s12, $s13;
	mov_b32	$s12, $s12;
	cmp_lt_b1_s32	$c0, $s1, 60;
	cmov_b32	$s11, $c0, $s12, $s11;
	mov_b32	$s35, $s11;

@BB0_20:
	cmp_lt_b1_s32	$c0, $s1, 52;
	cvt_u32_b1	$s12, $c0;
	st_spill_align(4)_u32	$s12, [%__spillStack][40];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c0, $s1, 51;
	cvt_u32_b1	$s12, $c0;
	st_spill_align(4)_u32	$s12, [%__spillStack][32];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c0, $s1, 50;
	cvt_u32_b1	$s12, $c0;
	st_spill_align(4)_u32	$s12, [%__spillStack][20];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c7, $s1, 44;
	cmp_lt_b1_s32	$c6, $s1, 43;
	cmp_lt_b1_s32	$c4, $s1, 42;
	cmp_lt_b1_s32	$c5, $s1, 36;
	cmp_lt_b1_s32	$c3, $s1, 35;
	cmp_lt_b1_s32	$c2, $s1, 34;
	add_u32	$s11, $s27, $s24;
	add_u32	$s24, $s26, $s24;
	cmp_lt_b1_s32	$c0, $s59, $s24;
	cmp_lt_b1_s32	$c1, $s16, $s11;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB0_22;
	// BB#21:
	ld_spill_align(4)_u32	$s11, [%__spillStack][44];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][24];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][36];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s13, $s11;
	ld_spill_align(4)_u32	$s13, [%__spillStack][8];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s13, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][872];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][868];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	mov_b32	$s12, $s12;
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s15, [%__spillStack][864];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s15, $s13;
	ld_spill_align(4)_u32	$s15, [%__spillStack][860];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s15, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack][912];
	// 4-byte Folded Reload
	mov_b32	$s15, $s15;
	ld_spill_align(4)_u32	$s26, [%__spillStack][908];
	// 4-byte Folded Reload
	mov_b32	$s26, $s26;
	ld_spill_align(4)_u32	$s51, [%__spillStack][40];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s51;
	cmov_b32	$s11, $c0, $s12, $s11;
	mov_b32	$s12, $s14;
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][900];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s14, $s15;
	ld_spill_align(4)_u32	$s15, [%__spillStack][892];
	// 4-byte Folded Reload
	mul_ftz_f32	$s15, $s15, $s26;
	ld_spill_align(4)_u32	$s26, [%__spillStack][940];
	// 4-byte Folded Reload
	mov_b32	$s26, $s26;
	mov_b32	$s17, $s17;
	ld_spill_align(4)_u32	$s51, [%__spillStack][32];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s51;
	cmov_b32	$s12, $c0, $s12, $s13;
	mov_b32	$s13, $s15;
	mov_b32	$s14, $s14;
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s15, [%__spillStack][932];
	// 4-byte Folded Reload
	mul_ftz_f32	$s15, $s15, $s26;
	ld_spill_align(4)_u32	$s26, [%__spillStack][936];
	// 4-byte Folded Reload
	mul_ftz_f32	$s17, $s26, $s17;
	mov_b32	$s26, $s28;
	mov_b32	$s27, $s30;
	ld_spill_align(4)_u32	$s28, [%__spillStack][20];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s28;
	cmov_b32	$s13, $c0, $s13, $s14;
	add_ftz_f32	$s11, $s11, 0F00000000;
	mov_b32	$s12, $s12;
	mov_b32	$s14, $s15;
	mov_b32	$s15, $s17;
	mul_ftz_f32	$s17, $s23, $s26;
	mul_ftz_f32	$s23, $s25, $s27;
	mov_b32	$s25, $s40;
	mov_b32	$s26, $s41;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s13;
	cmov_b32	$s13, $c7, $s14, $s15;
	mov_b32	$s14, $s17;
	mov_b32	$s15, $s23;
	mul_ftz_f32	$s17, $s38, $s25;
	mul_ftz_f32	$s23, $s39, $s26;
	mov_b32	$s25, $s50;
	mov_b32	$s26, $s53;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s13;
	cmov_b32	$s13, $c6, $s14, $s15;
	mov_b32	$s14, $s17;
	mov_b32	$s15, $s23;
	mul_ftz_f32	$s17, $s48, $s25;
	mul_ftz_f32	$s23, $s49, $s26;
	mov_b32	$s25, $s58;
	mov_b32	$s26, $s60;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s13;
	cmov_b32	$s13, $c4, $s14, $s15;
	mov_b32	$s14, $s17;
	mov_b32	$s15, $s23;
	mul_ftz_f32	$s17, $s56, $s25;
	mul_ftz_f32	$s23, $s57, $s26;
	mov_b32	$s7, $s7;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s13;
	cmov_b32	$s13, $c5, $s14, $s15;
	mov_b32	$s14, $s17;
	mov_b32	$s15, $s23;
	mul_ftz_f32	$s7, $s61, $s7;
	mul_ftz_f32	$s8, $s62, $s8;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s13;
	cmov_b32	$s13, $c3, $s14, $s15;
	mov_b32	$s7, $s7;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s13;
	cmov_b32	$s7, $c2, $s7, $s8;
	add_ftz_f32	$s8, $s11, $s12;
	mov_b32	$s7, $s7;
	add_ftz_f32	$s7, $s8, $s7;
	mul_u32	$s8, $s59, $s0;
	add_u32	$s8, $s8, $s16;
	cvt_s64_s32	$d1, $s8;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s7, [$d1];

@BB0_22:
	cvt_u32_b1	$s7, $c7;
	st_spill_align(4)_u32	$s7, [%__spillStack][892];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c6;
	st_spill_align(4)_u32	$s8, [%__spillStack][872];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c4;
	st_spill_align(4)_u32	$s8, [%__spillStack][868];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack][864];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c2;
	st_spill_align(4)_u32	$s8, [%__spillStack][860];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c6, $s1, 40;
	cmp_lt_b1_s32	$c0, $s1, 39;
	cvt_u32_b1	$s8, $c0;
	st_spill_align(4)_u32	$s8, [%__spillStack][44];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c0, $s1, 38;
	cvt_u32_b1	$s8, $c0;
	st_spill_align(4)_u32	$s8, [%__spillStack][36];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c2, $s1, 24;
	cmp_lt_b1_s32	$c0, $s1, 32;
	cvt_u32_b1	$s8, $c0;
	st_spill_align(4)_u32	$s8, [%__spillStack][24];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c0, $s1, 31;
	cvt_u32_b1	$s8, $c0;
	st_spill_align(4)_u32	$s8, [%__spillStack][8];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c7, $s1, 30;
	cmp_lt_b1_s32	$c4, $s1, 23;
	cmp_lt_b1_s32	$c3, $s1, 22;
	add_u32	$s7, $s59, 8;
	cmp_lt_b1_s32	$c0, $s7, $s24;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB0_24;
	// BB#23:
	ld_spill_align(4)_u32	$s8, [%__spillStack][684];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s11, [%__spillStack][688];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][16];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s12, $s8;
	ld_spill_align(4)_u32	$s12, [%__spillStack][28];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s12, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][736];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][732];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	mov_b32	$s8, $s8;
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s14, [%__spillStack][724];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s14, $s12;
	ld_spill_align(4)_u32	$s14, [%__spillStack][716];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s14, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][788];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack][784];
	// 4-byte Folded Reload
	mov_b32	$s15, $s15;
	cmov_b32	$s8, $c6, $s8, $s11;
	mov_b32	$s11, $s13;
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][780];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s14;
	ld_spill_align(4)_u32	$s14, [%__spillStack][768];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s14, $s15;
	ld_spill_align(4)_u32	$s15, [%__spillStack][824];
	// 4-byte Folded Reload
	mov_b32	$s15, $s15;
	ld_spill_align(4)_u32	$s17, [%__spillStack][820];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	ld_spill_align(4)_u32	$s25, [%__spillStack][44];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s25;
	cmov_b32	$s11, $c0, $s11, $s12;
	mov_b32	$s12, $s14;
	mov_b32	$s13, $s13;
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s14, [%__spillStack][816];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s14, $s15;
	ld_spill_align(4)_u32	$s15, [%__spillStack][808];
	// 4-byte Folded Reload
	mul_ftz_f32	$s15, $s15, $s17;
	ld_spill_align(4)_u32	$s17, [%__spillStack][896];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	ld_spill_align(4)_u32	$s23, [%__spillStack][904];
	// 4-byte Folded Reload
	mov_b32	$s23, $s23;
	ld_spill_align(4)_u32	$s25, [%__spillStack][36];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s25;
	cmov_b32	$s12, $c0, $s12, $s13;
	add_ftz_f32	$s8, $s8, 0F00000000;
	mov_b32	$s11, $s11;
	mov_b32	$s13, $s15;
	mov_b32	$s14, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack][884];
	// 4-byte Folded Reload
	mul_ftz_f32	$s15, $s15, $s17;
	ld_spill_align(4)_u32	$s17, [%__spillStack][888];
	// 4-byte Folded Reload
	mul_ftz_f32	$s17, $s17, $s23;
	mov_b32	$s21, $s21;
	mov_b32	$s22, $s22;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s11, $s12;
	ld_spill_align(4)_u32	$s23, [%__spillStack][24];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s23;
	cmov_b32	$s12, $c0, $s13, $s14;
	mov_b32	$s13, $s15;
	mov_b32	$s14, $s17;
	mul_ftz_f32	$s15, $s18, $s21;
	mul_ftz_f32	$s17, $s20, $s22;
	mov_b32	$s18, $s33;
	mov_b32	$s20, $s34;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s11, $s12;
	ld_spill_align(4)_u32	$s21, [%__spillStack][8];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s21;
	cmov_b32	$s12, $c0, $s13, $s14;
	mov_b32	$s13, $s15;
	mov_b32	$s14, $s17;
	mul_ftz_f32	$s15, $s31, $s18;
	mul_ftz_f32	$s17, $s32, $s20;
	mov_b32	$s18, $s45;
	mov_b32	$s20, $s47;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s11, $s12;
	cmov_b32	$s12, $c7, $s13, $s14;
	mov_b32	$s13, $s15;
	mov_b32	$s14, $s17;
	mul_ftz_f32	$s15, $s42, $s18;
	mul_ftz_f32	$s17, $s43, $s20;
	mov_b32	$s18, $s29;
	mov_b32	$s20, $s55;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s11, $s12;
	cmov_b32	$s12, $c2, $s13, $s14;
	mov_b32	$s13, $s15;
	mov_b32	$s14, $s17;
	mul_ftz_f32	$s10, $s10, $s18;
	mul_ftz_f32	$s15, $s54, $s20;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s11, $s12;
	cmov_b32	$s12, $c4, $s13, $s14;
	mov_b32	$s10, $s10;
	mov_b32	$s13, $s15;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s11, $s12;
	cmov_b32	$s10, $c3, $s10, $s13;
	add_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s10, $s10;
	add_ftz_f32	$s8, $s8, $s10;
	mul_u32	$s7, $s7, $s0;
	add_u32	$s7, $s7, $s16;
	cvt_s64_s32	$d1, $s7;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s8, [$d1];

@BB0_24:
	cvt_u32_b1	$s8, $c7;
	st_spill_align(4)_u32	$s8, [%__spillStack][716];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c4;
	st_spill_align(4)_u32	$s8, [%__spillStack][688];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack][684];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c0, $s1, 26;
	cmp_lt_b1_s32	$c7, $s1, 25;
	cmp_lt_b1_s32	$c3, $s1, 18;
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack][28];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c3, $s1, 17;
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack][16];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c4, $s1, 16;
	add_u32	$s7, $s59, 16;
	cmp_lt_b1_s32	$c3, $s7, $s24;
	and_b1	$c3, $c1, $c3;
	cmp_ne_b1_b1	$c3, $c3, 1;
	cbr_b1	$c3, @BB0_26;
	// BB#25:
	ld_spill_align(4)_u32	$s8, [%__spillStack][560];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s10, [%__spillStack][4];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][12];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s11, $s8;
	ld_spill_align(4)_u32	$s11, [%__spillStack];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s11, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][608];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][604];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	mov_b32	$s10, $s10;
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s13, [%__spillStack][592];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s13, $s11;
	ld_spill_align(4)_u32	$s13, [%__spillStack][588];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s13, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][660];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][648];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	cmov_b32	$s8, $c0, $s10, $s8;
	mov_b32	$s10, $s12;
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][644];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s13;
	ld_spill_align(4)_u32	$s13, [%__spillStack][640];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s14;
	ld_spill_align(4)_u32	$s14, [%__spillStack][708];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack][704];
	// 4-byte Folded Reload
	mov_b32	$s15, $s15;
	cmov_b32	$s10, $c7, $s10, $s11;
	mov_b32	$s8, $s8;
	mov_b32	$s11, $s13;
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][692];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s14;
	ld_spill_align(4)_u32	$s14, [%__spillStack][680];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s14, $s15;
	ld_spill_align(4)_u32	$s15, [%__spillStack][772];
	// 4-byte Folded Reload
	mov_b32	$s15, $s15;
	ld_spill_align(4)_u32	$s17, [%__spillStack][776];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	add_ftz_f32	$s8, $s8, 0F00000000;
	mov_b32	$s10, $s10;
	cmov_b32	$s11, $c2, $s11, $s12;
	mov_b32	$s12, $s14;
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][756];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s14, $s15;
	ld_spill_align(4)_u32	$s15, [%__spillStack][760];
	// 4-byte Folded Reload
	mul_ftz_f32	$s15, $s15, $s17;
	ld_spill_align(4)_u32	$s17, [%__spillStack][844];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	ld_spill_align(4)_u32	$s18, [%__spillStack][848];
	// 4-byte Folded Reload
	mov_b32	$s18, $s18;
	add_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s20, [%__spillStack][28];
	// 4-byte Folded Reload
	cvt_b1_u32	$c3, $s20;
	cmov_b32	$s11, $c3, $s12, $s13;
	mov_b32	$s12, $s14;
	mov_b32	$s13, $s15;
	ld_spill_align(4)_u32	$s14, [%__spillStack][828];
	// 4-byte Folded Reload
	mul_ftz_f32	$s14, $s14, $s17;
	ld_spill_align(4)_u32	$s15, [%__spillStack][832];
	// 4-byte Folded Reload
	mul_ftz_f32	$s15, $s15, $s18;
	add_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s17, [%__spillStack][16];
	// 4-byte Folded Reload
	cvt_b1_u32	$c3, $s17;
	cmov_b32	$s11, $c3, $s12, $s13;
	mov_b32	$s12, $s14;
	mov_b32	$s13, $s15;
	add_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s10, $s11;
	cmov_b32	$s11, $c4, $s12, $s13;
	add_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s10, $s11;
	add_ftz_f32	$s8, $s8, $s10;
	add_ftz_f32	$s8, $s8, $s19;
	add_ftz_f32	$s8, $s8, $s37;
	add_ftz_f32	$s8, $s8, $s35;
	mul_u32	$s7, $s7, $s0;
	add_u32	$s7, $s7, $s16;
	cvt_s64_s32	$d1, $s7;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s8, [$d1];

@BB0_26:
	cvt_u32_b1	$s8, $c4;
	st_spill_align(4)_u32	$s8, [%__spillStack][560];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c3, $s1, 14;
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack][12];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c3, $s1, 13;
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack][4];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c3, $s1, 12;
	cvt_u32_b1	$s8, $c3;
	st_spill_align(4)_u32	$s8, [%__spillStack];
	// 4-byte Folded Spill
	cmp_lt_b1_s32	$c3, $s1, 48;
	add_u32	$s7, $s59, 24;
	cmp_lt_b1_s32	$c4, $s7, $s24;
	and_b1	$c4, $c1, $c4;
	cmp_ne_b1_b1	$c4, $c4, 1;
	cbr_b1	$c4, @BB0_28;
	// BB#27:
	ld_spill_align(4)_u32	$s8, [%__spillStack][424];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s10, [%__spillStack][412];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][416];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s11, $s8;
	ld_spill_align(4)_u32	$s11, [%__spillStack][404];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s11, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][508];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][496];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	mov_b32	$s10, $s10;
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s13, [%__spillStack][492];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s13, $s11;
	ld_spill_align(4)_u32	$s13, [%__spillStack][480];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s13, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][556];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][540];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack][12];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s8, $c4, $s10, $s8;
	mov_b32	$s10, $s12;
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][536];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s13;
	ld_spill_align(4)_u32	$s13, [%__spillStack][532];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack][4];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s10, $c4, $s10, $s11;
	mov_b32	$s11, $s13;
	mov_b32	$s12, $s12;
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s15, [%__spillStack];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s11, $c4, $s11, $s12;
	add_ftz_f32	$s8, $s8, 0F00000000;
	mov_b32	$s10, $s10;
	add_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][856];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][852];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	add_ftz_f32	$s8, $s8, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][836];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s12;
	ld_spill_align(4)_u32	$s12, [%__spillStack][840];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s12, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][928];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][924];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][636];
	// 4-byte Folded Reload
	add_ftz_f32	$s8, $s8, $s14;
	mov_b32	$s11, $s11;
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s14, [%__spillStack][916];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s14, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][920];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s14, $s12;
	mov_b32	$s14, $s63;
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s15, [%__spillStack][712];
	// 4-byte Folded Reload
	add_ftz_f32	$s8, $s8, $s15;
	cmov_b32	$s10, $c3, $s10, $s11;
	mov_b32	$s11, $s12;
	mov_b32	$s12, $s13;
	mul_ftz_f32	$s2, $s2, $s4;
	mul_ftz_f32	$s3, $s3, $s14;
	ld_spill_align(4)_u32	$s4, [%__spillStack][792];
	// 4-byte Folded Reload
	add_ftz_f32	$s4, $s8, $s4;
	mov_b32	$s8, $s10;
	cmov_b32	$s10, $c3, $s12, $s11;
	mov_b32	$s3, $s3;
	mov_b32	$s2, $s2;
	add_ftz_f32	$s4, $s4, $s8;
	mov_b32	$s8, $s10;
	cmov_b32	$s2, $c3, $s2, $s3;
	add_ftz_f32	$s3, $s4, $s8;
	mov_b32	$s2, $s2;
	add_ftz_f32	$s2, $s3, $s2;
	mul_u32	$s3, $s7, $s0;
	add_u32	$s3, $s3, $s16;
	cvt_s64_s32	$d1, $s3;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s2, [$d1];

@BB0_28:
	add_u32	$s2, $s59, 32;
	cmp_lt_b1_s32	$c4, $s2, $s24;
	and_b1	$c4, $c1, $c4;
	cmp_ne_b1_b1	$c4, $c4, 1;
	cbr_b1	$c4, @BB0_30;
	// BB#29:
	ld_spill_align(4)_u32	$s3, [%__spillStack][328];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s4, [%__spillStack][320];
	// 4-byte Folded Reload
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s7, [%__spillStack][324];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s7, $s3;
	ld_spill_align(4)_u32	$s7, [%__spillStack][316];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s7, $s4;
	ld_spill_align(4)_u32	$s7, [%__spillStack][392];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][388];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	mov_b32	$s4, $s4;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s10, [%__spillStack][376];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s10, $s7;
	ld_spill_align(4)_u32	$s10, [%__spillStack][372];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s10, $s8;
	ld_spill_align(4)_u32	$s10, [%__spillStack][440];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][428];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s14, [%__spillStack][40];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s14;
	cmov_b32	$s3, $c4, $s4, $s3;
	mov_b32	$s4, $s8;
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][420];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][408];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][500];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][504];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s14, [%__spillStack][32];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s14;
	cmov_b32	$s4, $c4, $s4, $s7;
	mov_b32	$s7, $s10;
	mov_b32	$s8, $s8;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s10, [%__spillStack][472];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][476];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s12;
	ld_spill_align(4)_u32	$s12, [%__spillStack][580];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][584];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s15, [%__spillStack][20];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s7, $c4, $s7, $s8;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s4, $s4;
	mov_b32	$s8, $s11;
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][572];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s12;
	ld_spill_align(4)_u32	$s12, [%__spillStack][576];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s13;
	ld_spill_align(4)_u32	$s13, [%__spillStack][664];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][668];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	ld_spill_align(4)_u32	$s15, [%__spillStack][892];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s7, $c4, $s10, $s8;
	mov_b32	$s8, $s12;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][652];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s13;
	ld_spill_align(4)_u32	$s12, [%__spillStack][656];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s14;
	ld_spill_align(4)_u32	$s13, [%__spillStack][740];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][744];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	ld_spill_align(4)_u32	$s15, [%__spillStack][872];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s7, $c4, $s10, $s8;
	mov_b32	$s8, $s12;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][720];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s13;
	ld_spill_align(4)_u32	$s12, [%__spillStack][728];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s14;
	ld_spill_align(4)_u32	$s13, [%__spillStack][804];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][812];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	ld_spill_align(4)_u32	$s15, [%__spillStack][868];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s15;
	cmov_b32	$s7, $c4, $s10, $s8;
	mov_b32	$s8, $s12;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][796];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s13;
	ld_spill_align(4)_u32	$s12, [%__spillStack][800];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s14;
	mov_b32	$s9, $s9;
	mov_b32	$s5, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	cmov_b32	$s7, $c5, $s10, $s8;
	mov_b32	$s8, $s12;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][876];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s11, $s9;
	ld_spill_align(4)_u32	$s11, [%__spillStack][880];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s11, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	ld_spill_align(4)_u32	$s11, [%__spillStack][864];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s11;
	cmov_b32	$s7, $c4, $s10, $s8;
	mov_b32	$s5, $s5;
	mov_b32	$s8, $s9;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	ld_spill_align(4)_u32	$s7, [%__spillStack][860];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s7;
	cmov_b32	$s5, $c4, $s8, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mul_u32	$s2, $s2, $s0;
	add_u32	$s2, $s2, $s16;
	cvt_s64_s32	$d1, $s2;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s3, [$d1];

@BB0_30:
	add_u32	$s2, $s59, 40;
	cmp_lt_b1_s32	$c4, $s2, $s24;
	and_b1	$c4, $c1, $c4;
	cmp_ne_b1_b1	$c4, $c4, 1;
	cbr_b1	$c4, @BB0_32;
	// BB#31:
	ld_spill_align(4)_u32	$s3, [%__spillStack][212];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s4, [%__spillStack][204];
	// 4-byte Folded Reload
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][208];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s5, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][200];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s5, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][264];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][260];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	mov_b32	$s4, $s4;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s8, [%__spillStack][256];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s8, $s5;
	ld_spill_align(4)_u32	$s8, [%__spillStack][252];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s8, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][312];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][300];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	cmov_b32	$s3, $c6, $s4, $s3;
	mov_b32	$s4, $s7;
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][296];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][292];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][340];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][344];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s12, [%__spillStack][44];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s12;
	cmov_b32	$s4, $c4, $s4, $s5;
	mov_b32	$s5, $s8;
	mov_b32	$s7, $s7;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s8, [%__spillStack][332];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][336];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][432];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][436];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s13, [%__spillStack][36];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s13;
	cmov_b32	$s5, $c4, $s5, $s7;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s4, $s4;
	mov_b32	$s7, $s9;
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][396];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][400];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][520];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][524];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s13, [%__spillStack][24];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s13;
	cmov_b32	$s5, $c4, $s8, $s7;
	mov_b32	$s7, $s10;
	mov_b32	$s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][512];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	ld_spill_align(4)_u32	$s10, [%__spillStack][516];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s12;
	ld_spill_align(4)_u32	$s11, [%__spillStack][612];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][620];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s13, [%__spillStack][8];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s13;
	cmov_b32	$s5, $c4, $s8, $s7;
	mov_b32	$s7, $s10;
	mov_b32	$s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][596];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	ld_spill_align(4)_u32	$s10, [%__spillStack][600];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s12;
	ld_spill_align(4)_u32	$s11, [%__spillStack][696];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][700];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s13, [%__spillStack][716];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s13;
	cmov_b32	$s5, $c4, $s8, $s7;
	mov_b32	$s7, $s10;
	mov_b32	$s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][672];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	ld_spill_align(4)_u32	$s10, [%__spillStack][676];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s12;
	ld_spill_align(4)_u32	$s11, [%__spillStack][764];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	mov_b32	$s6, $s6;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c2, $s8, $s7;
	mov_b32	$s7, $s10;
	mov_b32	$s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][748];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	ld_spill_align(4)_u32	$s10, [%__spillStack][752];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s10, $s6;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s10, [%__spillStack][688];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s10;
	cmov_b32	$s5, $c4, $s8, $s7;
	mov_b32	$s6, $s6;
	mov_b32	$s7, $s9;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s8, [%__spillStack][684];
	// 4-byte Folded Reload
	cvt_b1_u32	$c4, $s8;
	cmov_b32	$s5, $c4, $s7, $s6;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mul_u32	$s2, $s2, $s0;
	add_u32	$s2, $s2, $s16;
	cvt_s64_s32	$d1, $s2;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s3, [$d1];

@BB0_32:
	add_u32	$s2, $s59, 48;
	cmp_lt_b1_s32	$c4, $s2, $s24;
	and_b1	$c4, $c1, $c4;
	cmp_ne_b1_b1	$c4, $c4, 1;
	cbr_b1	$c4, @BB0_34;
	// BB#33:
	ld_spill_align(4)_u32	$s3, [%__spillStack][136];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s4, [%__spillStack][120];
	// 4-byte Folded Reload
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][124];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s5, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][116];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s5, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][156];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][152];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	mov_b32	$s4, $s4;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s7, [%__spillStack][144];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s7, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][140];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s7, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][192];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][188];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	cmov_b32	$s3, $c0, $s4, $s3;
	mov_b32	$s4, $s6;
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][184];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s7;
	ld_spill_align(4)_u32	$s7, [%__spillStack][180];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][236];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][240];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	cmov_b32	$s4, $c7, $s4, $s5;
	mov_b32	$s3, $s3;
	mov_b32	$s5, $s7;
	mov_b32	$s6, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][216];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][228];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][304];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][308];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s4, $s4;
	cmov_b32	$s5, $c2, $s5, $s6;
	mov_b32	$s6, $s8;
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][284];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][288];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][380];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][384];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][484];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][488];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s15, [%__spillStack][28];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s15;
	cmov_b32	$s5, $c0, $s7, $s6;
	mov_b32	$s6, $s9;
	mov_b32	$s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][360];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	ld_spill_align(4)_u32	$s9, [%__spillStack][368];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	ld_spill_align(4)_u32	$s10, [%__spillStack][460];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][464];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s12;
	ld_spill_align(4)_u32	$s12, [%__spillStack][468];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s13;
	ld_spill_align(4)_u32	$s13, [%__spillStack][564];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][568];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s17, [%__spillStack][16];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s17;
	cmov_b32	$s5, $c0, $s7, $s6;
	mov_b32	$s6, $s9;
	mov_b32	$s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][452];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s9, $s12;
	mov_b32	$s10, $s11;
	cmp_lt_b1_s32	$c0, $s1, 62;
	ld_spill_align(4)_u32	$s11, [%__spillStack][544];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][548];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s13;
	ld_spill_align(4)_u32	$s13, [%__spillStack][552];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s14;
	mov_b32	$s14, $s36;
	mov_b32	$s15, $s44;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s17, [%__spillStack][560];
	// 4-byte Folded Reload
	cvt_b1_u32	$c2, $s17;
	cmov_b32	$s5, $c2, $s7, $s6;
	cmp_lt_b1_s32	$c2, $s1, 10;
	cmov_b32	$s6, $c0, $s10, $s9;
	mov_b32	$s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][528];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s9, $s13;
	mov_b32	$s10, $s12;
	cmp_lt_b1_s32	$c0, $s1, 61;
	ld_spill_align(4)_u32	$s11, [%__spillStack][624];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][628];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s14;
	ld_spill_align(4)_u32	$s13, [%__spillStack][632];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s15;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c2, $s7, $s6;
	cmp_lt_b1_s32	$c2, $s1, 9;
	cmov_b32	$s6, $c0, $s10, $s9;
	mov_b32	$s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][616];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s9, $s13;
	mov_b32	$s10, $s12;
	cmp_lt_b1_s32	$c0, $s1, 60;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c2, $s7, $s6;
	cmp_lt_b1_s32	$c2, $s1, 8;
	cmov_b32	$s6, $c0, $s10, $s9;
	mov_b32	$s7, $s8;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c2, $s7, $s6;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mul_u32	$s2, $s2, $s0;
	add_u32	$s2, $s2, $s16;
	cvt_s64_s32	$d1, $s2;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s3, [$d1];

@BB0_34:
	add_u32	$s2, $s59, 56;
	cmp_lt_b1_s32	$c0, $s2, $s24;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB0_36;
	// BB#35:
	ld_spill_align(4)_u32	$s3, [%__spillStack][60];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s4, [%__spillStack][52];
	// 4-byte Folded Reload
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][56];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s5, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][48];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s5, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][76];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][72];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	mov_b32	$s4, $s4;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s7, [%__spillStack][68];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s7, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][64];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s7, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][92];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][88];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][128];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][132];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s13, [%__spillStack][12];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s13;
	cmov_b32	$s3, $c0, $s4, $s3;
	mov_b32	$s4, $s6;
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][84];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s7;
	ld_spill_align(4)_u32	$s7, [%__spillStack][80];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][104];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s11, [%__spillStack][108];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s11, $s9;
	ld_spill_align(4)_u32	$s11, [%__spillStack][112];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s11, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][172];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][176];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s15, [%__spillStack][4];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s15;
	cmov_b32	$s4, $c0, $s4, $s5;
	mov_b32	$s5, $s7;
	mov_b32	$s6, $s6;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s7, [%__spillStack][100];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	mov_b32	$s8, $s10;
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][160];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s13, [%__spillStack][164];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s13, $s11;
	ld_spill_align(4)_u32	$s13, [%__spillStack][168];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s13, $s12;
	ld_spill_align(4)_u32	$s13, [%__spillStack][244];
	// 4-byte Folded Reload
	mov_b32	$s13, $s13;
	ld_spill_align(4)_u32	$s14, [%__spillStack][248];
	// 4-byte Folded Reload
	mov_b32	$s14, $s14;
	ld_spill_align(4)_u32	$s15, [%__spillStack];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s15;
	cmov_b32	$s5, $c0, $s5, $s6;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 6;
	ld_spill_align(4)_u32	$s15, [%__spillStack][96];
	// 4-byte Folded Reload
	cvt_b1_u32	$c1, $s15;
	cmov_b32	$s6, $c1, $s9, $s8;
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][148];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s9, $s12;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][220];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][224];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s13;
	ld_spill_align(4)_u32	$s13, [%__spillStack][232];
	// 4-byte Folded Reload
	mul_ftz_f32	$s13, $s13, $s14;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c0, $s7, $s6;
	cmp_lt_b1_s32	$c0, $s1, 5;
	cmov_b32	$s6, $c1, $s10, $s9;
	mov_b32	$s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][196];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s11;
	mov_b32	$s9, $s13;
	mov_b32	$s10, $s12;
	ld_spill_align(4)_u32	$s11, [%__spillStack][276];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][280];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c0, $s7, $s6;
	cmp_lt_b1_s32	$c0, $s1, 4;
	cmov_b32	$s1, $c1, $s10, $s9;
	mov_b32	$s6, $s8;
	ld_spill_align(4)_u32	$s7, [%__spillStack][268];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s11;
	ld_spill_align(4)_u32	$s8, [%__spillStack][272];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s12;
	ld_spill_align(4)_u32	$s9, [%__spillStack][356];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][364];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s1, $c0, $s6, $s1;
	mov_b32	$s5, $s8;
	mov_b32	$s6, $s7;
	ld_spill_align(4)_u32	$s7, [%__spillStack][348];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s9;
	ld_spill_align(4)_u32	$s8, [%__spillStack][352];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	ld_spill_align(4)_u32	$s9, [%__spillStack][456];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	mov_b32	$s10, $s46;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s1, $s1;
	cmov_b32	$s4, $c3, $s6, $s5;
	mov_b32	$s5, $s8;
	mov_b32	$s6, $s7;
	ld_spill_align(4)_u32	$s7, [%__spillStack][444];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s9;
	ld_spill_align(4)_u32	$s8, [%__spillStack][448];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	add_ftz_f32	$s1, $s3, $s1;
	mov_b32	$s3, $s4;
	cmov_b32	$s4, $c3, $s6, $s5;
	mov_b32	$s5, $s8;
	mov_b32	$s6, $s7;
	add_ftz_f32	$s1, $s1, $s3;
	mov_b32	$s3, $s4;
	cmov_b32	$s4, $c3, $s6, $s5;
	add_ftz_f32	$s1, $s1, $s3;
	mov_b32	$s3, $s4;
	add_ftz_f32	$s1, $s1, $s3;
	mul_u32	$s0, $s2, $s0;
	add_u32	$s0, $s0, $s16;
	cvt_s64_s32	$d1, $s0;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	st_global_align(4)_f32	$s1, [$d0];

@BB0_36:
	// %_ZZ17Stencil_Hcc_Shfl8RN2hc5arrayIfLi1EEES2_S2_iiiENK3$_8clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ19Stencil_Hcc_Shfl4_2RN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__719__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	align(4) spill_u8 %__spillStack[244];
	// BB#0:
	laneid_u32	$s0;
	cvt_s64_s32	$d0, $s0;
	mul_u64	$d0, $d0, 0x38e38e39;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s1, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s2, $d0;
	add_u32	$s4, $s2, $s1;
	shr_s32	$s7, $s0, 3;
	workitemabsid_u32	$s1, 1;
	shl_u32	$s1, $s1, 1;
	and_b32	$s1, $s1, -16;
	add_u32	$s5, $s7, $s1;
	and_b32	$s8, $s5, -8;
	ld_kernarg_align(4)_width(all)_u32	$s2, [%__arg_p9];
	shl_u32	$s9, $s2, 1;
	ld_kernarg_align(4)_width(all)_u32	$s59, [%__arg_p10];
	add_u32	$s10, $s8, $s4;
	add_u32	$s1, $s9, $s59;
	mul_u32	$s11, $s4, 18;
	workitemabsid_u32	$s4, 0;
	shl_u32	$s4, $s4, 1;
	and_b32	$s6, $s4, -16;
	ld_kernarg_align(4)_width(all)_u32	$s4, [%__arg_p11];
	sub_u32	$s11, $s0, $s11;
	add_u32	$s11, $s6, $s11;
	add_u32	$s9, $s9, $s4;
	cmp_lt_b1_s32	$c0, $s10, $s9;
	cmp_lt_b1_s32	$c1, $s11, $s1;
	and_b1	$c0, $c1, $c0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	cmp_ne_b1_b1	$c0, $c0, 1;
	// implicit-def: S13
	cbr_b1	$c0, @BB1_2;
	// BB#1:
	mul_u32	$s10, $s10, $s1;
	add_u32	$s10, $s10, $s11;
	cvt_s64_s32	$d3, $s10;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s13, [$d3];

@BB1_2:
	add_u32	$s10, $s0, 10;
	cvt_s64_s32	$d3, $s10;
	mul_u64	$d3, $d3, 0x38e38e39;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s11, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s12, $d3;
	add_u32	$s11, $s12, $s11;
	or_b32	$s12, $s8, 3;
	mul_u32	$s14, $s11, 18;
	sub_u32	$s10, $s10, $s14;
	add_u32	$s10, $s10, $s6;
	add_u32	$s11, $s11, $s12;
	cmp_lt_b1_s32	$c0, $s11, $s9;
	cmp_lt_b1_s32	$c1, $s10, $s1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	// implicit-def: S14
	cbr_b1	$c0, @BB1_4;
	// BB#3:
	mul_u32	$s11, $s11, $s1;
	add_u32	$s10, $s11, $s10;
	cvt_s64_s32	$d3, $s10;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s14, [$d3];

@BB1_4:
	add_u32	$s10, $s0, 2;
	cvt_s64_s32	$d3, $s10;
	mul_u64	$d3, $d3, 0x38e38e39;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s11, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s12, $d3;
	add_u32	$s11, $s12, $s11;
	or_b32	$s12, $s5, 7;
	mul_u32	$s15, $s11, 18;
	sub_u32	$s10, $s10, $s15;
	add_u32	$s10, $s10, $s6;
	add_u32	$s11, $s11, $s12;
	cmp_lt_b1_s32	$c0, $s11, $s9;
	cmp_lt_b1_s32	$c1, $s10, $s1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	// implicit-def: S15
	cbr_b1	$c0, @BB1_6;
	// BB#5:
	mul_u32	$s11, $s11, $s1;
	add_u32	$s10, $s11, $s10;
	cvt_s64_s32	$d3, $s10;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s15, [$d3];

@BB1_6:
	add_u32	$s10, $s0, 12;
	cvt_s64_s32	$d3, $s10;
	mul_u64	$d3, $d3, 0x38e38e39;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s11, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s12, $d3;
	add_u32	$s11, $s12, $s11;
	mul_u32	$s12, $s11, 18;
	sub_u32	$s10, $s10, $s12;
	add_u32	$s10, $s10, $s6;
	add_u32	$s11, $s8, $s11;
	add_u32	$s11, $s11, 10;
	cmp_lt_b1_s32	$c0, $s11, $s9;
	cmp_lt_b1_s32	$c1, $s10, $s1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	// implicit-def: S16
	cbr_b1	$c0, @BB1_8;
	// BB#7:
	mul_u32	$s11, $s11, $s1;
	add_u32	$s10, $s11, $s10;
	cvt_s64_s32	$d3, $s10;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s16, [$d3];

@BB1_8:
	add_u32	$s10, $s0, 4;
	cvt_s64_s32	$d3, $s10;
	mul_u64	$d3, $d3, 0x38e38e39;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s11, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s12, $d3;
	add_u32	$s11, $s12, $s11;
	mul_u32	$s12, $s11, 18;
	sub_u32	$s10, $s10, $s12;
	add_u32	$s10, $s10, $s6;
	add_u32	$s11, $s8, $s11;
	add_u32	$s11, $s11, 14;
	cmp_lt_b1_s32	$c0, $s11, $s9;
	cmp_lt_b1_s32	$c1, $s10, $s1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	// implicit-def: S24
	cbr_b1	$c0, @BB1_10;
	// BB#9:
	mul_u32	$s11, $s11, $s1;
	add_u32	$s10, $s11, $s10;
	cvt_s64_s32	$d3, $s10;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s24, [$d3];

@BB1_10:
	add_u32	$s10, $s0, 14;
	cvt_s64_s32	$d3, $s10;
	mul_u64	$d3, $d3, 0x38e38e39;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s11, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s12, $d3;
	add_u32	$s11, $s12, $s11;
	mul_u32	$s12, $s11, 18;
	sub_u32	$s10, $s10, $s12;
	add_u32	$s10, $s10, $s6;
	add_u32	$s8, $s8, $s11;
	add_u32	$s8, $s8, 17;
	cmp_lt_b1_s32	$c0, $s8, $s9;
	cmp_lt_b1_s32	$c1, $s10, $s1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	// implicit-def: S27
	cbr_b1	$c0, @BB1_12;
	// BB#11:
	mul_u32	$s8, $s8, $s1;
	add_u32	$s8, $s8, $s10;
	cvt_s64_s32	$d3, $s8;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s27, [$d2];

@BB1_12:
	mul_u32	$s7, $s7, 10;
	add_u32	$s40, $s7, $s0;
	and_b32	$s8, $s40, 63;
	activelanepermute_b32	$s20, $s13, $s8, 0, 0;
	activelanepermute_b32	$s19, $s14, $s8, 0, 0;
	add_u32	$s7, $s0, $s7;
	add_u32	$s41, $s7, 8;
	add_u32	$s39, $s7, 24;
	add_u32	$s42, $s7, 16;
	and_b32	$s9, $s41, 63;
	activelanepermute_b32	$s23, $s15, $s8, 0, 0;
	activelanepermute_b32	$s21, $s13, $s9, 0, 0;
	and_b32	$s11, $s42, 63;
	and_b32	$s25, $s39, 63;
	cmp_lt_b1_s32	$c0, $s0, 32;
	ld_global_align(4)_f32	$s7, [$d1];
	activelanepermute_b32	$s17, $s14, $s9, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1];
	activelanepermute_b32	$s18, $s15, $s9, 0, 0;
	activelanepermute_b32	$s22, $s15, $s11, 0, 0;
	ld_global_align(4)_f32	$s8, [$d1];
	activelanepermute_b32	$s10, $s16, $s11, 0, 0;
	ld_global_align(4)_f32	$s9, [$d1];
	activelanepermute_b32	$s11, $s24, $s11, 0, 0;
	ld_global_align(4)_f32	$s50, [$d1];
	activelanepermute_b32	$s51, $s15, $s25, 0, 0;
	ld_global_align(4)_f32	$s52, [$d1];
	activelanepermute_b32	$s28, $s16, $s25, 0, 0;
	ld_global_align(4)_f32	$s26, [$d1];
	activelanepermute_b32	$s25, $s24, $s25, 0, 0;
	not_b1	$c1, $c0;
	cbr_b1	$c1, @BB1_13;
	// BB#14:
	ld_global_align(4)_f32	$s19, [$d1];
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s19, $s19, $s20;
	add_ftz_f32	$s54, $s19, 0F00000000;
	cmp_ge_b1_s32	$c1, $s0, 26;
	cbr_b1	$c1, @BB1_15;
	// BB#17:
	ld_global_align(4)_f32	$s7, [$d1];
	mov_b32	$s12, $s21;
	mul_ftz_f32	$s7, $s7, $s12;
	add_ftz_f32	$s7, $s7, 0F00000000;
	cmp_ge_b1_s32	$c1, $s0, 24;
	cbr_b1	$c1, @BB1_16;
	// BB#18:
	ld_global_align(4)_f32	$s8, [$d1];
	mov_b32	$s9, $s22;
	mul_ftz_f32	$s53, $s8, $s9;
	mov_b1	$c1, 1;
	br	@BB1_19;

@BB1_13:
	// %.thread
	ld_global_align(4)_f32	$s20, [$d1];
	ld_global_align(4)_f32	$s21, [$d1];
	mov_b32	$s22, $s23;
	mul_ftz_f32	$s21, $s21, $s22;
	mov_b32	$s21, $s21;
	mov_b32	$s19, $s19;
	mul_ftz_f32	$s19, $s20, $s19;
	mov_b32	$s19, $s19;
	cmp_lt_b1_s32	$c1, $s0, 58;
	cmov_b32	$s19, $c1, $s19, $s21;
	mov_b32	$s19, $s19;
	add_ftz_f32	$s54, $s19, 0F00000000;

@BB1_15:
	// %.thread.i
	mov_b32	$s18, $s18;
	mov_b32	$s17, $s17;
	mul_ftz_f32	$s7, $s7, $s17;
	mul_ftz_f32	$s12, $s12, $s18;
	mov_b32	$s12, $s12;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c1, $s0, 56;
	cmov_b32	$s7, $c1, $s7, $s12;
	mov_b32	$s7, $s7;
	add_ftz_f32	$s7, $s7, 0F00000000;

@BB1_16:
	mov_b32	$s10, $s10;
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s8, $s8, $s10;
	mul_ftz_f32	$s9, $s9, $s11;
	mov_b32	$s9, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c1, $s0, 52;
	cmov_b32	$s8, $c1, $s8, $s9;
	mov_b1	$c1, 0;
	mov_b32	$s53, $s8;

@BB1_19:
	add_u32	$s8, $s40, 1;
	and_b32	$s8, $s8, 63;
	activelanepermute_b32	$s10, $s13, $s8, 0, 0;
	activelanepermute_b32	$s9, $s14, $s8, 0, 0;
	activelanepermute_b32	$s11, $s15, $s8, 0, 0;
	add_u32	$s8, $s41, 1;
	cmp_ne_b1_b1	$c2, $c0, 1;
	ld_global_align(4)_f32	$s12, [$d1+4];
	and_b32	$s8, $s8, 63;
	add_u32	$s17, $s39, 1;
	add_u32	$s18, $s42, 1;
	and_b32	$s18, $s18, 63;
	and_b32	$s19, $s17, 63;
	activelanepermute_b32	$s17, $s13, $s8, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+4];
	activelanepermute_b32	$s21, $s14, $s8, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+4];
	activelanepermute_b32	$s23, $s15, $s8, 0, 0;
	mov_b32	$s8, $s17;
	ld_global_align(4)_f32	$s55, [$d1+4];
	mul_ftz_f32	$s8, $s12, $s8;
	mov_b32	$s17, $s23;
	activelanepermute_b32	$s56, $s15, $s18, 0, 0;
	ld_global_align(4)_f32	$s57, [$d1+4];
	activelanepermute_b32	$s3, $s16, $s18, 0, 0;
	mov_b32	$s12, $s21;
	mul_ftz_f32	$s12, $s20, $s12;
	ld_global_align(4)_f32	$s58, [$d1+4];
	mul_ftz_f32	$s17, $s22, $s17;
	activelanepermute_b32	$s29, $s24, $s18, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+4];
	activelanepermute_b32	$s20, $s15, $s19, 0, 0;
	ld_global_align(4)_f32	$s21, [$d1+4];
	activelanepermute_b32	$s23, $s16, $s19, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+4];
	activelanepermute_b32	$s19, $s24, $s19, 0, 0;
	cbr_b1	$c2, @BB1_21;
	// BB#20:
	st_spill_align(4)_u32	$s29, [%__spillStack][64];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s3, [%__spillStack][60];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s19, [%__spillStack][36];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][32];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][28];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][24];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][20];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][16];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s25, [%__spillStack][12];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s28, [%__spillStack][8];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s26, [%__spillStack][4];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s9, [$d1+4];
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s3, $s9, $s10;
	br	@BB1_22;

@BB1_21:
	st_spill_align(4)_u32	$s29, [%__spillStack][64];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s3, [%__spillStack][60];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s19, [%__spillStack][36];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][32];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][28];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][24];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][20];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][16];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s25, [%__spillStack][12];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s28, [%__spillStack][8];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s26, [%__spillStack][4];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s10, [$d1+4];
	ld_global_align(4)_f32	$s18, [$d1+4];
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s18, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s9, $s10, $s9;
	mov_b32	$s9, $s9;
	cmp_lt_b1_s32	$c3, $s0, 57;
	cmov_b32	$s9, $c3, $s9, $s11;
	mov_b32	$s3, $s9;

@BB1_22:
	st_spill_align(4)_u32	$s3, [%__spillStack][92];
	// 4-byte Folded Spill
	mov_b32	$s9, $s17;
	mov_b32	$s10, $s12;
	cmp_lt_b1_s32	$c3, $s0, 56;
	cmov_b32	$s9, $c3, $s10, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c4, $s0, 25;
	cmov_b32	$s8, $c4, $s8, $s9;
	add_u32	$s9, $s40, 2;
	and_b32	$s9, $s9, 63;
	activelanepermute_b32	$s28, $s13, $s9, 0, 0;
	activelanepermute_b32	$s26, $s14, $s9, 0, 0;
	activelanepermute_b32	$s29, $s15, $s9, 0, 0;
	add_u32	$s10, $s41, 2;
	add_u32	$s11, $s42, 2;
	ld_global_align(4)_f32	$s9, [$d1+8];
	and_b32	$s18, $s10, 63;
	add_u32	$s10, $s39, 2;
	and_b32	$s25, $s11, 63;
	and_b32	$s30, $s10, 63;
	activelanepermute_b32	$s12, $s13, $s18, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+8];
	activelanepermute_b32	$s17, $s14, $s18, 0, 0;
	ld_global_align(4)_f32	$s11, [$d1+8];
	activelanepermute_b32	$s21, $s15, $s18, 0, 0;
	ld_global_align(4)_f32	$s19, [$d1+8];
	activelanepermute_b32	$s22, $s15, $s25, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+8];
	activelanepermute_b32	$s23, $s16, $s25, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+8];
	activelanepermute_b32	$s25, $s24, $s25, 0, 0;
	ld_global_align(4)_f32	$s3, [$d1+8];
	activelanepermute_b32	$s31, $s15, $s30, 0, 0;
	ld_global_align(4)_f32	$s32, [$d1+8];
	activelanepermute_b32	$s34, $s16, $s30, 0, 0;
	ld_global_align(4)_f32	$s33, [$d1+8];
	activelanepermute_b32	$s30, $s24, $s30, 0, 0;
	cbr_b1	$c2, @BB1_24;
	// BB#23:
	st_spill_align(4)_u32	$s30, [%__spillStack][56];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s26, [$d1+8];
	mov_b32	$s28, $s28;
	mul_ftz_f32	$s26, $s26, $s28;
	br	@BB1_25;

@BB1_24:
	st_spill_align(4)_u32	$s30, [%__spillStack][56];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s28, [$d1+8];
	ld_global_align(4)_f32	$s30, [$d1+8];
	mov_b32	$s29, $s29;
	mul_ftz_f32	$s29, $s30, $s29;
	mov_b32	$s29, $s29;
	mov_b32	$s26, $s26;
	mul_ftz_f32	$s26, $s28, $s26;
	mov_b32	$s26, $s26;
	cmov_b32	$s26, $c3, $s26, $s29;
	mov_b32	$s26, $s26;

@BB1_25:
	st_spill_align(4)_u32	$s26, [%__spillStack][100];
	// 4-byte Folded Spill
	mov_b32	$s8, $s8;
	not_b1	$c2, $c1;
	cbr_b1	$c2, @BB1_27;
	// BB#26:
	mov_b32	$s10, $s12;
	mov_b32	$s11, $s22;
	mul_ftz_f32	$s9, $s9, $s10;
	mul_ftz_f32	$s10, $s19, $s11;
	st_spill_align(4)_u32	$s10, [%__spillStack][96];
	// 4-byte Folded Spill
	br	@BB1_28;

@BB1_27:
	mov_b32	$s9, $s21;
	mov_b32	$s12, $s17;
	mov_b32	$s17, $s23;
	mov_b32	$s19, $s25;
	mul_ftz_f32	$s10, $s10, $s12;
	mul_ftz_f32	$s9, $s11, $s9;
	mul_ftz_f32	$s11, $s18, $s17;
	mul_ftz_f32	$s12, $s20, $s19;
	mov_b32	$s9, $s9;
	mov_b32	$s10, $s10;
	cmov_b32	$s9, $c3, $s10, $s9;
	mov_b32	$s10, $s12;
	mov_b32	$s11, $s11;
	cmp_lt_b1_s32	$c2, $s0, 50;
	cmov_b32	$s10, $c2, $s11, $s10;
	mov_b32	$s10, $s10;
	st_spill_align(4)_u32	$s10, [%__spillStack][96];
	// 4-byte Folded Spill
	mov_b32	$s9, $s9;

@BB1_28:
	add_ftz_f32	$s17, $s7, $s8;
	cmp_lt_b1_s32	$c4, $s0, 18;
	add_u32	$s7, $s40, 18;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s28, $s13, $s7, 0, 0;
	activelanepermute_b32	$s26, $s14, $s7, 0, 0;
	activelanepermute_b32	$s29, $s15, $s7, 0, 0;
	add_u32	$s7, $s41, 18;
	cmp_ne_b1_b1	$c5, $c1, 1;
	ld_global_align(4)_f32	$s18, [$d1+12];
	and_b32	$s7, $s7, 63;
	add_u32	$s8, $s39, 18;
	add_u32	$s10, $s42, 18;
	and_b32	$s12, $s10, 63;
	and_b32	$s30, $s8, 63;
	activelanepermute_b32	$s21, $s13, $s7, 0, 0;
	ld_global_align(4)_f32	$s19, [$d1+12];
	activelanepermute_b32	$s8, $s14, $s7, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+12];
	activelanepermute_b32	$s25, $s15, $s7, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+12];
	activelanepermute_b32	$s23, $s15, $s12, 0, 0;
	ld_global_align(4)_f32	$s7, [$d1+12];
	activelanepermute_b32	$s11, $s16, $s12, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+12];
	activelanepermute_b32	$s12, $s24, $s12, 0, 0;
	ld_global_align(4)_f32	$s35, [$d1+12];
	activelanepermute_b32	$s36, $s15, $s30, 0, 0;
	ld_global_align(4)_f32	$s37, [$d1+12];
	activelanepermute_b32	$s44, $s16, $s30, 0, 0;
	ld_global_align(4)_f32	$s38, [$d1+12];
	activelanepermute_b32	$s30, $s24, $s30, 0, 0;
	cbr_b1	$c5, @BB1_30;
	// BB#29:
	st_spill_align(4)_u32	$s30, [%__spillStack][88];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s26, [$d1+12];
	mov_b32	$s28, $s28;
	mul_ftz_f32	$s26, $s26, $s28;
	br	@BB1_31;

@BB1_30:
	st_spill_align(4)_u32	$s30, [%__spillStack][88];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s28, [$d1+12];
	ld_global_align(4)_f32	$s30, [$d1+12];
	mov_b32	$s29, $s29;
	mul_ftz_f32	$s29, $s30, $s29;
	mov_b32	$s29, $s29;
	mov_b32	$s26, $s26;
	mul_ftz_f32	$s26, $s28, $s26;
	mov_b32	$s26, $s26;
	cmp_lt_b1_s32	$c2, $s0, 50;
	cmov_b32	$s26, $c2, $s26, $s29;
	mov_b32	$s26, $s26;

@BB1_31:
	cmp_lt_b1_s32	$c3, $s0, 48;
	add_ftz_f32	$s9, $s17, $s9;
	not_b1	$c2, $c4;
	cbr_b1	$c2, @BB1_32;
	// BB#33:
	mov_b32	$s8, $s21;
	mul_ftz_f32	$s8, $s18, $s8;
	add_ftz_f32	$s19, $s9, $s8;
	cmp_ge_b1_s32	$c2, $s0, 16;
	cbr_b1	$c2, @BB1_34;
	// BB#73:
	st_spill_align(4)_u32	$s26, [%__spillStack][208];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c4;
	st_spill_align(4)_u32	$s8, [%__spillStack][200];
	// 4-byte Folded Spill
	mov_b32	$s7, $s23;
	mul_ftz_f32	$s7, $s20, $s7;
	st_spill_align(4)_u32	$s7, [%__spillStack][124];
	// 4-byte Folded Spill
	mov_b1	$c4, 1;
	br	@BB1_36;

@BB1_32:
	// %.thread3.i
	st_spill_align(4)_u32	$s26, [%__spillStack][208];
	// 4-byte Folded Spill
	cvt_u32_b1	$s18, $c4;
	st_spill_align(4)_u32	$s18, [%__spillStack][200];
	// 4-byte Folded Spill
	mov_b32	$s17, $s25;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s19, $s8;
	mul_ftz_f32	$s17, $s22, $s17;
	mov_b32	$s17, $s17;
	mov_b32	$s8, $s8;
	cmov_b32	$s8, $c3, $s8, $s17;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s19, $s9, $s8;
	br	@BB1_35;

@BB1_34:
	st_spill_align(4)_u32	$s26, [%__spillStack][208];
	// 4-byte Folded Spill
	cvt_u32_b1	$s8, $c4;
	st_spill_align(4)_u32	$s8, [%__spillStack][200];
	// 4-byte Folded Spill

@BB1_35:
	mov_b32	$s8, $s11;
	mov_b32	$s9, $s12;
	mul_ftz_f32	$s7, $s7, $s8;
	mul_ftz_f32	$s8, $s10, $s9;
	mov_b32	$s8, $s8;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c2, $s0, 44;
	cmov_b32	$s7, $c2, $s7, $s8;
	mov_b1	$c4, 0;
	mov_b32	$s7, $s7;
	st_spill_align(4)_u32	$s7, [%__spillStack][124];
	// 4-byte Folded Spill

@BB1_36:
	add_u32	$s7, $s40, 19;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s9, $s13, $s7, 0, 0;
	activelanepermute_b32	$s8, $s14, $s7, 0, 0;
	activelanepermute_b32	$s10, $s15, $s7, 0, 0;
	add_u32	$s7, $s41, 19;
	ld_global_align(4)_f32	$s11, [$d1+16];
	and_b32	$s7, $s7, 63;
	add_u32	$s12, $s39, 19;
	add_u32	$s17, $s42, 19;
	and_b32	$s17, $s17, 63;
	and_b32	$s18, $s12, 63;
	activelanepermute_b32	$s12, $s13, $s7, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+16];
	activelanepermute_b32	$s21, $s14, $s7, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+16];
	activelanepermute_b32	$s23, $s15, $s7, 0, 0;
	mov_b32	$s7, $s12;
	ld_global_align(4)_f32	$s25, [$d1+16];
	mul_ftz_f32	$s7, $s11, $s7;
	mov_b32	$s12, $s23;
	activelanepermute_b32	$s23, $s15, $s17, 0, 0;
	ld_global_align(4)_f32	$s26, [$d1+16];
	activelanepermute_b32	$s29, $s16, $s17, 0, 0;
	mov_b32	$s11, $s21;
	mul_ftz_f32	$s11, $s20, $s11;
	ld_global_align(4)_f32	$s28, [$d1+16];
	mul_ftz_f32	$s12, $s22, $s12;
	activelanepermute_b32	$s43, $s24, $s17, 0, 0;
	ld_global_align(4)_f32	$s30, [$d1+16];
	activelanepermute_b32	$s17, $s15, $s18, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+16];
	activelanepermute_b32	$s22, $s16, $s18, 0, 0;
	ld_global_align(4)_f32	$s21, [$d1+16];
	activelanepermute_b32	$s18, $s24, $s18, 0, 0;
	cbr_b1	$c5, @BB1_38;
	// BB#37:
	st_spill_align(4)_u32	$s29, [%__spillStack][168];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s28, [%__spillStack][164];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s26, [%__spillStack][160];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][156];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s25, [%__spillStack][152];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][120];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][116];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][112];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][108];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s17, [%__spillStack][104];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s31, [%__spillStack][40];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s8, [$d1+16];
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s8, $s8, $s9;
	br	@BB1_39;

@BB1_38:
	st_spill_align(4)_u32	$s29, [%__spillStack][168];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s28, [%__spillStack][164];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s26, [%__spillStack][160];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][156];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s25, [%__spillStack][152];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][120];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][116];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][112];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][108];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s17, [%__spillStack][104];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s31, [%__spillStack][40];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s9, [$d1+16];
	ld_global_align(4)_f32	$s17, [$d1+16];
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s17, $s10;
	mov_b32	$s10, $s10;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s9, $s8;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c2, $s0, 49;
	cmov_b32	$s8, $c2, $s8, $s10;
	mov_b32	$s8, $s8;

@BB1_39:
	st_spill_align(4)_u32	$s8, [%__spillStack][176];
	// 4-byte Folded Spill
	mov_b32	$s8, $s12;
	mov_b32	$s9, $s11;
	cmov_b32	$s8, $c3, $s9, $s8;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c2, $s0, 17;
	cmov_b32	$s20, $c2, $s7, $s8;
	add_u32	$s7, $s40, 20;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s28, $s13, $s7, 0, 0;
	activelanepermute_b32	$s26, $s14, $s7, 0, 0;
	activelanepermute_b32	$s29, $s15, $s7, 0, 0;
	add_u32	$s7, $s41, 20;
	add_u32	$s8, $s42, 20;
	ld_global_align(4)_f32	$s21, [$d1+20];
	and_b32	$s10, $s7, 63;
	add_u32	$s7, $s39, 20;
	and_b32	$s25, $s8, 63;
	and_b32	$s31, $s7, 63;
	activelanepermute_b32	$s8, $s13, $s10, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+20];
	activelanepermute_b32	$s9, $s14, $s10, 0, 0;
	ld_global_align(4)_f32	$s7, [$d1+20];
	activelanepermute_b32	$s17, $s15, $s10, 0, 0;
	ld_global_align(4)_f32	$s11, [$d1+20];
	activelanepermute_b32	$s18, $s15, $s25, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+20];
	activelanepermute_b32	$s23, $s16, $s25, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+20];
	activelanepermute_b32	$s25, $s24, $s25, 0, 0;
	ld_global_align(4)_f32	$s45, [$d1+20];
	activelanepermute_b32	$s46, $s15, $s31, 0, 0;
	ld_global_align(4)_f32	$s47, [$d1+20];
	activelanepermute_b32	$s49, $s16, $s31, 0, 0;
	ld_global_align(4)_f32	$s48, [$d1+20];
	activelanepermute_b32	$s31, $s24, $s31, 0, 0;
	cbr_b1	$c5, @BB1_41;
	// BB#40:
	st_spill_align(4)_u32	$s43, [%__spillStack][172];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s31, [%__spillStack][148];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s26, [$d1+20];
	mov_b32	$s28, $s28;
	mul_ftz_f32	$s43, $s26, $s28;
	br	@BB1_42;

@BB1_41:
	st_spill_align(4)_u32	$s43, [%__spillStack][172];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s31, [%__spillStack][148];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s28, [$d1+20];
	ld_global_align(4)_f32	$s31, [$d1+20];
	mov_b32	$s29, $s29;
	mul_ftz_f32	$s29, $s31, $s29;
	mov_b32	$s29, $s29;
	mov_b32	$s26, $s26;
	mul_ftz_f32	$s26, $s28, $s26;
	mov_b32	$s26, $s26;
	cmov_b32	$s26, $c3, $s26, $s29;
	mov_b32	$s43, $s26;

@BB1_42:
	mov_b32	$s20, $s20;
	not_b1	$c2, $c4;
	cbr_b1	$c2, @BB1_44;
	// BB#43:
	st_spill_align(4)_u32	$s49, [%__spillStack][144];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s48, [%__spillStack][140];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s47, [%__spillStack][136];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s46, [%__spillStack][132];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s45, [%__spillStack][128];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s44, [%__spillStack][84];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s38, [%__spillStack][80];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s37, [%__spillStack][76];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s36, [%__spillStack][72];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s35, [%__spillStack][68];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s34, [%__spillStack][52];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s33, [%__spillStack][48];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s32, [%__spillStack][44];
	// 4-byte Folded Spill
	mov_b32	$s7, $s8;
	mov_b32	$s8, $s18;
	mul_ftz_f32	$s31, $s21, $s7;
	mul_ftz_f32	$s25, $s11, $s8;
	br	@BB1_45;

@BB1_44:
	st_spill_align(4)_u32	$s49, [%__spillStack][144];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s48, [%__spillStack][140];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s47, [%__spillStack][136];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s46, [%__spillStack][132];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s45, [%__spillStack][128];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s44, [%__spillStack][84];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s38, [%__spillStack][80];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s37, [%__spillStack][76];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s36, [%__spillStack][72];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s35, [%__spillStack][68];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s34, [%__spillStack][52];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s33, [%__spillStack][48];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s32, [%__spillStack][44];
	// 4-byte Folded Spill
	mov_b32	$s8, $s17;
	mov_b32	$s9, $s9;
	mov_b32	$s11, $s23;
	mov_b32	$s17, $s25;
	mul_ftz_f32	$s9, $s22, $s9;
	mul_ftz_f32	$s7, $s7, $s8;
	mul_ftz_f32	$s8, $s10, $s11;
	mul_ftz_f32	$s10, $s12, $s17;
	mov_b32	$s7, $s7;
	mov_b32	$s9, $s9;
	cmov_b32	$s7, $c3, $s9, $s7;
	mov_b32	$s9, $s10;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c2, $s0, 42;
	cmov_b32	$s8, $c2, $s8, $s9;
	mov_b32	$s25, $s8;
	mov_b32	$s31, $s7;

@BB1_45:
	add_ftz_f32	$s36, $s19, $s20;
	cmp_lt_b1_s32	$c5, $s0, 10;
	add_u32	$s7, $s40, 36;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s9, $s13, $s7, 0, 0;
	activelanepermute_b32	$s8, $s14, $s7, 0, 0;
	activelanepermute_b32	$s10, $s15, $s7, 0, 0;
	add_u32	$s7, $s41, 36;
	add_u32	$s11, $s42, 36;
	cmp_ne_b1_b1	$c2, $c4, 1;
	ld_global_align(4)_f32	$s37, [$d1+24];
	and_b32	$s7, $s7, 63;
	add_u32	$s12, $s39, 36;
	and_b32	$s11, $s11, 63;
	and_b32	$s12, $s12, 63;
	activelanepermute_b32	$s45, $s13, $s7, 0, 0;
	ld_global_align(4)_f32	$s44, [$d1+24];
	activelanepermute_b32	$s49, $s14, $s7, 0, 0;
	ld_global_align(4)_f32	$s46, [$d1+24];
	activelanepermute_b32	$s7, $s15, $s7, 0, 0;
	ld_global_align(4)_f32	$s38, [$d1+24];
	activelanepermute_b32	$s47, $s15, $s11, 0, 0;
	ld_global_align(4)_f32	$s32, [$d1+24];
	activelanepermute_b32	$s34, $s16, $s11, 0, 0;
	ld_global_align(4)_f32	$s33, [$d1+24];
	activelanepermute_b32	$s35, $s24, $s11, 0, 0;
	ld_global_align(4)_f32	$s17, [$d1+24];
	activelanepermute_b32	$s21, $s15, $s12, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+24];
	activelanepermute_b32	$s22, $s16, $s12, 0, 0;
	ld_global_align(4)_f32	$s19, [$d1+24];
	activelanepermute_b32	$s23, $s24, $s12, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1+24];
	activelanepermute_b32	$s12, $s27, $s12, 0, 0;
	cbr_b1	$c2, @BB1_47;
	// BB#46:
	ld_global_align(4)_f32	$s8, [$d1+24];
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s48, $s8, $s9;
	br	@BB1_48;

@BB1_47:
	ld_global_align(4)_f32	$s9, [$d1+24];
	ld_global_align(4)_f32	$s11, [$d1+24];
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s11, $s10;
	mov_b32	$s10, $s10;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s9, $s8;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c6, $s0, 42;
	cmov_b32	$s8, $c6, $s8, $s10;
	mov_b32	$s48, $s8;

@BB1_48:
	cmp_lt_b1_s32	$c6, $s0, 40;
	add_ftz_f32	$s8, $s36, $s31;
	not_b1	$c7, $c5;
	cbr_b1	$c7, @BB1_49;
	// BB#50:
	mov_b32	$s7, $s45;
	mul_ftz_f32	$s7, $s37, $s7;
	add_ftz_f32	$s46, $s8, $s7;
	mov_b32	$s37, $s25;
	cmp_ge_b1_s32	$c7, $s0, 8;
	cbr_b1	$c7, @BB1_51;
	// BB#74:
	st_spill_align(4)_u32	$s12, [%__spillStack][240];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][236];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][232];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][228];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][224];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s19, [%__spillStack][220];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][216];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s17, [%__spillStack][212];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s30, [%__spillStack][204];
	// 4-byte Folded Spill
	mov_b32	$s23, $s58;
	mov_b32	$s36, $s57;
	mov_b32	$s31, $s56;
	mov_b32	$s30, $s55;
	st_spill_align(4)_u32	$s3, [%__spillStack][192];
	// 4-byte Folded Spill
	mov_b32	$s3, $s54;
	mov_b32	$s28, $s53;
	mov_b32	$s26, $s52;
	mov_b32	$s25, $s51;
	st_spill_align(4)_u32	$s50, [%__spillStack][196];
	// 4-byte Folded Spill
	mov_b32	$s7, $s47;
	mul_ftz_f32	$s44, $s38, $s7;
	mov_b1	$c7, 1;
	br	@BB1_53;

@BB1_49:
	// %.thread6.i
	mov_b32	$s37, $s25;
	st_spill_align(4)_u32	$s12, [%__spillStack][240];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][236];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][232];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][228];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][224];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s19, [%__spillStack][220];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][216];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s17, [%__spillStack][212];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s30, [%__spillStack][204];
	// 4-byte Folded Spill
	mov_b32	$s23, $s58;
	mov_b32	$s36, $s57;
	mov_b32	$s31, $s56;
	mov_b32	$s30, $s55;
	st_spill_align(4)_u32	$s3, [%__spillStack][192];
	// 4-byte Folded Spill
	mov_b32	$s3, $s54;
	mov_b32	$s28, $s53;
	mov_b32	$s26, $s52;
	mov_b32	$s25, $s51;
	st_spill_align(4)_u32	$s50, [%__spillStack][196];
	// 4-byte Folded Spill
	mov_b32	$s7, $s7;
	mov_b32	$s9, $s49;
	mul_ftz_f32	$s9, $s44, $s9;
	mul_ftz_f32	$s7, $s46, $s7;
	mov_b32	$s7, $s7;
	mov_b32	$s9, $s9;
	cmov_b32	$s7, $c6, $s9, $s7;
	mov_b32	$s7, $s7;
	add_ftz_f32	$s46, $s8, $s7;
	br	@BB1_52;

@BB1_51:
	st_spill_align(4)_u32	$s12, [%__spillStack][240];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s23, [%__spillStack][236];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s22, [%__spillStack][232];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s21, [%__spillStack][228];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s20, [%__spillStack][224];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s19, [%__spillStack][220];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s18, [%__spillStack][216];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s17, [%__spillStack][212];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s30, [%__spillStack][204];
	// 4-byte Folded Spill
	mov_b32	$s23, $s58;
	mov_b32	$s36, $s57;
	mov_b32	$s31, $s56;
	mov_b32	$s30, $s55;
	st_spill_align(4)_u32	$s3, [%__spillStack][192];
	// 4-byte Folded Spill
	mov_b32	$s3, $s54;
	mov_b32	$s28, $s53;
	mov_b32	$s26, $s52;
	mov_b32	$s25, $s51;
	st_spill_align(4)_u32	$s50, [%__spillStack][196];
	// 4-byte Folded Spill

@BB1_52:
	mov_b32	$s7, $s34;
	mov_b32	$s8, $s35;
	mul_ftz_f32	$s7, $s32, $s7;
	mul_ftz_f32	$s8, $s33, $s8;
	mov_b32	$s8, $s8;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c7, $s0, 36;
	cmov_b32	$s7, $c7, $s7, $s8;
	mov_b1	$c7, 0;
	mov_b32	$s44, $s7;

@BB1_53:
	add_u32	$s7, $s40, 37;
	and_b32	$s9, $s7, 63;
	activelanepermute_b32	$s8, $s13, $s9, 0, 0;
	activelanepermute_b32	$s7, $s14, $s9, 0, 0;
	activelanepermute_b32	$s9, $s15, $s9, 0, 0;
	add_u32	$s10, $s41, 37;
	add_u32	$s11, $s42, 37;
	ld_global_align(4)_f32	$s53, [$d1+28];
	and_b32	$s10, $s10, 63;
	add_u32	$s12, $s39, 37;
	and_b32	$s11, $s11, 63;
	and_b32	$s12, $s12, 63;
	activelanepermute_b32	$s54, $s13, $s10, 0, 0;
	ld_global_align(4)_f32	$s55, [$d1+28];
	activelanepermute_b32	$s57, $s14, $s10, 0, 0;
	ld_global_align(4)_f32	$s56, [$d1+28];
	activelanepermute_b32	$s58, $s15, $s10, 0, 0;
	ld_global_align(4)_f32	$s45, [$d1+28];
	activelanepermute_b32	$s49, $s15, $s11, 0, 0;
	ld_global_align(4)_f32	$s47, [$d1+28];
	activelanepermute_b32	$s51, $s16, $s11, 0, 0;
	ld_global_align(4)_f32	$s50, [$d1+28];
	activelanepermute_b32	$s52, $s24, $s11, 0, 0;
	ld_global_align(4)_f32	$s34, [$d1+28];
	activelanepermute_b32	$s35, $s15, $s12, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+28];
	activelanepermute_b32	$s32, $s16, $s12, 0, 0;
	ld_global_align(4)_f32	$s11, [$d1+28];
	activelanepermute_b32	$s33, $s24, $s12, 0, 0;
	ld_global_align(4)_f32	$s17, [$d1+28];
	activelanepermute_b32	$s38, $s27, $s12, 0, 0;
	cbr_b1	$c2, @BB1_55;
	// BB#54:
	st_spill_align(4)_u32	$s17, [%__spillStack][188];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s11, [%__spillStack][184];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s10, [%__spillStack][180];
	// 4-byte Folded Spill
	cvt_u32_b1	$s9, $c0;
	st_spill_align(4)_u32	$s9, [%__spillStack];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s7, [$d1+28];
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s29, $s7, $s8;
	br	@BB1_56;

@BB1_55:
	st_spill_align(4)_u32	$s17, [%__spillStack][188];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s11, [%__spillStack][184];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s10, [%__spillStack][180];
	// 4-byte Folded Spill
	cvt_u32_b1	$s11, $c0;
	st_spill_align(4)_u32	$s11, [%__spillStack];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s8, [$d1+28];
	ld_global_align(4)_f32	$s10, [$d1+28];
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s9, $s10, $s9;
	mov_b32	$s9, $s9;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s8, $s7;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c0, $s0, 41;
	cmov_b32	$s7, $c0, $s7, $s9;
	mov_b32	$s29, $s7;

@BB1_56:
	and_b32	$s60, $s0, 7;
	add_u32	$s7, $s40, 38;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s21, $s13, $s7, 0, 0;
	activelanepermute_b32	$s20, $s14, $s7, 0, 0;
	activelanepermute_b32	$s22, $s15, $s7, 0, 0;
	add_u32	$s7, $s41, 38;
	add_u32	$s8, $s42, 38;
	ld_global_align(4)_f32	$s61, [$d1+32];
	and_b32	$s9, $s7, 63;
	add_u32	$s7, $s39, 38;
	and_b32	$s19, $s8, 63;
	and_b32	$s41, $s7, 63;
	activelanepermute_b32	$s7, $s13, $s9, 0, 0;
	ld_global_align(4)_f32	$s62, [$d1+32];
	activelanepermute_b32	$s8, $s14, $s9, 0, 0;
	ld_global_align(4)_f32	$s63, [$d1+32];
	activelanepermute_b32	$s12, $s15, $s9, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+32];
	activelanepermute_b32	$s17, $s15, $s19, 0, 0;
	ld_global_align(4)_f32	$s9, [$d1+32];
	activelanepermute_b32	$s18, $s16, $s19, 0, 0;
	ld_global_align(4)_f32	$s11, [$d1+32];
	activelanepermute_b32	$s19, $s24, $s19, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+32];
	activelanepermute_b32	$s39, $s15, $s41, 0, 0;
	ld_global_align(4)_f32	$s14, [$d1+32];
	activelanepermute_b32	$s40, $s16, $s41, 0, 0;
	ld_global_align(4)_f32	$s15, [$d1+32];
	activelanepermute_b32	$s24, $s24, $s41, 0, 0;
	ld_global_align(4)_f32	$s16, [$d1+32];
	activelanepermute_b32	$s27, $s27, $s41, 0, 0;
	cbr_b1	$c2, @BB1_58;
	// BB#57:
	ld_global_align(4)_f32	$s20, [$d1+32];
	mov_b32	$s21, $s21;
	mul_ftz_f32	$s42, $s20, $s21;
	br	@BB1_59;

@BB1_58:
	ld_global_align(4)_f32	$s21, [$d1+32];
	ld_global_align(4)_f32	$s41, [$d1+32];
	mov_b32	$s22, $s22;
	mul_ftz_f32	$s22, $s41, $s22;
	mov_b32	$s22, $s22;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s20, $s21, $s20;
	mov_b32	$s20, $s20;
	cmov_b32	$s20, $c6, $s20, $s22;
	mov_b32	$s42, $s20;

@BB1_59:
	or_b32	$s6, $s6, $s60;
	add_u32	$s41, $s5, $s2;
	not_b1	$c0, $c7;
	cbr_b1	$c0, @BB1_61;
	// BB#60:
	mov_b32	$s5, $s7;
	mov_b32	$s7, $s17;
	mul_ftz_f32	$s60, $s61, $s5;
	mul_ftz_f32	$s5, $s10, $s7;
	br	@BB1_62;

@BB1_61:
	mov_b32	$s5, $s12;
	mov_b32	$s7, $s8;
	mov_b32	$s8, $s18;
	mov_b32	$s10, $s19;
	mul_ftz_f32	$s7, $s62, $s7;
	mul_ftz_f32	$s5, $s63, $s5;
	mul_ftz_f32	$s8, $s9, $s8;
	mul_ftz_f32	$s9, $s11, $s10;
	mov_b32	$s5, $s5;
	mov_b32	$s7, $s7;
	cmov_b32	$s7, $c6, $s7, $s5;
	mov_b32	$s5, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c0, $s0, 34;
	cmov_b32	$s5, $c0, $s8, $s5;
	mov_b32	$s5, $s5;
	mov_b32	$s60, $s7;

@BB1_62:
	add_u32	$s6, $s6, $s2;
	add_u32	$s61, $s4, $s2;
	cmp_ge_b1_s32	$c0, $s41, $s61;
	cbr_b1	$c0, @BB1_67;
	// BB#63:
	add_u32	$s4, $s59, $s2;
	cmp_ge_b1_s32	$c0, $s6, $s4;
	cbr_b1	$c0, @BB1_65;
	// BB#64:
	ld_spill_align(4)_u32	$s7, [%__spillStack][92];
	// 4-byte Folded Reload
	add_ftz_f32	$s7, $s3, $s7;
	ld_spill_align(4)_u32	$s3, [%__spillStack][100];
	// 4-byte Folded Reload
	add_ftz_f32	$s7, $s7, $s3;
	ld_spill_align(4)_u32	$s3, [%__spillStack][208];
	// 4-byte Folded Reload
	add_ftz_f32	$s7, $s7, $s3;
	ld_spill_align(4)_u32	$s3, [%__spillStack][176];
	// 4-byte Folded Reload
	add_ftz_f32	$s7, $s7, $s3;
	add_ftz_f32	$s7, $s7, $s43;
	add_ftz_f32	$s7, $s7, $s48;
	add_ftz_f32	$s7, $s7, $s29;
	add_ftz_f32	$s7, $s7, $s42;
	mul_u32	$s8, $s41, $s1;
	add_u32	$s8, $s8, $s6;
	cvt_s64_s32	$d1, $s8;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s7, [$d1];

@BB1_65:
	// %.thread20.i
	add_u32	$s7, $s6, 8;
	cmp_ge_b1_s32	$c0, $s7, $s4;
	cbr_b1	$c0, @BB1_67;
	// BB#66:
	mov_b32	$s4, $s58;
	mov_b32	$s8, $s57;
	mov_b32	$s9, $s54;
	mul_ftz_f32	$s8, $s55, $s8;
	mul_ftz_f32	$s4, $s56, $s4;
	mul_ftz_f32	$s9, $s53, $s9;
	mov_b32	$s4, $s4;
	mov_b32	$s8, $s8;
	cmov_b32	$s4, $c6, $s8, $s4;
	mov_b32	$s8, $s9;
	cmp_lt_b1_s32	$c0, $s0, 9;
	cmov_b32	$s4, $c0, $s8, $s4;
	mov_b32	$s4, $s4;
	add_ftz_f32	$s4, $s46, $s4;
	add_ftz_f32	$s4, $s4, $s60;
	mul_u32	$s8, $s41, $s1;
	add_u32	$s7, $s7, $s8;
	cvt_s64_s32	$d1, $s7;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s4, [$d1];

@BB1_67:
	// %.thread21.i
	add_u32	$s4, $s41, 8;
	cmp_ge_b1_s32	$c0, $s4, $s61;
	cbr_b1	$c0, @BB1_72;
	// BB#68:
	add_u32	$s3, $s59, $s2;
	cmp_ge_b1_s32	$c0, $s6, $s3;
	cbr_b1	$c0, @BB1_70;
	// BB#69:
	ld_spill_align(4)_u32	$s2, [%__spillStack][60];
	// 4-byte Folded Reload
	mov_b32	$s2, $s2;
	ld_spill_align(4)_u32	$s7, [%__spillStack][64];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	mov_b32	$s8, $s31;
	mul_ftz_f32	$s2, $s36, $s2;
	mul_ftz_f32	$s7, $s23, $s7;
	mul_ftz_f32	$s8, $s30, $s8;
	mov_b32	$s7, $s7;
	mov_b32	$s2, $s2;
	cmp_lt_b1_s32	$c0, $s0, 51;
	cmov_b32	$s2, $c0, $s2, $s7;
	mov_b32	$s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][168];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][172];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	cmov_b32	$s2, $c1, $s7, $s2;
	ld_spill_align(4)_u32	$s7, [%__spillStack][156];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s10, [%__spillStack][160];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s10, $s8;
	ld_spill_align(4)_u32	$s10, [%__spillStack][164];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s10, $s9;
	add_ftz_f32	$s10, $s28, 0F00000000;
	mov_b32	$s2, $s2;
	ld_spill_align(4)_u32	$s11, [%__spillStack][152];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s11, $s7;
	mov_b32	$s9, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c0, $s0, 43;
	add_ftz_f32	$s2, $s10, $s2;
	cmov_b32	$s8, $c0, $s8, $s9;
	mov_b32	$s7, $s7;
	mov_b32	$s9, $s51;
	mov_b32	$s10, $s52;
	ld_spill_align(4)_u32	$s11, [%__spillStack][96];
	// 4-byte Folded Reload
	add_ftz_f32	$s2, $s2, $s11;
	cmov_b32	$s7, $c4, $s7, $s8;
	mov_b32	$s8, $s49;
	mul_ftz_f32	$s9, $s47, $s9;
	mul_ftz_f32	$s10, $s50, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][124];
	// 4-byte Folded Reload
	add_ftz_f32	$s2, $s2, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s8, $s45, $s8;
	mov_b32	$s10, $s10;
	mov_b32	$s9, $s9;
	cmp_lt_b1_s32	$c0, $s0, 35;
	add_ftz_f32	$s2, $s2, $s7;
	cmov_b32	$s7, $c0, $s9, $s10;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s2, $s2, $s37;
	cmov_b32	$s7, $c7, $s8, $s7;
	add_ftz_f32	$s2, $s2, $s44;
	mov_b32	$s7, $s7;
	add_ftz_f32	$s2, $s2, $s7;
	add_ftz_f32	$s2, $s2, $s5;
	mul_u32	$s5, $s4, $s1;
	add_u32	$s5, $s5, $s6;
	cvt_s64_s32	$d1, $s5;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s2, [$d1];

@BB1_70:
	// %.thread22.i
	add_u32	$s2, $s6, 8;
	cmp_ge_b1_s32	$c0, $s2, $s3;
	cbr_b1	$c0, @BB1_72;
	// BB#71:
	ld_spill_align(4)_u32	$s3, [%__spillStack][8];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][12];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	mov_b32	$s6, $s25;
	mul_ftz_f32	$s3, $s26, $s3;
	ld_spill_align(4)_u32	$s7, [%__spillStack][4];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s7, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][32];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][36];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][196];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s9, $s6;
	mov_b32	$s5, $s5;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s9, [%__spillStack][20];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][24];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s10, $s7;
	ld_spill_align(4)_u32	$s10, [%__spillStack][28];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s10, $s8;
	ld_spill_align(4)_u32	$s10, [%__spillStack][52];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][56];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	cmov_b32	$s3, $c3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmp_lt_b1_s32	$c0, $s0, 20;
	ld_spill_align(4)_u32	$s6, [%__spillStack][16];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s9;
	mov_b32	$s8, $s8;
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s9, [%__spillStack][40];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s12, [%__spillStack][44];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s12, $s10;
	ld_spill_align(4)_u32	$s12, [%__spillStack][48];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s12, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][84];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s17, [%__spillStack][88];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	cmov_b32	$s3, $c0, $s5, $s3;
	cmov_b32	$s5, $c3, $s7, $s8;
	mov_b32	$s6, $s6;
	cmp_lt_b1_s32	$c0, $s0, 19;
	ld_spill_align(4)_u32	$s7, [%__spillStack][192];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s9;
	mov_b32	$s8, $s11;
	mov_b32	$s9, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][72];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][76];
	// 4-byte Folded Reload
	mul_ftz_f32	$s11, $s11, $s12;
	ld_spill_align(4)_u32	$s12, [%__spillStack][80];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s17;
	ld_spill_align(4)_u32	$s17, [%__spillStack][116];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	ld_spill_align(4)_u32	$s18, [%__spillStack][120];
	// 4-byte Folded Reload
	mov_b32	$s18, $s18;
	mov_b32	$s3, $s3;
	cmov_b32	$s5, $c0, $s6, $s5;
	cmov_b32	$s6, $c3, $s9, $s8;
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][68];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	mov_b32	$s9, $s12;
	mov_b32	$s10, $s11;
	ld_spill_align(4)_u32	$s11, [%__spillStack][104];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	ld_spill_align(4)_u32	$s12, [%__spillStack][108];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s17;
	ld_spill_align(4)_u32	$s17, [%__spillStack][112];
	// 4-byte Folded Reload
	mul_ftz_f32	$s17, $s17, $s18;
	ld_spill_align(4)_u32	$s18, [%__spillStack][144];
	// 4-byte Folded Reload
	mov_b32	$s18, $s18;
	ld_spill_align(4)_u32	$s19, [%__spillStack][148];
	// 4-byte Folded Reload
	mov_b32	$s19, $s19;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s23, [%__spillStack][200];
	// 4-byte Folded Reload
	cvt_b1_u32	$c0, $s23;
	cmov_b32	$s6, $c0, $s7, $s6;
	cmov_b32	$s7, $c6, $s10, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c0, $s0, 12;
	ld_spill_align(4)_u32	$s9, [%__spillStack][204];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	mov_b32	$s10, $s17;
	mov_b32	$s11, $s12;
	ld_spill_align(4)_u32	$s12, [%__spillStack][132];
	// 4-byte Folded Reload
	mov_b32	$s12, $s12;
	ld_spill_align(4)_u32	$s17, [%__spillStack][136];
	// 4-byte Folded Reload
	mul_ftz_f32	$s17, $s17, $s18;
	ld_spill_align(4)_u32	$s18, [%__spillStack][140];
	// 4-byte Folded Reload
	mul_ftz_f32	$s18, $s18, $s19;
	ld_spill_align(4)_u32	$s19, [%__spillStack][228];
	// 4-byte Folded Reload
	mov_b32	$s19, $s19;
	ld_spill_align(4)_u32	$s20, [%__spillStack][236];
	// 4-byte Folded Reload
	mov_b32	$s20, $s20;
	ld_spill_align(4)_u32	$s21, [%__spillStack][232];
	// 4-byte Folded Reload
	mov_b32	$s21, $s21;
	ld_spill_align(4)_u32	$s22, [%__spillStack][240];
	// 4-byte Folded Reload
	mov_b32	$s22, $s22;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c0, $s8, $s7;
	cmov_b32	$s7, $c6, $s11, $s10;
	mov_b32	$s8, $s9;
	cmp_lt_b1_s32	$c0, $s0, 11;
	ld_spill_align(4)_u32	$s9, [%__spillStack][128];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s12;
	mov_b32	$s10, $s18;
	mov_b32	$s11, $s17;
	ld_spill_align(4)_u32	$s12, [%__spillStack][212];
	// 4-byte Folded Reload
	mul_ftz_f32	$s12, $s12, $s19;
	ld_spill_align(4)_u32	$s17, [%__spillStack][216];
	// 4-byte Folded Reload
	mul_ftz_f32	$s17, $s17, $s21;
	ld_spill_align(4)_u32	$s18, [%__spillStack][220];
	// 4-byte Folded Reload
	mul_ftz_f32	$s18, $s18, $s20;
	ld_spill_align(4)_u32	$s19, [%__spillStack][224];
	// 4-byte Folded Reload
	mul_ftz_f32	$s19, $s19, $s22;
	mov_b32	$s20, $s35;
	mov_b32	$s21, $s33;
	mov_b32	$s22, $s32;
	mov_b32	$s23, $s38;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c0, $s8, $s7;
	cmov_b32	$s7, $c6, $s11, $s10;
	mov_b32	$s8, $s9;
	mov_b32	$s9, $s19;
	mov_b32	$s10, $s18;
	cmp_lt_b1_s32	$c0, $s0, 62;
	mov_b32	$s11, $s17;
	mov_b32	$s12, $s12;
	cmp_lt_b1_s32	$c1, $s0, 4;
	mul_ftz_f32	$s17, $s34, $s20;
	ld_spill_align(4)_u32	$s18, [%__spillStack][180];
	// 4-byte Folded Reload
	mul_ftz_f32	$s18, $s18, $s22;
	ld_spill_align(4)_u32	$s19, [%__spillStack][184];
	// 4-byte Folded Reload
	mul_ftz_f32	$s19, $s19, $s21;
	ld_spill_align(4)_u32	$s20, [%__spillStack][188];
	// 4-byte Folded Reload
	mul_ftz_f32	$s20, $s20, $s23;
	mov_b32	$s21, $s39;
	mov_b32	$s22, $s24;
	mov_b32	$s23, $s40;
	mov_b32	$s24, $s27;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c5, $s8, $s7;
	cmov_b32	$s7, $c0, $s10, $s9;
	cmov_b32	$s8, $c1, $s12, $s11;
	mov_b32	$s9, $s20;
	mov_b32	$s10, $s19;
	cmp_lt_b1_s32	$c0, $s0, 61;
	mov_b32	$s11, $s18;
	mov_b32	$s12, $s17;
	cmp_lt_b1_s32	$c1, $s0, 3;
	mul_ftz_f32	$s13, $s13, $s21;
	mul_ftz_f32	$s14, $s14, $s23;
	mul_ftz_f32	$s15, $s15, $s22;
	mul_ftz_f32	$s16, $s16, $s24;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	ld_spill_align(4)_u32	$s17, [%__spillStack];
	// 4-byte Folded Reload
	cvt_b1_u32	$c2, $s17;
	cmov_b32	$s6, $c2, $s8, $s7;
	cmov_b32	$s7, $c0, $s10, $s9;
	cmov_b32	$s8, $c1, $s12, $s11;
	mov_b32	$s9, $s16;
	mov_b32	$s10, $s15;
	cmp_lt_b1_s32	$c0, $s0, 60;
	mov_b32	$s11, $s14;
	mov_b32	$s12, $s13;
	cmp_lt_b1_s32	$c1, $s0, 2;
	add_ftz_f32	$s0, $s3, $s5;
	mov_b32	$s3, $s6;
	cmov_b32	$s5, $c2, $s8, $s7;
	cmov_b32	$s6, $c0, $s10, $s9;
	cmov_b32	$s7, $c1, $s12, $s11;
	add_ftz_f32	$s0, $s0, $s3;
	mov_b32	$s3, $s5;
	cmov_b32	$s5, $c2, $s7, $s6;
	add_ftz_f32	$s0, $s0, $s3;
	mov_b32	$s3, $s5;
	add_ftz_f32	$s0, $s0, $s3;
	mul_u32	$s1, $s4, $s1;
	add_u32	$s1, $s1, $s2;
	cvt_s64_s32	$d1, $s1;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	st_global_align(4)_f32	$s0, [$d0];

@BB1_72:
	// %_ZZ19Stencil_Hcc_Shfl4_2RN2hc5arrayIfLi1EEES2_S2_iiiENK3$_7clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ17Stencil_Hcc_Shfl4RN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__619__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	align(4) spill_u8 %__spillStack[292];
	// BB#0:
	laneid_u32	$s1;
	cvt_s64_s32	$d0, $s1;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s0, $d1;
	shr_u64	$d0, $d0, 32;
	cvt_u32_u64	$s2, $d0;
	shr_s32	$s2, $s2, 2;
	add_u32	$s5, $s2, $s0;
	shr_s32	$s2, $s1, 3;
	workitemabsid_u32	$s0, 1;
	shl_u32	$s0, $s0, 2;
	and_b32	$s0, $s0, -32;
	add_u32	$s54, $s2, $s0;
	and_b32	$s6, $s54, -8;
	ld_kernarg_align(4)_width(all)_u32	$s33, [%__arg_p9];
	shl_u32	$s4, $s33, 1;
	ld_kernarg_align(4)_width(all)_u32	$s34, [%__arg_p11];
	ld_kernarg_align(4)_width(all)_u32	$s35, [%__arg_p10];
	add_u32	$s0, $s5, $s6;
	add_u32	$s3, $s0, 32;
	add_u32	$s0, $s4, $s35;
	add_u32	$s7, $s4, $s34;
	workitemabsid_u32	$s61, 0;
	mul_u32	$s4, $s5, 10;
	and_b32	$s8, $s61, -8;
	sub_u32	$s4, $s1, $s4;
	add_u32	$s4, $s4, $s8;
	cmp_lt_b1_s32	$c0, $s3, $s7;
	cmp_lt_b1_s32	$c1, $s4, $s0;
	and_b1	$c0, $c1, $c0;
	add_u32	$s7, $s1, 2;
	cvt_s64_s32	$d0, $s7;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	add_u32	$s9, $s1, 6;
	cvt_s64_s32	$d2, $s9;
	mul_u64	$d3, $d2, 0x66666667;
	shr_u64	$d4, $d3, 32;
	cvt_u32_u64	$s10, $d1;
	shr_u64	$d0, $d0, 32;
	cvt_u32_u64	$s11, $d0;
	shr_s32	$s11, $s11, 2;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	add_u32	$s10, $s11, $s10;
	cvt_u32_u64	$s11, $d4;
	shr_u64	$d3, $d3, 63;
	add_u32	$s12, $s6, $s10;
	cvt_u32_u64	$s13, $d3;
	shr_s32	$s11, $s11, 2;
	add_u32	$s14, $s1, 4;
	cvt_s64_s32	$d3, $s14;
	mul_u64	$d3, $d3, 0x66666667;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s15, $d4;
	add_u32	$s11, $s11, $s13;
	add_u32	$s12, $s12, 19;
	mul_u32	$s12, $s12, $s0;
	add_u32	$s13, $s6, $s11;
	mul_u32	$s10, $s10, 10;
	sub_u32	$s7, $s7, $s10;
	add_u32	$s7, $s7, $s8;
	shr_u64	$d3, $d3, 32;
	cvt_u32_u64	$s10, $d3;
	shr_s32	$s10, $s10, 2;
	add_u32	$s13, $s13, 25;
	add_u32	$s7, $s7, $s12;
	add_u32	$s10, $s10, $s15;
	or_b32	$s12, $s6, 6;
	add_u32	$s15, $s1, 8;
	cvt_s64_s32	$d3, $s15;
	mul_u64	$d3, $d3, 0x66666667;
	add_u32	$s5, $s6, $s5;
	add_u32	$s12, $s10, $s12;
	mul_u32	$s12, $s12, $s0;
	shr_u64	$d4, $d3, 63;
	cvt_s64_s32	$d5, $s7;
	mul_u32	$s7, $s13, $s0;
	mul_u32	$s11, $s11, 10;
	sub_u32	$s9, $s9, $s11;
	add_u32	$s9, $s9, $s8;
	add_u32	$s7, $s9, $s7;
	cvt_s64_s32	$d6, $s7;
	mul_u32	$s7, $s10, 10;
	sub_u32	$s7, $s14, $s7;
	add_u32	$s7, $s7, $s8;
	shl_u64	$d6, $d6, 2;
	shl_u64	$d5, $d5, 2;
	add_u32	$s7, $s7, $s12;
	cvt_u32_u64	$s9, $d4;
	shr_u64	$d3, $d3, 32;
	cvt_u32_u64	$s10, $d3;
	shr_s32	$s10, $s10, 2;
	add_u32	$s9, $s10, $s9;
	add_u32	$s6, $s6, $s9;
	add_u32	$s6, $s6, 12;
	cvt_s64_s32	$d3, $s7;
	add_u64	$d4, $d2, $d5;
	add_u64	$d5, $d2, $d6;
	mul_u32	$s5, $s5, $s0;
	add_u32	$s5, $s5, $s4;
	cvt_s64_s32	$d6, $s5;
	shl_u64	$d6, $d6, 2;
	add_u64	$d6, $d2, $d6;
	cmp_ne_b1_b1	$c0, $c0, 1;
	ld_global_align(4)_u32	$s23, [$d6];
	ld_global_align(4)_u32	$s12, [$d5];
	ld_global_align(4)_u32	$s18, [$d4];
	shl_u64	$d3, $d3, 2;
	mul_u32	$s5, $s6, $s0;
	mul_u32	$s6, $s9, 10;
	sub_u32	$s6, $s15, $s6;
	add_u32	$s6, $s6, $s8;
	add_u32	$s5, $s6, $s5;
	cvt_s64_s32	$d4, $s5;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d2, $d4;
	ld_global_align(4)_u32	$s19, [$d4];
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s29, [$d3];
	// implicit-def: S26
	cbr_b1	$c0, @BB2_2;
	// BB#1:
	mul_u32	$s3, $s3, $s0;
	add_u32	$s3, $s3, $s4;
	cvt_s64_s32	$d3, $s3;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s26, [$d2];

@BB2_2:
	shl_u32	$s2, $s2, 1;
	add_u32	$s43, $s2, $s1;
	and_b32	$s3, $s43, 63;
	ld_global_align(4)_f32	$s4, [$d1];
	st_spill_align(4)_u32	$s4, [%__spillStack][276];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s23, $s3, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][284];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1];
	st_spill_align(4)_u32	$s4, [%__spillStack][280];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s29, $s3, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][288];
	// 4-byte Folded Spill
	add_u32	$s2, $s1, $s2;
	add_u32	$s44, $s2, 16;
	add_u32	$s41, $s2, 32;
	and_b32	$s3, $s41, 63;
	and_b32	$s4, $s44, 63;
	ld_global_align(4)_f32	$s5, [$d1];
	st_spill_align(4)_u32	$s5, [%__spillStack][140];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s29, $s4, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][148];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1];
	st_spill_align(4)_u32	$s5, [%__spillStack][144];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s4, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][152];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1];
	st_spill_align(4)_u32	$s4, [%__spillStack][52];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s3, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][60];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1];
	st_spill_align(4)_u32	$s4, [%__spillStack][56];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s18, $s3, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][64];
	// 4-byte Folded Spill
	add_u32	$s49, $s2, 48;
	add_u32	$s3, $s43, 1;
	and_b32	$s3, $s3, 63;
	and_b32	$s4, $s49, 63;
	ld_global_align(4)_f32	$s5, [$d1];
	st_spill_align(4)_u32	$s5, [%__spillStack];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s18, $s4, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][8];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1];
	st_spill_align(4)_u32	$s5, [%__spillStack][4];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s12, $s4, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][12];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s20, [$d1+4];
	activelanepermute_b32	$s24, $s23, $s3, 0, 0;
	ld_global_align(4)_f32	$s21, [$d1+4];
	activelanepermute_b32	$s25, $s29, $s3, 0, 0;
	add_u32	$s3, $s2, 33;
	and_b32	$s3, $s3, 63;
	add_u32	$s4, $s2, 17;
	and_b32	$s4, $s4, 63;
	ld_global_align(4)_f32	$s5, [$d1+4];
	st_spill_align(4)_u32	$s5, [%__spillStack][164];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s29, $s4, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][180];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1+4];
	st_spill_align(4)_u32	$s5, [%__spillStack][172];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s4, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][184];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+4];
	st_spill_align(4)_u32	$s4, [%__spillStack][68];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s3, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][76];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+4];
	st_spill_align(4)_u32	$s4, [%__spillStack][72];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s18, $s3, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][80];
	// 4-byte Folded Spill
	add_u32	$s3, $s43, 2;
	add_u32	$s4, $s2, 18;
	and_b32	$s4, $s4, 63;
	and_b32	$s3, $s3, 63;
	add_u32	$s5, $s2, 49;
	and_b32	$s5, $s5, 63;
	ld_global_align(4)_f32	$s6, [$d1+4];
	st_spill_align(4)_u32	$s6, [%__spillStack][16];
	// 4-byte Folded Spill
	activelanepermute_b32	$s6, $s18, $s5, 0, 0;
	st_spill_align(4)_u32	$s6, [%__spillStack][24];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s6, [$d1+4];
	st_spill_align(4)_u32	$s6, [%__spillStack][20];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s12, $s5, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][28];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s31, [$d1+8];
	activelanepermute_b32	$s38, $s23, $s3, 0, 0;
	ld_global_align(4)_f32	$s37, [$d1+8];
	activelanepermute_b32	$s39, $s29, $s3, 0, 0;
	ld_global_align(4)_f32	$s3, [$d1+8];
	st_spill_align(4)_u32	$s3, [%__spillStack][196];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s29, $s4, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][212];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s3, [$d1+8];
	st_spill_align(4)_u32	$s3, [%__spillStack][200];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s19, $s4, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][216];
	// 4-byte Folded Spill
	add_u32	$s3, $s2, 50;
	add_u32	$s4, $s43, 10;
	and_b32	$s4, $s4, 63;
	and_b32	$s3, $s3, 63;
	add_u32	$s5, $s2, 34;
	and_b32	$s5, $s5, 63;
	ld_global_align(4)_f32	$s6, [$d1+8];
	st_spill_align(4)_u32	$s6, [%__spillStack][88];
	// 4-byte Folded Spill
	activelanepermute_b32	$s6, $s19, $s5, 0, 0;
	st_spill_align(4)_u32	$s6, [%__spillStack][96];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s6, [$d1+8];
	st_spill_align(4)_u32	$s6, [%__spillStack][92];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s18, $s5, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][100];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1+8];
	st_spill_align(4)_u32	$s5, [%__spillStack][32];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s18, $s3, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][40];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1+8];
	st_spill_align(4)_u32	$s5, [%__spillStack][36];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s12, $s3, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][44];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s40, [$d1+12];
	activelanepermute_b32	$s45, $s23, $s4, 0, 0;
	ld_global_align(4)_f32	$s42, [$d1+12];
	activelanepermute_b32	$s46, $s29, $s4, 0, 0;
	add_u32	$s3, $s2, 42;
	and_b32	$s3, $s3, 63;
	add_u32	$s4, $s2, 26;
	and_b32	$s4, $s4, 63;
	ld_global_align(4)_f32	$s5, [$d1+12];
	st_spill_align(4)_u32	$s5, [%__spillStack][220];
	// 4-byte Folded Spill
	activelanepermute_b32	$s5, $s29, $s4, 0, 0;
	st_spill_align(4)_u32	$s5, [%__spillStack][236];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1+12];
	st_spill_align(4)_u32	$s5, [%__spillStack][224];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s4, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][240];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+12];
	st_spill_align(4)_u32	$s4, [%__spillStack][104];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s3, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][112];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+12];
	st_spill_align(4)_u32	$s4, [%__spillStack][108];
	// 4-byte Folded Spill
	activelanepermute_b32	$s3, $s18, $s3, 0, 0;
	st_spill_align(4)_u32	$s3, [%__spillStack][116];
	// 4-byte Folded Spill
	add_u32	$s2, $s2, 58;
	and_b32	$s4, $s2, 63;
	activelanepermute_b32	$s3, $s18, $s4, 0, 0;
	activelanepermute_b32	$s2, $s12, $s4, 0, 0;
	activelanepermute_b32	$s4, $s26, $s4, 0, 0;
	cmp_gt_b1_s32	$c0, $s1, 5;
	cbr_b1	$c0, @BB2_4;
	// BB#3:
	ld_global_align(4)_f32	$s2, [$d1+12];
	mov_b32	$s3, $s3;
	mul_ftz_f32	$s2, $s2, $s3;
	br	@BB2_5;

@BB2_4:
	ld_global_align(4)_f32	$s3, [$d1+12];
	ld_global_align(4)_f32	$s5, [$d1+12];
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s5, $s4;
	mov_b32	$s4, $s4;
	mov_b32	$s2, $s2;
	mul_ftz_f32	$s2, $s3, $s2;
	mov_b32	$s2, $s2;
	cmp_lt_b1_s32	$c0, $s1, 56;
	cmov_b32	$s2, $c0, $s2, $s4;
	mov_b32	$s2, $s2;

@BB2_5:
	st_spill_align(4)_u32	$s2, [%__spillStack][48];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s52, [$d1+16];
	add_u32	$s2, $s43, 11;
	and_b32	$s2, $s2, 63;
	activelanepermute_b32	$s55, $s23, $s2, 0, 0;
	ld_global_align(4)_f32	$s53, [$d1+16];
	activelanepermute_b32	$s56, $s29, $s2, 0, 0;
	ld_global_align(4)_f32	$s2, [$d1+16];
	st_spill_align(4)_u32	$s2, [%__spillStack][252];
	// 4-byte Folded Spill
	add_u32	$s2, $s44, 11;
	add_u32	$s3, $s41, 11;
	and_b32	$s3, $s3, 63;
	and_b32	$s2, $s2, 63;
	activelanepermute_b32	$s4, $s29, $s2, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][268];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+16];
	st_spill_align(4)_u32	$s4, [%__spillStack][260];
	// 4-byte Folded Spill
	activelanepermute_b32	$s2, $s19, $s2, 0, 0;
	st_spill_align(4)_u32	$s2, [%__spillStack][272];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s2, [$d1+16];
	st_spill_align(4)_u32	$s2, [%__spillStack][124];
	// 4-byte Folded Spill
	activelanepermute_b32	$s2, $s19, $s3, 0, 0;
	st_spill_align(4)_u32	$s2, [%__spillStack][132];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s2, [$d1+16];
	st_spill_align(4)_u32	$s2, [%__spillStack][128];
	// 4-byte Folded Spill
	activelanepermute_b32	$s2, $s18, $s3, 0, 0;
	st_spill_align(4)_u32	$s2, [%__spillStack][136];
	// 4-byte Folded Spill
	add_u32	$s2, $s49, 11;
	and_b32	$s4, $s2, 63;
	activelanepermute_b32	$s3, $s18, $s4, 0, 0;
	activelanepermute_b32	$s2, $s12, $s4, 0, 0;
	activelanepermute_b32	$s4, $s26, $s4, 0, 0;
	cmp_gt_b1_s32	$c0, $s1, 4;
	cbr_b1	$c0, @BB2_7;
	// BB#6:
	ld_global_align(4)_f32	$s2, [$d1+16];
	mov_b32	$s3, $s3;
	mul_ftz_f32	$s2, $s2, $s3;
	br	@BB2_8;

@BB2_7:
	ld_global_align(4)_f32	$s3, [$d1+16];
	ld_global_align(4)_f32	$s5, [$d1+16];
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s5, $s4;
	mov_b32	$s4, $s4;
	mov_b32	$s2, $s2;
	mul_ftz_f32	$s2, $s3, $s2;
	mov_b32	$s2, $s2;
	cmp_lt_b1_s32	$c0, $s1, 56;
	cmov_b32	$s2, $c0, $s2, $s4;
	mov_b32	$s2, $s2;

@BB2_8:
	st_spill_align(4)_u32	$s2, [%__spillStack][84];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s62, [$d1+20];
	add_u32	$s2, $s43, 12;
	and_b32	$s3, $s2, 63;
	activelanepermute_b32	$s2, $s23, $s3, 0, 0;
	ld_global_align(4)_f32	$s63, [$d1+20];
	activelanepermute_b32	$s3, $s29, $s3, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+20];
	add_u32	$s4, $s44, 12;
	add_u32	$s5, $s41, 12;
	and_b32	$s5, $s5, 63;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s32, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s27, [$d1+20];
	activelanepermute_b32	$s36, $s19, $s4, 0, 0;
	ld_global_align(4)_f32	$s4, [$d1+20];
	st_spill_align(4)_u32	$s4, [%__spillStack][188];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s19, $s5, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][204];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+20];
	st_spill_align(4)_u32	$s4, [%__spillStack][192];
	// 4-byte Folded Spill
	activelanepermute_b32	$s4, $s18, $s5, 0, 0;
	st_spill_align(4)_u32	$s4, [%__spillStack][208];
	// 4-byte Folded Spill
	add_u32	$s4, $s49, 12;
	and_b32	$s6, $s4, 63;
	activelanepermute_b32	$s5, $s18, $s6, 0, 0;
	activelanepermute_b32	$s4, $s12, $s6, 0, 0;
	activelanepermute_b32	$s6, $s26, $s6, 0, 0;
	cmp_gt_b1_s32	$c0, $s1, 3;
	cbr_b1	$c0, @BB2_10;
	// BB#9:
	ld_global_align(4)_f32	$s4, [$d1+20];
	mov_b32	$s5, $s5;
	mul_ftz_f32	$s4, $s4, $s5;
	br	@BB2_11;

@BB2_10:
	ld_global_align(4)_f32	$s5, [$d1+20];
	ld_global_align(4)_f32	$s7, [$d1+20];
	mov_b32	$s6, $s6;
	mul_ftz_f32	$s6, $s7, $s6;
	mov_b32	$s6, $s6;
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s5, $s4;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 56;
	cmov_b32	$s4, $c0, $s4, $s6;
	mov_b32	$s4, $s4;

@BB2_11:
	st_spill_align(4)_u32	$s4, [%__spillStack][120];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1+24];
	add_u32	$s4, $s43, 20;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s7, $s23, $s4, 0, 0;
	ld_global_align(4)_f32	$s6, [$d1+24];
	activelanepermute_b32	$s8, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s47, [$d1+24];
	add_u32	$s4, $s44, 20;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s50, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s48, [$d1+24];
	activelanepermute_b32	$s51, $s19, $s4, 0, 0;
	add_u32	$s4, $s49, 20;
	add_u32	$s9, $s41, 20;
	and_b32	$s10, $s9, 63;
	and_b32	$s11, $s4, 63;
	activelanepermute_b32	$s9, $s19, $s10, 0, 0;
	activelanepermute_b32	$s4, $s18, $s10, 0, 0;
	activelanepermute_b32	$s10, $s12, $s10, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+24];
	st_spill_align(4)_u32	$s13, [%__spillStack][156];
	// 4-byte Folded Spill
	activelanepermute_b32	$s13, $s12, $s11, 0, 0;
	st_spill_align(4)_u32	$s13, [%__spillStack][168];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s13, [$d1+24];
	st_spill_align(4)_u32	$s13, [%__spillStack][160];
	// 4-byte Folded Spill
	activelanepermute_b32	$s11, $s26, $s11, 0, 0;
	st_spill_align(4)_u32	$s11, [%__spillStack][176];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 9;
	cbr_b1	$c0, @BB2_13;
	// BB#12:
	ld_global_align(4)_f32	$s4, [$d1+24];
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s4, $s4, $s9;
	br	@BB2_14;

@BB2_13:
	ld_global_align(4)_f32	$s9, [$d1+24];
	ld_global_align(4)_f32	$s11, [$d1+24];
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s11, $s10;
	mov_b32	$s10, $s10;
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s9, $s4;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 62;
	cmov_b32	$s4, $c0, $s4, $s10;
	mov_b32	$s4, $s4;

@BB2_14:
	st_spill_align(4)_u32	$s4, [%__spillStack][256];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s9, [$d1+28];
	add_u32	$s4, $s43, 21;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s11, $s23, $s4, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+28];
	activelanepermute_b32	$s13, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s57, [$d1+28];
	add_u32	$s4, $s44, 21;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s59, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s58, [$d1+28];
	activelanepermute_b32	$s60, $s19, $s4, 0, 0;
	add_u32	$s4, $s49, 21;
	add_u32	$s14, $s41, 21;
	and_b32	$s15, $s14, 63;
	and_b32	$s16, $s4, 63;
	activelanepermute_b32	$s14, $s19, $s15, 0, 0;
	activelanepermute_b32	$s4, $s18, $s15, 0, 0;
	activelanepermute_b32	$s15, $s12, $s15, 0, 0;
	ld_global_align(4)_f32	$s17, [$d1+28];
	st_spill_align(4)_u32	$s17, [%__spillStack][228];
	// 4-byte Folded Spill
	activelanepermute_b32	$s17, $s12, $s16, 0, 0;
	st_spill_align(4)_u32	$s17, [%__spillStack][244];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s17, [$d1+28];
	st_spill_align(4)_u32	$s17, [%__spillStack][232];
	// 4-byte Folded Spill
	activelanepermute_b32	$s16, $s26, $s16, 0, 0;
	st_spill_align(4)_u32	$s16, [%__spillStack][248];
	// 4-byte Folded Spill
	cmp_gt_b1_s32	$c0, $s1, 8;
	cbr_b1	$c0, @BB2_16;
	// BB#15:
	ld_global_align(4)_f32	$s4, [$d1+28];
	mov_b32	$s14, $s14;
	mul_ftz_f32	$s30, $s4, $s14;
	br	@BB2_17;

@BB2_16:
	ld_global_align(4)_f32	$s14, [$d1+28];
	ld_global_align(4)_f32	$s16, [$d1+28];
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s15, $s16, $s15;
	mov_b32	$s15, $s15;
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s14, $s4;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 61;
	cmov_b32	$s4, $c0, $s4, $s15;
	mov_b32	$s30, $s4;

@BB2_17:
	add_u32	$s61, $s61, $s33;
	add_u32	$s28, $s54, $s33;
	ld_global_align(4)_f32	$s54, [$d1+32];
	add_u32	$s4, $s43, 22;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s15, $s23, $s4, 0, 0;
	ld_global_align(4)_f32	$s14, [$d1+32];
	activelanepermute_b32	$s16, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s43, [$d1+32];
	add_u32	$s4, $s44, 22;
	and_b32	$s4, $s4, 63;
	activelanepermute_b32	$s44, $s29, $s4, 0, 0;
	ld_global_align(4)_f32	$s29, [$d1+32];
	activelanepermute_b32	$s4, $s19, $s4, 0, 0;
	add_u32	$s23, $s49, 22;
	add_u32	$s41, $s41, 22;
	and_b32	$s41, $s41, 63;
	and_b32	$s17, $s23, 63;
	activelanepermute_b32	$s49, $s19, $s41, 0, 0;
	activelanepermute_b32	$s23, $s18, $s41, 0, 0;
	activelanepermute_b32	$s41, $s12, $s41, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+32];
	st_spill_align(4)_u32	$s18, [%__spillStack][264];
	// 4-byte Folded Spill
	activelanepermute_b32	$s18, $s12, $s17, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+32];
	activelanepermute_b32	$s19, $s26, $s17, 0, 0;
	cmp_gt_b1_s32	$c0, $s1, 7;
	cbr_b1	$c0, @BB2_19;
	// BB#18:
	ld_global_align(4)_f32	$s17, [$d1+32];
	mov_b32	$s23, $s49;
	mul_ftz_f32	$s26, $s17, $s23;
	br	@BB2_20;

@BB2_19:
	ld_global_align(4)_f32	$s17, [$d1+32];
	ld_global_align(4)_f32	$s26, [$d1+32];
	mov_b32	$s41, $s41;
	mul_ftz_f32	$s26, $s26, $s41;
	mov_b32	$s26, $s26;
	mov_b32	$s23, $s23;
	mul_ftz_f32	$s17, $s17, $s23;
	mov_b32	$s17, $s17;
	cmp_lt_b1_s32	$c0, $s1, 60;
	cmov_b32	$s17, $c0, $s17, $s26;
	mov_b32	$s26, $s17;

@BB2_20:
	add_u32	$s17, $s35, $s33;
	add_u32	$s23, $s34, $s33;
	cmp_lt_b1_s32	$c1, $s28, $s23;
	cmp_lt_b1_s32	$c0, $s61, $s17;
	and_b1	$c1, $c0, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	cbr_b1	$c1, @BB2_22;
	// BB#21:
	ld_spill_align(4)_u32	$s17, [%__spillStack][284];
	// 4-byte Folded Reload
	mov_b32	$s17, $s17;
	ld_spill_align(4)_u32	$s33, [%__spillStack][288];
	// 4-byte Folded Reload
	mov_b32	$s33, $s33;
	ld_spill_align(4)_u32	$s34, [%__spillStack][276];
	// 4-byte Folded Reload
	mul_ftz_f32	$s17, $s34, $s17;
	ld_spill_align(4)_u32	$s34, [%__spillStack][280];
	// 4-byte Folded Reload
	mul_ftz_f32	$s33, $s34, $s33;
	mov_b32	$s24, $s24;
	mov_b32	$s25, $s25;
	mov_b32	$s17, $s17;
	mov_b32	$s33, $s33;
	cmp_lt_b1_s32	$c1, $s1, 52;
	mul_ftz_f32	$s20, $s20, $s24;
	mul_ftz_f32	$s21, $s21, $s25;
	mov_b32	$s24, $s39;
	mov_b32	$s25, $s38;
	cmov_b32	$s17, $c1, $s17, $s33;
	mov_b32	$s20, $s20;
	mov_b32	$s21, $s21;
	cmp_lt_b1_s32	$c1, $s1, 51;
	mul_ftz_f32	$s24, $s37, $s24;
	mul_ftz_f32	$s25, $s31, $s25;
	mov_b32	$s31, $s46;
	mov_b32	$s33, $s45;
	cmov_b32	$s20, $c1, $s20, $s21;
	mov_b32	$s21, $s25;
	mov_b32	$s24, $s24;
	cmp_lt_b1_s32	$c1, $s1, 50;
	mul_ftz_f32	$s25, $s42, $s31;
	mul_ftz_f32	$s31, $s40, $s33;
	mov_b32	$s17, $s17;
	mov_b32	$s33, $s55;
	mov_b32	$s34, $s56;
	cmov_b32	$s21, $c1, $s21, $s24;
	mov_b32	$s24, $s31;
	mov_b32	$s25, $s25;
	cmp_lt_b1_s32	$c1, $s1, 44;
	add_ftz_f32	$s17, $s17, 0F00000000;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s31, $s52, $s33;
	mul_ftz_f32	$s33, $s53, $s34;
	mov_b32	$s2, $s2;
	mov_b32	$s3, $s3;
	cmov_b32	$s24, $c1, $s24, $s25;
	add_ftz_f32	$s17, $s17, $s20;
	mov_b32	$s20, $s21;
	mov_b32	$s21, $s31;
	mov_b32	$s25, $s33;
	cmp_lt_b1_s32	$c1, $s1, 43;
	mul_ftz_f32	$s2, $s62, $s2;
	mul_ftz_f32	$s3, $s63, $s3;
	mov_b32	$s7, $s7;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s17, $s17, $s20;
	mov_b32	$s20, $s24;
	cmov_b32	$s21, $c1, $s21, $s25;
	mov_b32	$s2, $s2;
	mov_b32	$s3, $s3;
	cmp_lt_b1_s32	$c1, $s1, 42;
	mul_ftz_f32	$s5, $s5, $s7;
	mul_ftz_f32	$s6, $s6, $s8;
	mov_b32	$s7, $s11;
	mov_b32	$s8, $s13;
	add_ftz_f32	$s11, $s17, $s20;
	mov_b32	$s13, $s21;
	cmov_b32	$s2, $c1, $s2, $s3;
	mov_b32	$s3, $s5;
	mov_b32	$s5, $s6;
	cmp_lt_b1_s32	$c1, $s1, 36;
	mul_ftz_f32	$s6, $s9, $s7;
	mul_ftz_f32	$s7, $s10, $s8;
	mov_b32	$s8, $s15;
	mov_b32	$s9, $s16;
	add_ftz_f32	$s10, $s11, $s13;
	mov_b32	$s2, $s2;
	cmov_b32	$s3, $c1, $s3, $s5;
	mov_b32	$s5, $s6;
	mov_b32	$s6, $s7;
	cmp_lt_b1_s32	$c1, $s1, 35;
	mul_ftz_f32	$s7, $s54, $s8;
	mul_ftz_f32	$s8, $s14, $s9;
	add_ftz_f32	$s2, $s10, $s2;
	mov_b32	$s3, $s3;
	cmov_b32	$s5, $c1, $s5, $s6;
	mov_b32	$s6, $s7;
	mov_b32	$s7, $s8;
	cmp_lt_b1_s32	$c1, $s1, 34;
	add_ftz_f32	$s2, $s2, $s3;
	mov_b32	$s3, $s5;
	cmov_b32	$s5, $c1, $s6, $s7;
	add_ftz_f32	$s2, $s2, $s3;
	mov_b32	$s3, $s5;
	add_ftz_f32	$s2, $s2, $s3;
	mul_u32	$s3, $s28, $s0;
	add_u32	$s3, $s3, $s61;
	cvt_s64_s32	$d1, $s3;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s2, [$d1];

@BB2_22:
	cmp_lt_b1_s32	$c1, $s1, 24;
	add_u32	$s2, $s28, 8;
	cmp_lt_b1_s32	$c2, $s2, $s23;
	and_b1	$c2, $c0, $c2;
	cmp_ne_b1_b1	$c2, $c2, 1;
	cbr_b1	$c2, @BB2_24;
	// BB#23:
	ld_spill_align(4)_u32	$s3, [%__spillStack][148];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][152];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][140];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s6, $s3;
	ld_spill_align(4)_u32	$s6, [%__spillStack][144];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s6, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][180];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][184];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	mov_b32	$s3, $s3;
	mov_b32	$s5, $s5;
	cmp_lt_b1_s32	$c2, $s1, 40;
	ld_spill_align(4)_u32	$s8, [%__spillStack][164];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s8, $s6;
	ld_spill_align(4)_u32	$s8, [%__spillStack][172];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s8, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][216];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	ld_spill_align(4)_u32	$s9, [%__spillStack][212];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	cmov_b32	$s3, $c2, $s3, $s5;
	mov_b32	$s5, $s6;
	mov_b32	$s6, $s7;
	cmp_lt_b1_s32	$c2, $s1, 39;
	ld_spill_align(4)_u32	$s7, [%__spillStack][200];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][196];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][240];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][236];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	cmov_b32	$s5, $c2, $s5, $s6;
	mov_b32	$s6, $s8;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c2, $s1, 38;
	ld_spill_align(4)_u32	$s8, [%__spillStack][224];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][220];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s10;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s10, [%__spillStack][268];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][272];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	cmov_b32	$s6, $c2, $s6, $s7;
	mov_b32	$s7, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c2, $s1, 32;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s9, [%__spillStack][252];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][260];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s10, $s11;
	mov_b32	$s11, $s32;
	mov_b32	$s13, $s36;
	cmov_b32	$s7, $c2, $s7, $s8;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	mov_b32	$s6, $s9;
	mov_b32	$s8, $s10;
	cmp_lt_b1_s32	$c2, $s1, 31;
	mul_ftz_f32	$s9, $s22, $s11;
	mul_ftz_f32	$s10, $s27, $s13;
	mov_b32	$s11, $s50;
	mov_b32	$s13, $s51;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s7;
	cmov_b32	$s6, $c2, $s6, $s8;
	mov_b32	$s7, $s9;
	mov_b32	$s8, $s10;
	cmp_lt_b1_s32	$c2, $s1, 30;
	mul_ftz_f32	$s9, $s47, $s11;
	mul_ftz_f32	$s10, $s48, $s13;
	mov_b32	$s11, $s59;
	mov_b32	$s13, $s60;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c2, $s7, $s8;
	mov_b32	$s7, $s9;
	mov_b32	$s8, $s10;
	mul_ftz_f32	$s9, $s57, $s11;
	mul_ftz_f32	$s10, $s58, $s13;
	mov_b32	$s11, $s44;
	mov_b32	$s4, $s4;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c1, $s7, $s8;
	mov_b32	$s7, $s9;
	mov_b32	$s8, $s10;
	cmp_lt_b1_s32	$c2, $s1, 23;
	mul_ftz_f32	$s9, $s43, $s11;
	mul_ftz_f32	$s4, $s29, $s4;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c2, $s7, $s8;
	mov_b32	$s7, $s9;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c2, $s1, 22;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s4, $c2, $s7, $s4;
	add_ftz_f32	$s3, $s3, $s5;
	mov_b32	$s4, $s4;
	add_ftz_f32	$s3, $s3, $s4;
	mul_u32	$s2, $s2, $s0;
	add_u32	$s2, $s2, $s61;
	cvt_s64_s32	$d1, $s2;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s3, [$d1];

@BB2_24:
	add_u32	$s2, $s28, 16;
	cmp_lt_b1_s32	$c2, $s2, $s23;
	and_b1	$c2, $c0, $c2;
	cmp_ne_b1_b1	$c2, $c2, 1;
	cbr_b1	$c2, @BB2_26;
	// BB#25:
	ld_spill_align(4)_u32	$s3, [%__spillStack][60];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s4, [%__spillStack][64];
	// 4-byte Folded Reload
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][52];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s5, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][56];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s5, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][76];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][80];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	mov_b32	$s3, $s3;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c2, $s1, 26;
	ld_spill_align(4)_u32	$s7, [%__spillStack][68];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s7, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][72];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s7, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][100];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][96];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	cmov_b32	$s3, $c2, $s3, $s4;
	mov_b32	$s4, $s5;
	mov_b32	$s5, $s6;
	cmp_lt_b1_s32	$c2, $s1, 25;
	ld_spill_align(4)_u32	$s6, [%__spillStack][116];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	ld_spill_align(4)_u32	$s9, [%__spillStack][112];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][92];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s10, $s7;
	ld_spill_align(4)_u32	$s10, [%__spillStack][88];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s10, $s8;
	cmov_b32	$s4, $c2, $s4, $s5;
	ld_spill_align(4)_u32	$s5, [%__spillStack][108];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s5, $s6;
	ld_spill_align(4)_u32	$s6, [%__spillStack][104];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s9;
	mov_b32	$s8, $s8;
	mov_b32	$s7, $s7;
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s9, [%__spillStack][132];
	// 4-byte Folded Reload
	mov_b32	$s9, $s9;
	ld_spill_align(4)_u32	$s10, [%__spillStack][136];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	mov_b32	$s6, $s6;
	mov_b32	$s5, $s5;
	cmp_lt_b1_s32	$c2, $s1, 18;
	cmov_b32	$s7, $c1, $s8, $s7;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s8, [%__spillStack][124];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s9;
	ld_spill_align(4)_u32	$s9, [%__spillStack][128];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s10;
	ld_spill_align(4)_u32	$s10, [%__spillStack][204];
	// 4-byte Folded Reload
	mov_b32	$s10, $s10;
	ld_spill_align(4)_u32	$s11, [%__spillStack][208];
	// 4-byte Folded Reload
	mov_b32	$s11, $s11;
	cmov_b32	$s5, $c2, $s6, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s7;
	mov_b32	$s6, $s8;
	mov_b32	$s7, $s9;
	cmp_lt_b1_s32	$c1, $s1, 17;
	ld_spill_align(4)_u32	$s8, [%__spillStack][188];
	// 4-byte Folded Reload
	mul_ftz_f32	$s8, $s8, $s10;
	ld_spill_align(4)_u32	$s9, [%__spillStack][192];
	// 4-byte Folded Reload
	mul_ftz_f32	$s9, $s9, $s11;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c1, $s6, $s7;
	mov_b32	$s6, $s8;
	mov_b32	$s7, $s9;
	cmp_lt_b1_s32	$c1, $s1, 16;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	cmov_b32	$s5, $c1, $s6, $s7;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	add_ftz_f32	$s3, $s3, $s4;
	ld_spill_align(4)_u32	$s4, [%__spillStack][256];
	// 4-byte Folded Reload
	add_ftz_f32	$s3, $s3, $s4;
	add_ftz_f32	$s3, $s3, $s30;
	add_ftz_f32	$s3, $s3, $s26;
	mul_u32	$s2, $s2, $s0;
	add_u32	$s2, $s2, $s61;
	cvt_s64_s32	$d1, $s2;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	st_global_align(4)_f32	$s3, [$d1];

@BB2_26:
	add_u32	$s2, $s28, 24;
	cmp_lt_b1_s32	$c1, $s2, $s23;
	and_b1	$c0, $c0, $c1;
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB2_28;
	// BB#27:
	ld_spill_align(4)_u32	$s3, [%__spillStack][8];
	// 4-byte Folded Reload
	mov_b32	$s3, $s3;
	ld_spill_align(4)_u32	$s4, [%__spillStack][12];
	// 4-byte Folded Reload
	mov_b32	$s4, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack];
	// 4-byte Folded Reload
	mul_ftz_f32	$s3, $s5, $s3;
	ld_spill_align(4)_u32	$s5, [%__spillStack][4];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s5, $s4;
	ld_spill_align(4)_u32	$s5, [%__spillStack][24];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][28];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	mov_b32	$s3, $s3;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 14;
	ld_spill_align(4)_u32	$s7, [%__spillStack][16];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s7, $s5;
	ld_spill_align(4)_u32	$s7, [%__spillStack][20];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s7, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][44];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][40];
	// 4-byte Folded Reload
	mov_b32	$s8, $s8;
	cmov_b32	$s3, $c0, $s3, $s4;
	mov_b32	$s4, $s5;
	mov_b32	$s5, $s6;
	cmp_lt_b1_s32	$c0, $s1, 13;
	ld_spill_align(4)_u32	$s6, [%__spillStack][36];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s7;
	ld_spill_align(4)_u32	$s7, [%__spillStack][32];
	// 4-byte Folded Reload
	mul_ftz_f32	$s7, $s7, $s8;
	cmov_b32	$s4, $c0, $s4, $s5;
	mov_b32	$s5, $s7;
	mov_b32	$s6, $s6;
	cmp_lt_b1_s32	$c0, $s1, 12;
	mov_b32	$s3, $s3;
	cmov_b32	$s5, $c0, $s5, $s6;
	add_ftz_f32	$s3, $s3, 0F00000000;
	mov_b32	$s4, $s4;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s4, $s5;
	ld_spill_align(4)_u32	$s5, [%__spillStack][168];
	// 4-byte Folded Reload
	mov_b32	$s5, $s5;
	ld_spill_align(4)_u32	$s6, [%__spillStack][176];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	add_ftz_f32	$s3, $s3, $s4;
	ld_spill_align(4)_u32	$s4, [%__spillStack][156];
	// 4-byte Folded Reload
	mul_ftz_f32	$s4, $s4, $s5;
	ld_spill_align(4)_u32	$s5, [%__spillStack][160];
	// 4-byte Folded Reload
	mul_ftz_f32	$s5, $s5, $s6;
	ld_spill_align(4)_u32	$s6, [%__spillStack][244];
	// 4-byte Folded Reload
	mov_b32	$s6, $s6;
	ld_spill_align(4)_u32	$s7, [%__spillStack][248];
	// 4-byte Folded Reload
	mov_b32	$s7, $s7;
	ld_spill_align(4)_u32	$s8, [%__spillStack][48];
	// 4-byte Folded Reload
	add_ftz_f32	$s3, $s3, $s8;
	mov_b32	$s5, $s5;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 48;
	ld_spill_align(4)_u32	$s1, [%__spillStack][228];
	// 4-byte Folded Reload
	mul_ftz_f32	$s1, $s1, $s6;
	ld_spill_align(4)_u32	$s6, [%__spillStack][232];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s7;
	mov_b32	$s7, $s18;
	mov_b32	$s8, $s19;
	ld_spill_align(4)_u32	$s9, [%__spillStack][84];
	// 4-byte Folded Reload
	add_ftz_f32	$s3, $s3, $s9;
	cmov_b32	$s4, $c0, $s4, $s5;
	mov_b32	$s5, $s6;
	mov_b32	$s1, $s1;
	ld_spill_align(4)_u32	$s6, [%__spillStack][264];
	// 4-byte Folded Reload
	mul_ftz_f32	$s6, $s6, $s7;
	mul_ftz_f32	$s7, $s12, $s8;
	ld_spill_align(4)_u32	$s8, [%__spillStack][120];
	// 4-byte Folded Reload
	add_ftz_f32	$s3, $s3, $s8;
	mov_b32	$s4, $s4;
	cmov_b32	$s1, $c0, $s1, $s5;
	mov_b32	$s5, $s7;
	mov_b32	$s6, $s6;
	add_ftz_f32	$s3, $s3, $s4;
	mov_b32	$s1, $s1;
	cmov_b32	$s4, $c0, $s6, $s5;
	add_ftz_f32	$s1, $s3, $s1;
	mov_b32	$s3, $s4;
	add_ftz_f32	$s1, $s1, $s3;
	mul_u32	$s0, $s2, $s0;
	add_u32	$s0, $s0, $s61;
	cvt_s64_s32	$d1, $s0;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	st_global_align(4)_f32	$s1, [$d0];

@BB2_28:
	// %_ZZ17Stencil_Hcc_Shfl4RN2hc5arrayIfLi1EEES2_S2_iiiENK3$_6clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ18Stencil_Hcc_Shfl4xRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__519__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	// BB#0:
	laneid_u32	$s1;
	cvt_s64_s32	$d0, $s1;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s0, $d1;
	shr_u64	$d0, $d0, 32;
	cvt_u32_u64	$s2, $d0;
	shr_s32	$s2, $s2, 2;
	add_u32	$s4, $s2, $s0;
	shr_s32	$s9, $s1, 3;
	workitemabsid_u32	$s0, 1;
	shl_u32	$s0, $s0, 2;
	and_b32	$s0, $s0, -32;
	add_u32	$s3, $s9, $s0;
	and_b32	$s6, $s3, -8;
	ld_kernarg_align(4)_width(all)_u32	$s7, [%__arg_p9];
	shl_u32	$s2, $s7, 1;
	ld_kernarg_align(4)_width(all)_u32	$s11, [%__arg_p11];
	ld_kernarg_align(4)_width(all)_u32	$s12, [%__arg_p10];
	add_u32	$s0, $s4, $s6;
	add_u32	$s5, $s0, 32;
	add_u32	$s0, $s2, $s12;
	add_u32	$s2, $s2, $s11;
	workitemabsid_u32	$s8, 0;
	mul_u32	$s10, $s4, 10;
	and_b32	$s14, $s8, -8;
	sub_u32	$s10, $s1, $s10;
	add_u32	$s13, $s10, $s14;
	cmp_lt_b1_s32	$c0, $s5, $s2;
	cmp_lt_b1_s32	$c1, $s13, $s0;
	and_b1	$c0, $c1, $c0;
	add_u32	$s10, $s1, 2;
	cvt_s64_s32	$d0, $s10;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	add_u32	$s15, $s1, 6;
	cvt_s64_s32	$d2, $s15;
	mul_u64	$d3, $d2, 0x66666667;
	shr_u64	$d4, $d3, 32;
	cvt_u32_u64	$s2, $d1;
	shr_u64	$d0, $d0, 32;
	cvt_u32_u64	$s16, $d0;
	shr_s32	$s16, $s16, 2;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	add_u32	$s16, $s16, $s2;
	cvt_u32_u64	$s17, $d4;
	shr_u64	$d3, $d3, 63;
	add_u32	$s2, $s8, $s7;
	add_u32	$s8, $s6, $s16;
	cvt_u32_u64	$s18, $d3;
	shr_s32	$s17, $s17, 2;
	add_u32	$s19, $s1, 4;
	cvt_s64_s32	$d3, $s19;
	mul_u64	$d3, $d3, 0x66666667;
	shr_u64	$d4, $d3, 63;
	cvt_u32_u64	$s20, $d4;
	add_u32	$s3, $s3, $s7;
	add_u32	$s17, $s17, $s18;
	add_u32	$s8, $s8, 19;
	mul_u32	$s8, $s8, $s0;
	add_u32	$s18, $s6, $s17;
	mul_u32	$s16, $s16, 10;
	sub_u32	$s10, $s10, $s16;
	add_u32	$s10, $s10, $s14;
	shr_u64	$d3, $d3, 32;
	cvt_u32_u64	$s16, $d3;
	shr_s32	$s16, $s16, 2;
	add_u32	$s18, $s18, 25;
	add_u32	$s8, $s10, $s8;
	add_u32	$s10, $s16, $s20;
	or_b32	$s16, $s6, 6;
	add_u32	$s20, $s1, 8;
	cvt_s64_s32	$d3, $s20;
	mul_u64	$d3, $d3, 0x66666667;
	add_u32	$s4, $s6, $s4;
	add_u32	$s16, $s10, $s16;
	mul_u32	$s16, $s16, $s0;
	shr_u64	$d4, $d3, 63;
	cvt_s64_s32	$d5, $s8;
	mul_u32	$s8, $s18, $s0;
	mul_u32	$s17, $s17, 10;
	sub_u32	$s15, $s15, $s17;
	add_u32	$s15, $s15, $s14;
	add_u32	$s8, $s15, $s8;
	cvt_s64_s32	$d6, $s8;
	mul_u32	$s8, $s10, 10;
	sub_u32	$s8, $s19, $s8;
	add_u32	$s8, $s8, $s14;
	shl_u64	$d6, $d6, 2;
	shl_u64	$d5, $d5, 2;
	add_u32	$s8, $s8, $s16;
	cvt_u32_u64	$s10, $d4;
	shr_u64	$d3, $d3, 32;
	cvt_u32_u64	$s15, $d3;
	shr_s32	$s15, $s15, 2;
	add_u32	$s10, $s15, $s10;
	add_u32	$s6, $s6, $s10;
	add_u32	$s15, $s6, 12;
	cvt_s64_s32	$d3, $s8;
	add_u64	$d4, $d2, $d5;
	add_u64	$d5, $d2, $d6;
	mul_u32	$s4, $s4, $s0;
	add_u32	$s4, $s4, $s13;
	cvt_s64_s32	$d6, $s4;
	shl_u64	$d6, $d6, 2;
	add_u64	$d6, $d2, $d6;
	cmp_ne_b1_b1	$c0, $c0, 1;
	ld_global_align(4)_u32	$s17, [$d6];
	ld_global_align(4)_u32	$s4, [$d5];
	ld_global_align(4)_u32	$s6, [$d4];
	shl_u64	$d3, $d3, 2;
	mul_u32	$s8, $s15, $s0;
	mul_u32	$s10, $s10, 10;
	sub_u32	$s10, $s20, $s10;
	add_u32	$s10, $s10, $s14;
	add_u32	$s8, $s10, $s8;
	cvt_s64_s32	$d4, $s8;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d2, $d4;
	ld_global_align(4)_u32	$s8, [$d4];
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s10, [$d3];
	cbr_b1	$c0, @BB3_2;
	// BB#1:
	mul_u32	$s5, $s5, $s0;
	add_u32	$s5, $s5, $s13;
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s5, [$d2];

@BB3_2:
	// %._crit_edge.i
	add_u32	$s12, $s12, $s7;
	add_u32	$s7, $s11, $s7;
	cmp_lt_b1_s32	$c1, $s3, $s7;
	cmp_lt_b1_s32	$c0, $s2, $s12;
	and_b1	$c1, $c0, $c1;
	shl_u32	$s9, $s9, 1;
	add_u32	$s11, $s9, $s1;
	and_b32	$s12, $s11, 63;
	activelanepermute_b32	$s19, $s17, $s12, 0, 0;
	activelanepermute_b32	$s20, $s10, $s12, 0, 0;
	add_u32	$s12, $s11, 1;
	and_b32	$s12, $s12, 63;
	add_u32	$s13, $s11, 12;
	add_u32	$s14, $s11, 10;
	activelanepermute_b32	$s18, $s17, $s12, 0, 0;
	add_u32	$s15, $s11, 22;
	add_u32	$s16, $s11, 21;
	add_u32	$s21, $s11, 20;
	add_u32	$s22, $s11, 11;
	add_u32	$s11, $s11, 2;
	and_b32	$s11, $s11, 63;
	and_b32	$s14, $s14, 63;
	and_b32	$s22, $s22, 63;
	and_b32	$s23, $s13, 63;
	and_b32	$s21, $s21, 63;
	and_b32	$s29, $s16, 63;
	and_b32	$s30, $s15, 63;
	cmp_ne_b1_b1	$c1, $c1, 1;
	activelanepermute_b32	$s25, $s10, $s12, 0, 0;
	activelanepermute_b32	$s27, $s17, $s11, 0, 0;
	activelanepermute_b32	$s28, $s10, $s11, 0, 0;
	activelanepermute_b32	$s11, $s17, $s14, 0, 0;
	activelanepermute_b32	$s12, $s10, $s14, 0, 0;
	activelanepermute_b32	$s13, $s17, $s22, 0, 0;
	activelanepermute_b32	$s14, $s10, $s22, 0, 0;
	activelanepermute_b32	$s15, $s17, $s23, 0, 0;
	activelanepermute_b32	$s16, $s10, $s23, 0, 0;
	activelanepermute_b32	$s24, $s17, $s21, 0, 0;
	activelanepermute_b32	$s26, $s10, $s21, 0, 0;
	activelanepermute_b32	$s22, $s17, $s29, 0, 0;
	activelanepermute_b32	$s23, $s10, $s29, 0, 0;
	activelanepermute_b32	$s17, $s17, $s30, 0, 0;
	activelanepermute_b32	$s21, $s10, $s30, 0, 0;
	cbr_b1	$c1, @BB3_4;
	// BB#3:
	ld_global_align(4)_f32	$s29, [$d1];
	ld_global_align(4)_f32	$s30, [$d1];
	ld_global_align(4)_f32	$s31, [$d1+4];
	ld_global_align(4)_f32	$s32, [$d1+4];
	ld_global_align(4)_f32	$s33, [$d1+8];
	ld_global_align(4)_f32	$s34, [$d1+8];
	ld_global_align(4)_f32	$s35, [$d1+12];
	ld_global_align(4)_f32	$s36, [$d1+12];
	ld_global_align(4)_f32	$s37, [$d1+16];
	ld_global_align(4)_f32	$s38, [$d1+16];
	ld_global_align(4)_f32	$s39, [$d1+20];
	ld_global_align(4)_f32	$s40, [$d1+20];
	ld_global_align(4)_f32	$s41, [$d1+24];
	ld_global_align(4)_f32	$s42, [$d1+24];
	ld_global_align(4)_f32	$s43, [$d1+28];
	ld_global_align(4)_f32	$s44, [$d1+28];
	ld_global_align(4)_f32	$s45, [$d1+32];
	ld_global_align(4)_f32	$s46, [$d1+32];
	mov_b32	$s28, $s28;
	mul_ftz_f32	$s28, $s34, $s28;
	mov_b32	$s28, $s28;
	mov_b32	$s27, $s27;
	mul_ftz_f32	$s27, $s33, $s27;
	mov_b32	$s27, $s27;
	cmp_lt_b1_s32	$c1, $s1, 50;
	cmov_b32	$s27, $c1, $s27, $s28;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s20, $s30, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s19, $s19;
	mul_ftz_f32	$s19, $s29, $s19;
	mov_b32	$s19, $s19;
	cmp_lt_b1_s32	$c1, $s1, 52;
	cmov_b32	$s19, $c1, $s19, $s20;
	mov_b32	$s20, $s25;
	mul_ftz_f32	$s20, $s32, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s18, $s18;
	mul_ftz_f32	$s18, $s31, $s18;
	mov_b32	$s18, $s18;
	cmp_lt_b1_s32	$c1, $s1, 51;
	cmov_b32	$s18, $c1, $s18, $s20;
	mov_b32	$s20, $s26;
	mul_ftz_f32	$s20, $s42, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s24, $s24;
	mul_ftz_f32	$s24, $s41, $s24;
	mov_b32	$s24, $s24;
	cmp_lt_b1_s32	$c1, $s1, 36;
	cmov_b32	$s20, $c1, $s24, $s20;
	mov_b32	$s23, $s23;
	mul_ftz_f32	$s23, $s44, $s23;
	mov_b32	$s23, $s23;
	mov_b32	$s22, $s22;
	mul_ftz_f32	$s22, $s43, $s22;
	mov_b32	$s22, $s22;
	cmp_lt_b1_s32	$c1, $s1, 35;
	cmov_b32	$s22, $c1, $s22, $s23;
	mov_b32	$s21, $s21;
	mul_ftz_f32	$s21, $s46, $s21;
	mov_b32	$s21, $s21;
	mov_b32	$s17, $s17;
	mul_ftz_f32	$s17, $s45, $s17;
	mov_b32	$s17, $s17;
	cmp_lt_b1_s32	$c1, $s1, 34;
	cmov_b32	$s17, $c1, $s17, $s21;
	mov_b32	$s16, $s16;
	mul_ftz_f32	$s16, $s40, $s16;
	mov_b32	$s16, $s16;
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s15, $s39, $s15;
	mov_b32	$s15, $s15;
	cmp_lt_b1_s32	$c1, $s1, 42;
	cmov_b32	$s15, $c1, $s15, $s16;
	mov_b32	$s14, $s14;
	mul_ftz_f32	$s14, $s38, $s14;
	mov_b32	$s14, $s14;
	mov_b32	$s13, $s13;
	mul_ftz_f32	$s13, $s37, $s13;
	mov_b32	$s13, $s13;
	cmp_lt_b1_s32	$c1, $s1, 43;
	cmov_b32	$s13, $c1, $s13, $s14;
	mov_b32	$s12, $s12;
	mul_ftz_f32	$s12, $s36, $s12;
	mov_b32	$s12, $s12;
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s35, $s11;
	mov_b32	$s11, $s11;
	cmp_lt_b1_s32	$c1, $s1, 44;
	cmov_b32	$s11, $c1, $s11, $s12;
	mov_b32	$s12, $s18;
	mov_b32	$s14, $s19;
	add_ftz_f32	$s14, $s14, 0F00000000;
	add_ftz_f32	$s12, $s14, $s12;
	mov_b32	$s14, $s27;
	add_ftz_f32	$s12, $s12, $s14;
	mov_b32	$s11, $s11;
	add_ftz_f32	$s11, $s12, $s11;
	mov_b32	$s12, $s13;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s15;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s12, $s17;
	mov_b32	$s13, $s22;
	mov_b32	$s14, $s20;
	mul_u32	$s15, $s3, $s0;
	add_u32	$s15, $s15, $s2;
	cvt_s64_s32	$d2, $s15;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d0, $d2;
	add_ftz_f32	$s11, $s11, $s14;
	add_ftz_f32	$s11, $s11, $s13;
	add_ftz_f32	$s11, $s11, $s12;
	st_global_align(4)_f32	$s11, [$d2];

@BB3_4:
	add_u32	$s11, $s3, 8;
	cmp_lt_b1_s32	$c1, $s11, $s7;
	and_b1	$c2, $c0, $c1;
	cmp_lt_b1_s32	$c1, $s1, 24;
	add_u32	$s9, $s1, $s9;
	add_u32	$s12, $s9, 16;
	and_b32	$s12, $s12, 63;
	activelanepermute_b32	$s19, $s10, $s12, 0, 0;
	activelanepermute_b32	$s20, $s8, $s12, 0, 0;
	add_u32	$s12, $s9, 17;
	and_b32	$s12, $s12, 63;
	add_u32	$s13, $s9, 28;
	add_u32	$s14, $s9, 26;
	activelanepermute_b32	$s18, $s10, $s12, 0, 0;
	add_u32	$s15, $s9, 38;
	add_u32	$s16, $s9, 37;
	add_u32	$s17, $s9, 36;
	add_u32	$s21, $s9, 27;
	add_u32	$s22, $s9, 18;
	and_b32	$s22, $s22, 63;
	and_b32	$s14, $s14, 63;
	and_b32	$s21, $s21, 63;
	and_b32	$s23, $s13, 63;
	and_b32	$s26, $s17, 63;
	and_b32	$s29, $s16, 63;
	and_b32	$s30, $s15, 63;
	cmp_ne_b1_b1	$c2, $c2, 1;
	activelanepermute_b32	$s25, $s8, $s12, 0, 0;
	activelanepermute_b32	$s27, $s10, $s22, 0, 0;
	activelanepermute_b32	$s28, $s8, $s22, 0, 0;
	activelanepermute_b32	$s12, $s10, $s14, 0, 0;
	activelanepermute_b32	$s13, $s8, $s14, 0, 0;
	activelanepermute_b32	$s14, $s10, $s21, 0, 0;
	activelanepermute_b32	$s15, $s8, $s21, 0, 0;
	activelanepermute_b32	$s16, $s10, $s23, 0, 0;
	activelanepermute_b32	$s17, $s8, $s23, 0, 0;
	activelanepermute_b32	$s24, $s10, $s26, 0, 0;
	activelanepermute_b32	$s26, $s8, $s26, 0, 0;
	activelanepermute_b32	$s22, $s10, $s29, 0, 0;
	activelanepermute_b32	$s23, $s8, $s29, 0, 0;
	activelanepermute_b32	$s10, $s10, $s30, 0, 0;
	activelanepermute_b32	$s21, $s8, $s30, 0, 0;
	cbr_b1	$c2, @BB3_6;
	// BB#5:
	ld_global_align(4)_f32	$s29, [$d1];
	ld_global_align(4)_f32	$s30, [$d1];
	ld_global_align(4)_f32	$s31, [$d1+4];
	ld_global_align(4)_f32	$s32, [$d1+4];
	ld_global_align(4)_f32	$s33, [$d1+8];
	ld_global_align(4)_f32	$s34, [$d1+8];
	ld_global_align(4)_f32	$s35, [$d1+12];
	ld_global_align(4)_f32	$s36, [$d1+12];
	ld_global_align(4)_f32	$s37, [$d1+16];
	ld_global_align(4)_f32	$s38, [$d1+16];
	ld_global_align(4)_f32	$s39, [$d1+20];
	ld_global_align(4)_f32	$s40, [$d1+20];
	ld_global_align(4)_f32	$s41, [$d1+24];
	ld_global_align(4)_f32	$s42, [$d1+24];
	ld_global_align(4)_f32	$s43, [$d1+28];
	ld_global_align(4)_f32	$s44, [$d1+28];
	ld_global_align(4)_f32	$s45, [$d1+32];
	ld_global_align(4)_f32	$s46, [$d1+32];
	mov_b32	$s28, $s28;
	mul_ftz_f32	$s28, $s34, $s28;
	mov_b32	$s28, $s28;
	mov_b32	$s27, $s27;
	mul_ftz_f32	$s27, $s33, $s27;
	mov_b32	$s27, $s27;
	cmp_lt_b1_s32	$c2, $s1, 38;
	cmov_b32	$s27, $c2, $s27, $s28;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s20, $s30, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s19, $s19;
	mul_ftz_f32	$s19, $s29, $s19;
	mov_b32	$s19, $s19;
	cmp_lt_b1_s32	$c2, $s1, 40;
	cmov_b32	$s19, $c2, $s19, $s20;
	mov_b32	$s20, $s25;
	mul_ftz_f32	$s20, $s32, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s18, $s18;
	mul_ftz_f32	$s18, $s31, $s18;
	mov_b32	$s18, $s18;
	cmp_lt_b1_s32	$c2, $s1, 39;
	cmov_b32	$s18, $c2, $s18, $s20;
	mov_b32	$s20, $s26;
	mul_ftz_f32	$s20, $s42, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s24, $s24;
	mul_ftz_f32	$s24, $s41, $s24;
	mov_b32	$s24, $s24;
	cmov_b32	$s20, $c1, $s24, $s20;
	mov_b32	$s23, $s23;
	mul_ftz_f32	$s23, $s44, $s23;
	mov_b32	$s23, $s23;
	mov_b32	$s22, $s22;
	mul_ftz_f32	$s22, $s43, $s22;
	mov_b32	$s22, $s22;
	cmp_lt_b1_s32	$c2, $s1, 23;
	cmov_b32	$s22, $c2, $s22, $s23;
	mov_b32	$s21, $s21;
	mul_ftz_f32	$s21, $s46, $s21;
	mov_b32	$s21, $s21;
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s45, $s10;
	mov_b32	$s10, $s10;
	cmp_lt_b1_s32	$c2, $s1, 22;
	cmov_b32	$s10, $c2, $s10, $s21;
	mov_b32	$s17, $s17;
	mul_ftz_f32	$s17, $s40, $s17;
	mov_b32	$s17, $s17;
	mov_b32	$s16, $s16;
	mul_ftz_f32	$s16, $s39, $s16;
	mov_b32	$s16, $s16;
	cmp_lt_b1_s32	$c2, $s1, 30;
	cmov_b32	$s16, $c2, $s16, $s17;
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s15, $s38, $s15;
	mov_b32	$s15, $s15;
	mov_b32	$s14, $s14;
	mul_ftz_f32	$s14, $s37, $s14;
	mov_b32	$s14, $s14;
	cmp_lt_b1_s32	$c2, $s1, 31;
	cmov_b32	$s14, $c2, $s14, $s15;
	mov_b32	$s13, $s13;
	mul_ftz_f32	$s13, $s36, $s13;
	mov_b32	$s13, $s13;
	mov_b32	$s12, $s12;
	mul_ftz_f32	$s12, $s35, $s12;
	mov_b32	$s12, $s12;
	cmp_lt_b1_s32	$c2, $s1, 32;
	cmov_b32	$s12, $c2, $s12, $s13;
	mov_b32	$s13, $s18;
	mov_b32	$s15, $s19;
	add_ftz_f32	$s15, $s15, 0F00000000;
	add_ftz_f32	$s13, $s15, $s13;
	mov_b32	$s15, $s27;
	add_ftz_f32	$s13, $s13, $s15;
	mov_b32	$s12, $s12;
	add_ftz_f32	$s12, $s13, $s12;
	mov_b32	$s13, $s14;
	add_ftz_f32	$s12, $s12, $s13;
	mov_b32	$s13, $s16;
	add_ftz_f32	$s12, $s12, $s13;
	mov_b32	$s10, $s10;
	mov_b32	$s13, $s22;
	mov_b32	$s14, $s20;
	mul_u32	$s11, $s11, $s0;
	add_u32	$s11, $s11, $s2;
	cvt_s64_s32	$d2, $s11;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d0, $d2;
	add_ftz_f32	$s11, $s12, $s14;
	add_ftz_f32	$s11, $s11, $s13;
	add_ftz_f32	$s10, $s11, $s10;
	st_global_align(4)_f32	$s10, [$d2];

@BB3_6:
	add_u32	$s36, $s9, 32;
	and_b32	$s10, $s36, 63;
	ld_global_align(4)_f32	$s17, [$d1];
	activelanepermute_b32	$s23, $s8, $s10, 0, 0;
	ld_global_align(4)_f32	$s20, [$d1];
	activelanepermute_b32	$s24, $s6, $s10, 0, 0;
	add_u32	$s10, $s9, 33;
	add_u32	$s11, $s9, 34;
	and_b32	$s11, $s11, 63;
	and_b32	$s10, $s10, 63;
	ld_global_align(4)_f32	$s15, [$d1+4];
	activelanepermute_b32	$s30, $s8, $s10, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+4];
	activelanepermute_b32	$s34, $s6, $s10, 0, 0;
	ld_global_align(4)_f32	$s19, [$d1+8];
	activelanepermute_b32	$s27, $s8, $s11, 0, 0;
	ld_global_align(4)_f32	$s21, [$d1+8];
	activelanepermute_b32	$s31, $s6, $s11, 0, 0;
	add_u32	$s10, $s9, 42;
	add_u32	$s11, $s9, 43;
	add_u32	$s12, $s9, 44;
	and_b32	$s22, $s12, 63;
	and_b32	$s11, $s11, 63;
	and_b32	$s10, $s10, 63;
	ld_global_align(4)_f32	$s13, [$d1+12];
	activelanepermute_b32	$s28, $s8, $s10, 0, 0;
	ld_global_align(4)_f32	$s16, [$d1+12];
	activelanepermute_b32	$s32, $s6, $s10, 0, 0;
	activelanepermute_b32	$s10, $s6, 0, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+16];
	activelanepermute_b32	$s29, $s8, $s11, 0, 0;
	ld_global_align(4)_f32	$s14, [$d1+16];
	activelanepermute_b32	$s33, $s6, $s11, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+20];
	activelanepermute_b32	$s25, $s8, $s22, 0, 0;
	ld_global_align(4)_f32	$s11, [$d1+20];
	activelanepermute_b32	$s26, $s6, $s22, 0, 0;
	add_u32	$s22, $s9, 52;
	and_b32	$s37, $s22, 63;
	activelanepermute_b32	$s35, $s8, $s37, 0, 0;
	activelanepermute_b32	$s22, $s6, $s37, 0, 0;
	activelanepermute_b32	$s37, $s4, $s37, 0, 0;
	activelanepermute_b32	$s38, $s4, 0, 0, 0;
	cmp_gt_b1_s32	$c2, $s1, 9;
	cbr_b1	$c2, @BB3_8;
	// BB#7:
	ld_global_align(4)_f32	$s22, [$d1+24];
	mov_b32	$s35, $s35;
	mul_ftz_f32	$s22, $s22, $s35;
	br	@BB3_9;

@BB3_8:
	ld_global_align(4)_f32	$s35, [$d1+24];
	ld_global_align(4)_f32	$s38, [$d1+24];
	mov_b32	$s37, $s37;
	mul_ftz_f32	$s37, $s38, $s37;
	mov_b32	$s37, $s37;
	mov_b32	$s22, $s22;
	mul_ftz_f32	$s22, $s35, $s22;
	mov_b32	$s22, $s22;
	cmp_lt_b1_s32	$c2, $s1, 62;
	cmov_b32	$s22, $c2, $s22, $s37;
	mov_b32	$s22, $s22;

@BB3_9:
	add_u32	$s35, $s36, 21;
	and_b32	$s38, $s35, 63;
	activelanepermute_b32	$s37, $s8, $s38, 0, 0;
	activelanepermute_b32	$s35, $s6, $s38, 0, 0;
	activelanepermute_b32	$s38, $s4, $s38, 0, 0;
	cmp_gt_b1_s32	$c2, $s1, 8;
	cbr_b1	$c2, @BB3_11;
	// BB#10:
	ld_global_align(4)_f32	$s35, [$d1+28];
	mov_b32	$s37, $s37;
	mul_ftz_f32	$s35, $s35, $s37;
	br	@BB3_12;

@BB3_11:
	ld_global_align(4)_f32	$s37, [$d1+28];
	ld_global_align(4)_f32	$s39, [$d1+28];
	mov_b32	$s38, $s38;
	mul_ftz_f32	$s38, $s39, $s38;
	mov_b32	$s38, $s38;
	mov_b32	$s35, $s35;
	mul_ftz_f32	$s35, $s37, $s35;
	mov_b32	$s35, $s35;
	cmp_lt_b1_s32	$c2, $s1, 61;
	cmov_b32	$s35, $c2, $s35, $s38;
	mov_b32	$s35, $s35;

@BB3_12:
	add_u32	$s36, $s36, 22;
	and_b32	$s37, $s36, 63;
	activelanepermute_b32	$s36, $s8, $s37, 0, 0;
	activelanepermute_b32	$s8, $s6, $s37, 0, 0;
	activelanepermute_b32	$s37, $s4, $s37, 0, 0;
	cmp_gt_b1_s32	$c2, $s1, 7;
	cbr_b1	$c2, @BB3_14;
	// BB#13:
	ld_global_align(4)_f32	$s8, [$d1+32];
	mov_b32	$s36, $s36;
	mul_ftz_f32	$s8, $s8, $s36;
	br	@BB3_15;

@BB3_14:
	ld_global_align(4)_f32	$s36, [$d1+32];
	ld_global_align(4)_f32	$s38, [$d1+32];
	mov_b32	$s37, $s37;
	mul_ftz_f32	$s37, $s38, $s37;
	mov_b32	$s37, $s37;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s36, $s8;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c2, $s1, 60;
	cmov_b32	$s8, $c2, $s8, $s37;
	mov_b32	$s8, $s8;

@BB3_15:
	add_u32	$s36, $s3, 16;
	cmp_lt_b1_s32	$c2, $s36, $s7;
	and_b1	$c2, $c0, $c2;
	cmp_ne_b1_b1	$c2, $c2, 1;
	cbr_b1	$c2, @BB3_17;
	// BB#16:
	mov_b32	$s23, $s23;
	mov_b32	$s24, $s24;
	mul_ftz_f32	$s17, $s17, $s23;
	mul_ftz_f32	$s20, $s20, $s24;
	mov_b32	$s23, $s30;
	mov_b32	$s24, $s34;
	mov_b32	$s17, $s17;
	mov_b32	$s20, $s20;
	cmp_lt_b1_s32	$c2, $s1, 26;
	mov_b32	$s27, $s27;
	mov_b32	$s30, $s31;
	mul_ftz_f32	$s15, $s15, $s23;
	mul_ftz_f32	$s18, $s18, $s24;
	cmov_b32	$s17, $c2, $s17, $s20;
	mul_ftz_f32	$s19, $s19, $s27;
	mul_ftz_f32	$s20, $s21, $s30;
	mov_b32	$s15, $s15;
	mov_b32	$s18, $s18;
	cmp_lt_b1_s32	$c2, $s1, 25;
	mov_b32	$s21, $s28;
	mov_b32	$s23, $s32;
	mov_b32	$s19, $s19;
	mov_b32	$s20, $s20;
	cmov_b32	$s15, $c2, $s15, $s18;
	mov_b32	$s18, $s29;
	mov_b32	$s24, $s33;
	mul_ftz_f32	$s13, $s13, $s21;
	mul_ftz_f32	$s16, $s16, $s23;
	mov_b32	$s17, $s17;
	cmov_b32	$s19, $c1, $s19, $s20;
	mov_b32	$s20, $s25;
	mov_b32	$s21, $s26;
	mul_ftz_f32	$s12, $s12, $s18;
	mul_ftz_f32	$s14, $s14, $s24;
	mov_b32	$s13, $s13;
	mov_b32	$s16, $s16;
	cmp_lt_b1_s32	$c1, $s1, 18;
	add_ftz_f32	$s17, $s17, 0F00000000;
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s10, $s10, $s20;
	mul_ftz_f32	$s11, $s11, $s21;
	mov_b32	$s12, $s12;
	mov_b32	$s14, $s14;
	cmp_lt_b1_s32	$c2, $s1, 17;
	cmov_b32	$s13, $c1, $s13, $s16;
	add_ftz_f32	$s15, $s17, $s15;
	mov_b32	$s16, $s19;
	mov_b32	$s10, $s10;
	mov_b32	$s11, $s11;
	cmp_lt_b1_s32	$c1, $s1, 16;
	cmov_b32	$s12, $c2, $s12, $s14;
	add_ftz_f32	$s14, $s15, $s16;
	mov_b32	$s13, $s13;
	cmov_b32	$s10, $c1, $s10, $s11;
	add_ftz_f32	$s11, $s14, $s13;
	mov_b32	$s12, $s12;
	add_ftz_f32	$s11, $s11, $s12;
	mov_b32	$s10, $s10;
	add_ftz_f32	$s10, $s11, $s10;
	add_ftz_f32	$s10, $s10, $s22;
	add_ftz_f32	$s10, $s10, $s35;
	add_ftz_f32	$s8, $s10, $s8;
	mul_u32	$s10, $s36, $s0;
	add_u32	$s10, $s10, $s2;
	cvt_s64_s32	$d2, $s10;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d0, $d2;
	st_global_align(4)_f32	$s8, [$d2];

@BB3_17:
	add_u32	$s21, $s9, 48;
	and_b32	$s8, $s21, 63;
	ld_global_align(4)_f32	$s10, [$d1];
	activelanepermute_b32	$s15, $s6, $s8, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1];
	activelanepermute_b32	$s16, $s4, $s8, 0, 0;
	add_u32	$s8, $s9, 49;
	and_b32	$s12, $s8, 63;
	ld_global_align(4)_f32	$s8, [$d1+4];
	activelanepermute_b32	$s17, $s6, $s12, 0, 0;
	add_u32	$s11, $s9, 50;
	and_b32	$s19, $s11, 63;
	ld_global_align(4)_f32	$s11, [$d1+4];
	activelanepermute_b32	$s20, $s4, $s12, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+8];
	activelanepermute_b32	$s18, $s6, $s19, 0, 0;
	ld_global_align(4)_f32	$s14, [$d1+8];
	activelanepermute_b32	$s19, $s4, $s19, 0, 0;
	add_u32	$s9, $s9, 58;
	and_b32	$s23, $s9, 63;
	activelanepermute_b32	$s22, $s6, $s23, 0, 0;
	activelanepermute_b32	$s9, $s4, $s23, 0, 0;
	activelanepermute_b32	$s23, $s5, $s23, 0, 0;
	cmp_gt_b1_s32	$c1, $s1, 5;
	cbr_b1	$c1, @BB3_19;
	// BB#18:
	ld_global_align(4)_f32	$s9, [$d1+12];
	mov_b32	$s22, $s22;
	mul_ftz_f32	$s9, $s9, $s22;
	br	@BB3_20;

@BB3_19:
	ld_global_align(4)_f32	$s22, [$d1+12];
	ld_global_align(4)_f32	$s24, [$d1+12];
	mov_b32	$s23, $s23;
	mul_ftz_f32	$s23, $s24, $s23;
	mov_b32	$s23, $s23;
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s9, $s22, $s9;
	mov_b32	$s9, $s9;
	cmp_lt_b1_s32	$c1, $s1, 56;
	cmov_b32	$s9, $c1, $s9, $s23;
	mov_b32	$s9, $s9;

@BB3_20:
	add_u32	$s22, $s21, 11;
	and_b32	$s24, $s22, 63;
	activelanepermute_b32	$s23, $s6, $s24, 0, 0;
	activelanepermute_b32	$s22, $s4, $s24, 0, 0;
	activelanepermute_b32	$s24, $s5, $s24, 0, 0;
	cmp_gt_b1_s32	$c1, $s1, 4;
	cbr_b1	$c1, @BB3_22;
	// BB#21:
	ld_global_align(4)_f32	$s22, [$d1+16];
	mov_b32	$s23, $s23;
	mul_ftz_f32	$s22, $s22, $s23;
	br	@BB3_23;

@BB3_22:
	ld_global_align(4)_f32	$s23, [$d1+16];
	ld_global_align(4)_f32	$s25, [$d1+16];
	mov_b32	$s24, $s24;
	mul_ftz_f32	$s24, $s25, $s24;
	mov_b32	$s24, $s24;
	mov_b32	$s22, $s22;
	mul_ftz_f32	$s22, $s23, $s22;
	mov_b32	$s22, $s22;
	cmp_lt_b1_s32	$c1, $s1, 56;
	cmov_b32	$s22, $c1, $s22, $s24;
	mov_b32	$s22, $s22;

@BB3_23:
	add_u32	$s23, $s21, 12;
	and_b32	$s24, $s23, 63;
	activelanepermute_b32	$s23, $s6, $s24, 0, 0;
	activelanepermute_b32	$s6, $s4, $s24, 0, 0;
	activelanepermute_b32	$s24, $s5, $s24, 0, 0;
	cmp_gt_b1_s32	$c1, $s1, 3;
	cbr_b1	$c1, @BB3_25;
	// BB#24:
	ld_global_align(4)_f32	$s6, [$d1+20];
	mov_b32	$s23, $s23;
	mul_ftz_f32	$s6, $s6, $s23;
	br	@BB3_26;

@BB3_25:
	ld_global_align(4)_f32	$s23, [$d1+20];
	ld_global_align(4)_f32	$s25, [$d1+20];
	mov_b32	$s24, $s24;
	mul_ftz_f32	$s24, $s25, $s24;
	mov_b32	$s24, $s24;
	mov_b32	$s6, $s6;
	mul_ftz_f32	$s6, $s23, $s6;
	mov_b32	$s6, $s6;
	cmp_lt_b1_s32	$c1, $s1, 56;
	cmov_b32	$s6, $c1, $s6, $s24;
	mov_b32	$s6, $s6;

@BB3_26:
	add_u32	$s3, $s3, 24;
	cmp_lt_b1_s32	$c1, $s3, $s7;
	and_b1	$c0, $c0, $c1;
	add_u32	$s7, $s21, 20;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s24, $s4, $s7, 0, 0;
	activelanepermute_b32	$s23, $s5, $s7, 0, 0;
	add_u32	$s7, $s21, 21;
	and_b32	$s25, $s7, 63;
	activelanepermute_b32	$s7, $s4, $s25, 0, 0;
	add_u32	$s21, $s21, 22;
	and_b32	$s26, $s21, 63;
	cmp_ne_b1_b1	$c0, $c0, 1;
	activelanepermute_b32	$s21, $s5, $s25, 0, 0;
	activelanepermute_b32	$s4, $s4, $s26, 0, 0;
	activelanepermute_b32	$s5, $s5, $s26, 0, 0;
	cbr_b1	$c0, @BB3_28;
	// BB#27:
	mov_b32	$s15, $s15;
	mov_b32	$s16, $s16;
	mul_ftz_f32	$s10, $s10, $s15;
	mul_ftz_f32	$s13, $s13, $s16;
	mov_b32	$s15, $s17;
	mov_b32	$s16, $s20;
	mov_b32	$s10, $s10;
	mov_b32	$s13, $s13;
	cmp_lt_b1_s32	$c0, $s1, 14;
	mov_b32	$s17, $s18;
	mov_b32	$s18, $s19;
	mul_ftz_f32	$s8, $s8, $s15;
	mul_ftz_f32	$s11, $s11, $s16;
	cmov_b32	$s10, $c0, $s10, $s13;
	mul_ftz_f32	$s12, $s12, $s17;
	mul_ftz_f32	$s13, $s14, $s18;
	mov_b32	$s8, $s8;
	mov_b32	$s11, $s11;
	cmp_lt_b1_s32	$c0, $s1, 13;
	mov_b32	$s12, $s12;
	mov_b32	$s13, $s13;
	cmp_lt_b1_s32	$c1, $s1, 12;
	cmov_b32	$s8, $c0, $s8, $s11;
	mov_b32	$s10, $s10;
	cmov_b32	$s11, $c1, $s12, $s13;
	add_ftz_f32	$s10, $s10, 0F00000000;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s8, $s10, $s8;
	mov_b32	$s10, $s11;
	add_ftz_f32	$s8, $s8, $s10;
	add_ftz_f32	$s8, $s8, $s9;
	add_ftz_f32	$s8, $s8, $s22;
	cmp_lt_b1_s32	$c0, $s1, 48;
	ld_global_align(4)_f32	$s1, [$d1+24];
	ld_global_align(4)_f32	$s9, [$d1+24];
	ld_global_align(4)_f32	$s10, [$d1+28];
	ld_global_align(4)_f32	$s11, [$d1+28];
	ld_global_align(4)_f32	$s12, [$d1+32];
	ld_global_align(4)_f32	$s13, [$d1+32];
	mov_b32	$s14, $s24;
	mov_b32	$s15, $s21;
	mul_ftz_f32	$s11, $s11, $s15;
	mul_ftz_f32	$s1, $s1, $s14;
	mov_b32	$s14, $s23;
	mul_ftz_f32	$s9, $s9, $s14;
	mov_b32	$s5, $s5;
	mul_ftz_f32	$s5, $s13, $s5;
	mov_b32	$s9, $s9;
	mov_b32	$s1, $s1;
	mov_b32	$s11, $s11;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s10, $s7;
	mov_b32	$s7, $s7;
	cmov_b32	$s7, $c0, $s7, $s11;
	cmov_b32	$s1, $c0, $s1, $s9;
	mov_b32	$s5, $s5;
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s12, $s4;
	mov_b32	$s4, $s4;
	cmov_b32	$s4, $c0, $s4, $s5;
	mov_b32	$s1, $s1;
	add_ftz_f32	$s5, $s8, $s6;
	add_ftz_f32	$s1, $s5, $s1;
	mov_b32	$s5, $s7;
	mul_u32	$s0, $s3, $s0;
	add_ftz_f32	$s1, $s1, $s5;
	mov_b32	$s3, $s4;
	add_ftz_f32	$s1, $s1, $s3;
	add_u32	$s0, $s0, $s2;
	cvt_s64_s32	$d1, $s0;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	st_global_align(4)_f32	$s1, [$d0];

@BB3_28:
	// %_ZZ18Stencil_Hcc_Shfl4xRN2hc5arrayIfLi1EEES2_S2_iiiENK3$_5clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ18Stencil_Hcc_Shfl2xRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__419__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	// BB#0:
	laneid_u32	$s1;
	add_u32	$s2, $s1, 8;
	cvt_s64_s32	$d0, $s2;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s0, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s3, $d0;
	add_u32	$s3, $s3, $s0;
	shr_s32	$s7, $s1, 3;
	workitemabsid_u32	$s0, 1;
	shl_u32	$s0, $s0, 1;
	and_b32	$s0, $s0, -16;
	add_u32	$s4, $s7, $s0;
	and_b32	$s12, $s4, -8;
	ld_kernarg_align(4)_width(all)_u32	$s6, [%__arg_p9];
	shl_u32	$s5, $s6, 1;
	ld_kernarg_align(4)_width(all)_u32	$s8, [%__arg_p11];
	ld_kernarg_align(4)_width(all)_u32	$s9, [%__arg_p10];
	add_u32	$s0, $s12, $s3;
	add_u32	$s10, $s0, 12;
	add_u32	$s0, $s5, $s9;
	add_u32	$s5, $s5, $s8;
	workitemabsid_u32	$s13, 0;
	mul_u32	$s3, $s3, 10;
	and_b32	$s14, $s13, -8;
	sub_u32	$s2, $s2, $s3;
	add_u32	$s11, $s2, $s14;
	cmp_lt_b1_s32	$c0, $s10, $s5;
	cmp_lt_b1_s32	$c1, $s11, $s0;
	and_b1	$c0, $c1, $c0;
	cvt_s64_s32	$d0, $s1;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s2, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s3, $d0;
	add_u32	$s2, $s3, $s2;
	add_u32	$s3, $s1, 4;
	add_u32	$s5, $s12, $s2;
	cvt_s64_s32	$d0, $s3;
	mul_u32	$s15, $s2, 10;
	mul_u64	$d3, $d0, 0x66666667;
	add_u32	$s2, $s13, $s6;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	mul_u32	$s13, $s5, $s0;
	sub_u32	$s15, $s1, $s15;
	shr_u64	$d4, $d3, 63;
	add_u32	$s5, $s4, $s6;
	add_u32	$s4, $s15, $s14;
	add_u32	$s4, $s4, $s13;
	cvt_u32_u64	$s13, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s15, $d3;
	add_u32	$s13, $s15, $s13;
	or_b32	$s12, $s12, 6;
	add_u32	$s12, $s13, $s12;
	mul_u32	$s12, $s12, $s0;
	cvt_s64_s32	$d3, $s4;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	mul_u32	$s4, $s13, 10;
	sub_u32	$s3, $s3, $s4;
	add_u32	$s3, $s3, $s14;
	cmp_ne_b1_b1	$c0, $c0, 1;
	ld_global_align(4)_u32	$s16, [$d3];
	add_u32	$s3, $s3, $s12;
	cvt_s64_s32	$d3, $s3;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s3, [$d3];
	// implicit-def: S4
	cbr_b1	$c0, @BB4_2;
	// BB#1:
	mul_u32	$s4, $s10, $s0;
	add_u32	$s4, $s4, $s11;
	cvt_s64_s32	$d3, $s4;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s4, [$d2];

@BB4_2:
	// %._crit_edge.i
	add_u32	$s9, $s9, $s6;
	add_u32	$s6, $s8, $s6;
	cmp_lt_b1_s32	$c1, $s5, $s6;
	cmp_lt_b1_s32	$c0, $s2, $s9;
	and_b1	$c1, $c0, $c1;
	shl_u32	$s7, $s7, 1;
	add_u32	$s8, $s7, $s1;
	and_b32	$s9, $s8, 63;
	activelanepermute_b32	$s15, $s16, $s9, 0, 0;
	activelanepermute_b32	$s17, $s3, $s9, 0, 0;
	add_u32	$s9, $s8, 1;
	and_b32	$s9, $s9, 63;
	activelanepermute_b32	$s14, $s16, $s9, 0, 0;
	add_u32	$s10, $s8, 22;
	add_u32	$s11, $s8, 21;
	add_u32	$s12, $s8, 20;
	add_u32	$s13, $s8, 12;
	add_u32	$s18, $s8, 11;
	add_u32	$s19, $s8, 10;
	add_u32	$s8, $s8, 2;
	and_b32	$s8, $s8, 63;
	and_b32	$s19, $s19, 63;
	and_b32	$s18, $s18, 63;
	and_b32	$s13, $s13, 63;
	and_b32	$s20, $s12, 63;
	and_b32	$s26, $s11, 63;
	and_b32	$s27, $s10, 63;
	cmp_ne_b1_b1	$c1, $c1, 1;
	activelanepermute_b32	$s22, $s3, $s9, 0, 0;
	activelanepermute_b32	$s24, $s16, $s8, 0, 0;
	activelanepermute_b32	$s25, $s3, $s8, 0, 0;
	activelanepermute_b32	$s8, $s16, $s19, 0, 0;
	activelanepermute_b32	$s9, $s3, $s19, 0, 0;
	activelanepermute_b32	$s10, $s16, $s18, 0, 0;
	activelanepermute_b32	$s11, $s3, $s18, 0, 0;
	activelanepermute_b32	$s12, $s16, $s13, 0, 0;
	activelanepermute_b32	$s13, $s3, $s13, 0, 0;
	activelanepermute_b32	$s21, $s16, $s20, 0, 0;
	activelanepermute_b32	$s23, $s3, $s20, 0, 0;
	activelanepermute_b32	$s19, $s16, $s26, 0, 0;
	activelanepermute_b32	$s20, $s3, $s26, 0, 0;
	activelanepermute_b32	$s16, $s16, $s27, 0, 0;
	activelanepermute_b32	$s18, $s3, $s27, 0, 0;
	cbr_b1	$c1, @BB4_4;
	// BB#3:
	ld_global_align(4)_f32	$s26, [$d1];
	ld_global_align(4)_f32	$s27, [$d1];
	ld_global_align(4)_f32	$s28, [$d1+4];
	ld_global_align(4)_f32	$s29, [$d1+4];
	ld_global_align(4)_f32	$s30, [$d1+8];
	ld_global_align(4)_f32	$s31, [$d1+8];
	ld_global_align(4)_f32	$s32, [$d1+12];
	ld_global_align(4)_f32	$s33, [$d1+12];
	ld_global_align(4)_f32	$s34, [$d1+16];
	ld_global_align(4)_f32	$s35, [$d1+16];
	ld_global_align(4)_f32	$s36, [$d1+20];
	ld_global_align(4)_f32	$s37, [$d1+20];
	ld_global_align(4)_f32	$s38, [$d1+24];
	ld_global_align(4)_f32	$s39, [$d1+24];
	ld_global_align(4)_f32	$s40, [$d1+28];
	ld_global_align(4)_f32	$s41, [$d1+28];
	ld_global_align(4)_f32	$s42, [$d1+32];
	ld_global_align(4)_f32	$s43, [$d1+32];
	mov_b32	$s25, $s25;
	mul_ftz_f32	$s25, $s31, $s25;
	mov_b32	$s25, $s25;
	mov_b32	$s24, $s24;
	mul_ftz_f32	$s24, $s30, $s24;
	mov_b32	$s24, $s24;
	cmp_lt_b1_s32	$c1, $s1, 50;
	cmov_b32	$s24, $c1, $s24, $s25;
	mov_b32	$s17, $s17;
	mul_ftz_f32	$s17, $s27, $s17;
	mov_b32	$s17, $s17;
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s15, $s26, $s15;
	mov_b32	$s15, $s15;
	cmp_lt_b1_s32	$c1, $s1, 52;
	cmov_b32	$s15, $c1, $s15, $s17;
	mov_b32	$s17, $s22;
	mul_ftz_f32	$s17, $s29, $s17;
	mov_b32	$s17, $s17;
	mov_b32	$s14, $s14;
	mul_ftz_f32	$s14, $s28, $s14;
	mov_b32	$s14, $s14;
	cmp_lt_b1_s32	$c1, $s1, 51;
	cmov_b32	$s14, $c1, $s14, $s17;
	mov_b32	$s17, $s23;
	mul_ftz_f32	$s17, $s39, $s17;
	mov_b32	$s17, $s17;
	mov_b32	$s21, $s21;
	mul_ftz_f32	$s21, $s38, $s21;
	mov_b32	$s21, $s21;
	cmp_lt_b1_s32	$c1, $s1, 36;
	cmov_b32	$s17, $c1, $s21, $s17;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s20, $s41, $s20;
	mov_b32	$s20, $s20;
	mov_b32	$s19, $s19;
	mul_ftz_f32	$s19, $s40, $s19;
	mov_b32	$s19, $s19;
	cmp_lt_b1_s32	$c1, $s1, 35;
	cmov_b32	$s19, $c1, $s19, $s20;
	mov_b32	$s18, $s18;
	mul_ftz_f32	$s18, $s43, $s18;
	mov_b32	$s18, $s18;
	mov_b32	$s16, $s16;
	mul_ftz_f32	$s16, $s42, $s16;
	mov_b32	$s16, $s16;
	cmp_lt_b1_s32	$c1, $s1, 34;
	cmov_b32	$s16, $c1, $s16, $s18;
	mov_b32	$s13, $s13;
	mul_ftz_f32	$s13, $s37, $s13;
	mov_b32	$s13, $s13;
	mov_b32	$s12, $s12;
	mul_ftz_f32	$s12, $s36, $s12;
	mov_b32	$s12, $s12;
	cmp_lt_b1_s32	$c1, $s1, 42;
	cmov_b32	$s12, $c1, $s12, $s13;
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s35, $s11;
	mov_b32	$s11, $s11;
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s34, $s10;
	mov_b32	$s10, $s10;
	cmp_lt_b1_s32	$c1, $s1, 43;
	cmov_b32	$s10, $c1, $s10, $s11;
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s9, $s33, $s9;
	mov_b32	$s9, $s9;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s32, $s8;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c1, $s1, 44;
	cmov_b32	$s8, $c1, $s8, $s9;
	mov_b32	$s9, $s14;
	mov_b32	$s11, $s15;
	add_ftz_f32	$s11, $s11, 0F00000000;
	add_ftz_f32	$s9, $s11, $s9;
	mov_b32	$s11, $s24;
	add_ftz_f32	$s9, $s9, $s11;
	mov_b32	$s8, $s8;
	add_ftz_f32	$s8, $s9, $s8;
	mov_b32	$s9, $s10;
	add_ftz_f32	$s8, $s8, $s9;
	mov_b32	$s9, $s12;
	add_ftz_f32	$s8, $s8, $s9;
	mov_b32	$s9, $s16;
	mov_b32	$s10, $s19;
	mov_b32	$s11, $s17;
	mul_u32	$s12, $s5, $s0;
	add_u32	$s12, $s12, $s2;
	cvt_s64_s32	$d2, $s12;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d0, $d2;
	add_ftz_f32	$s8, $s8, $s11;
	add_ftz_f32	$s8, $s8, $s10;
	add_ftz_f32	$s8, $s8, $s9;
	st_global_align(4)_f32	$s8, [$d2];

@BB4_4:
	add_u32	$s5, $s5, 8;
	cmp_lt_b1_s32	$c1, $s5, $s6;
	and_b1	$c0, $c0, $c1;
	add_u32	$s6, $s1, $s7;
	add_u32	$s7, $s6, 16;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s12, $s3, $s7, 0, 0;
	activelanepermute_b32	$s13, $s4, $s7, 0, 0;
	add_u32	$s7, $s6, 17;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s14, $s3, $s7, 0, 0;
	add_u32	$s8, $s6, 38;
	add_u32	$s9, $s6, 37;
	add_u32	$s10, $s6, 36;
	add_u32	$s11, $s6, 28;
	add_u32	$s15, $s6, 27;
	add_u32	$s16, $s6, 26;
	add_u32	$s6, $s6, 18;
	and_b32	$s6, $s6, 63;
	and_b32	$s16, $s16, 63;
	and_b32	$s15, $s15, 63;
	and_b32	$s11, $s11, 63;
	and_b32	$s18, $s10, 63;
	and_b32	$s22, $s9, 63;
	and_b32	$s23, $s8, 63;
	cmp_ne_b1_b1	$c0, $c0, 1;
	activelanepermute_b32	$s19, $s4, $s7, 0, 0;
	activelanepermute_b32	$s20, $s3, $s6, 0, 0;
	activelanepermute_b32	$s21, $s4, $s6, 0, 0;
	activelanepermute_b32	$s6, $s3, $s16, 0, 0;
	activelanepermute_b32	$s7, $s4, $s16, 0, 0;
	activelanepermute_b32	$s8, $s3, $s15, 0, 0;
	activelanepermute_b32	$s9, $s4, $s15, 0, 0;
	activelanepermute_b32	$s10, $s3, $s11, 0, 0;
	activelanepermute_b32	$s11, $s4, $s11, 0, 0;
	activelanepermute_b32	$s17, $s3, $s18, 0, 0;
	activelanepermute_b32	$s18, $s4, $s18, 0, 0;
	activelanepermute_b32	$s15, $s3, $s22, 0, 0;
	activelanepermute_b32	$s16, $s4, $s22, 0, 0;
	activelanepermute_b32	$s3, $s3, $s23, 0, 0;
	activelanepermute_b32	$s4, $s4, $s23, 0, 0;
	cbr_b1	$c0, @BB4_6;
	// BB#5:
	cmp_lt_b1_s32	$c0, $s1, 32;
	ld_global_align(4)_f32	$s22, [$d1];
	ld_global_align(4)_f32	$s23, [$d1];
	ld_global_align(4)_f32	$s24, [$d1+4];
	ld_global_align(4)_f32	$s25, [$d1+4];
	ld_global_align(4)_f32	$s26, [$d1+8];
	ld_global_align(4)_f32	$s27, [$d1+8];
	ld_global_align(4)_f32	$s28, [$d1+12];
	ld_global_align(4)_f32	$s29, [$d1+12];
	ld_global_align(4)_f32	$s30, [$d1+16];
	ld_global_align(4)_f32	$s31, [$d1+16];
	ld_global_align(4)_f32	$s32, [$d1+20];
	ld_global_align(4)_f32	$s33, [$d1+20];
	ld_global_align(4)_f32	$s34, [$d1+24];
	ld_global_align(4)_f32	$s35, [$d1+24];
	ld_global_align(4)_f32	$s36, [$d1+28];
	ld_global_align(4)_f32	$s37, [$d1+28];
	ld_global_align(4)_f32	$s38, [$d1+32];
	ld_global_align(4)_f32	$s39, [$d1+32];
	mov_b32	$s21, $s21;
	mul_ftz_f32	$s21, $s27, $s21;
	mov_b32	$s21, $s21;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s20, $s26, $s20;
	mov_b32	$s20, $s20;
	cmp_lt_b1_s32	$c1, $s1, 38;
	cmov_b32	$s20, $c1, $s20, $s21;
	mov_b32	$s19, $s19;
	mul_ftz_f32	$s19, $s25, $s19;
	mov_b32	$s19, $s19;
	mov_b32	$s14, $s14;
	mul_ftz_f32	$s14, $s24, $s14;
	mov_b32	$s14, $s14;
	cmp_lt_b1_s32	$c1, $s1, 39;
	cmov_b32	$s14, $c1, $s14, $s19;
	mov_b32	$s13, $s13;
	mul_ftz_f32	$s13, $s23, $s13;
	mov_b32	$s13, $s13;
	mov_b32	$s12, $s12;
	mul_ftz_f32	$s12, $s22, $s12;
	mov_b32	$s12, $s12;
	cmp_lt_b1_s32	$c1, $s1, 40;
	cmov_b32	$s12, $c1, $s12, $s13;
	mov_b32	$s13, $s18;
	mul_ftz_f32	$s13, $s35, $s13;
	mov_b32	$s13, $s13;
	mov_b32	$s17, $s17;
	mul_ftz_f32	$s17, $s34, $s17;
	mov_b32	$s17, $s17;
	cmp_lt_b1_s32	$c1, $s1, 24;
	cmov_b32	$s13, $c1, $s17, $s13;
	mov_b32	$s16, $s16;
	mul_ftz_f32	$s16, $s37, $s16;
	mov_b32	$s16, $s16;
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s15, $s36, $s15;
	mov_b32	$s15, $s15;
	cmp_lt_b1_s32	$c1, $s1, 23;
	cmov_b32	$s15, $c1, $s15, $s16;
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s39, $s4;
	mov_b32	$s4, $s4;
	mov_b32	$s3, $s3;
	mul_ftz_f32	$s3, $s38, $s3;
	mov_b32	$s3, $s3;
	cmp_lt_b1_s32	$c1, $s1, 22;
	cmov_b32	$s3, $c1, $s3, $s4;
	mov_b32	$s4, $s11;
	mul_ftz_f32	$s4, $s33, $s4;
	mov_b32	$s4, $s4;
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s32, $s10;
	mov_b32	$s10, $s10;
	cmp_lt_b1_s32	$c1, $s1, 30;
	cmov_b32	$s4, $c1, $s10, $s4;
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s9, $s31, $s9;
	mov_b32	$s9, $s9;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s30, $s8;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c1, $s1, 31;
	cmov_b32	$s1, $c1, $s8, $s9;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s29, $s7;
	mov_b32	$s7, $s7;
	mov_b32	$s6, $s6;
	mul_ftz_f32	$s6, $s28, $s6;
	mov_b32	$s6, $s6;
	cmov_b32	$s6, $c0, $s6, $s7;
	mov_b32	$s7, $s12;
	add_ftz_f32	$s8, 0F00000000, $s0;
	add_ftz_f32	$s7, $s7, $s8;
	mov_b32	$s8, $s14;
	add_ftz_f32	$s7, $s7, $s8;
	mov_b32	$s8, $s20;
	add_ftz_f32	$s7, $s7, $s8;
	mov_b32	$s6, $s6;
	add_ftz_f32	$s6, $s7, $s6;
	mov_b32	$s1, $s1;
	add_ftz_f32	$s1, $s6, $s1;
	mov_b32	$s4, $s4;
	add_ftz_f32	$s1, $s1, $s4;
	mov_b32	$s3, $s3;
	mov_b32	$s4, $s15;
	mov_b32	$s6, $s13;
	mul_u32	$s0, $s5, $s0;
	add_u32	$s0, $s0, $s2;
	cvt_s64_s32	$d1, $s0;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	add_ftz_f32	$s0, $s1, $s6;
	add_ftz_f32	$s0, $s0, $s4;
	add_ftz_f32	$s0, $s0, $s3;
	st_global_align(4)_f32	$s0, [$d0];

@BB4_6:
	// %_ZZ18Stencil_Hcc_Shfl2xRN2hc5arrayIfLi1EEES2_S2_iiiENK3$_4clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ17Stencil_Hcc_Shfl2RN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__319__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	align(4) spill_u8 %__spillStack[36];
	// BB#0:
	laneid_u32	$s0;
	add_u32	$s2, $s0, 8;
	cvt_s64_s32	$d0, $s2;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s1, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s3, $d0;
	add_u32	$s3, $s3, $s1;
	shr_s32	$s4, $s0, 3;
	workitemabsid_u32	$s1, 1;
	shl_u32	$s1, $s1, 1;
	and_b32	$s1, $s1, -16;
	add_u32	$s7, $s4, $s1;
	and_b32	$s8, $s7, -8;
	ld_kernarg_align(4)_width(all)_u32	$s40, [%__arg_p9];
	shl_u32	$s6, $s40, 1;
	ld_kernarg_align(4)_width(all)_u32	$s41, [%__arg_p11];
	ld_kernarg_align(4)_width(all)_u32	$s42, [%__arg_p10];
	add_u32	$s1, $s8, $s3;
	add_u32	$s5, $s1, 12;
	add_u32	$s1, $s6, $s42;
	add_u32	$s9, $s6, $s41;
	workitemabsid_u32	$s10, 0;
	mul_u32	$s3, $s3, 10;
	and_b32	$s11, $s10, -8;
	sub_u32	$s2, $s2, $s3;
	add_u32	$s6, $s2, $s11;
	cmp_lt_b1_s32	$c0, $s5, $s9;
	cmp_lt_b1_s32	$c1, $s6, $s1;
	and_b1	$c0, $c1, $c0;
	cvt_s64_s32	$d0, $s0;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s2, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s3, $d0;
	add_u32	$s2, $s3, $s2;
	add_u32	$s9, $s0, 4;
	add_u32	$s3, $s8, $s2;
	cvt_s64_s32	$d0, $s9;
	mul_u32	$s12, $s2, 10;
	mul_u64	$d3, $d0, 0x66666667;
	add_u32	$s33, $s10, $s40;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	mul_u32	$s10, $s3, $s1;
	sub_u32	$s12, $s0, $s12;
	shr_u64	$d4, $d3, 63;
	add_u32	$s3, $s7, $s40;
	add_u32	$s7, $s12, $s11;
	add_u32	$s7, $s7, $s10;
	cvt_u32_u64	$s10, $d4;
	shr_s64	$d3, $d3, 34;
	cvt_u32_u64	$s12, $d3;
	add_u32	$s10, $s12, $s10;
	or_b32	$s8, $s8, 6;
	add_u32	$s8, $s10, $s8;
	mul_u32	$s8, $s8, $s1;
	cvt_s64_s32	$d3, $s7;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	mul_u32	$s7, $s10, 10;
	sub_u32	$s7, $s9, $s7;
	add_u32	$s7, $s7, $s11;
	cmp_ne_b1_b1	$c0, $c0, 1;
	ld_global_align(4)_u32	$s17, [$d3];
	add_u32	$s7, $s7, $s8;
	cvt_s64_s32	$d3, $s7;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s25, [$d3];
	// implicit-def: S39
	cbr_b1	$c0, @BB5_2;
	// BB#1:
	mul_u32	$s5, $s5, $s1;
	add_u32	$s5, $s5, $s6;
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s39, [$d2];

@BB5_2:
	// %._crit_edge.i
	shl_u32	$s4, $s4, 1;
	add_u32	$s34, $s4, $s0;
	and_b32	$s5, $s34, 63;
	activelanepermute_b32	$s47, $s17, $s5, 0, 0;
	activelanepermute_b32	$s48, $s25, $s5, 0, 0;
	add_u32	$s58, $s0, $s4;
	add_u32	$s4, $s58, 16;
	and_b32	$s7, $s4, 63;
	ld_global_align(4)_f32	$s6, [$d1];
	activelanepermute_b32	$s4, $s25, $s7, 0, 0;
	ld_global_align(4)_f32	$s8, [$d1];
	activelanepermute_b32	$s5, $s39, $s7, 0, 0;
	add_u32	$s7, $s58, 17;
	and_b32	$s10, $s7, 63;
	add_u32	$s7, $s34, 1;
	and_b32	$s7, $s7, 63;
	activelanepermute_b32	$s49, $s17, $s7, 0, 0;
	activelanepermute_b32	$s50, $s25, $s7, 0, 0;
	ld_global_align(4)_f32	$s9, [$d1+4];
	activelanepermute_b32	$s7, $s25, $s10, 0, 0;
	ld_global_align(4)_f32	$s11, [$d1+4];
	activelanepermute_b32	$s20, $s39, $s10, 0, 0;
	add_u32	$s10, $s58, 18;
	and_b32	$s12, $s10, 63;
	add_u32	$s10, $s34, 2;
	and_b32	$s10, $s10, 63;
	activelanepermute_b32	$s53, $s17, $s10, 0, 0;
	activelanepermute_b32	$s54, $s25, $s10, 0, 0;
	ld_global_align(4)_f32	$s10, [$d1+8];
	activelanepermute_b32	$s27, $s25, $s12, 0, 0;
	ld_global_align(4)_f32	$s18, [$d1+8];
	activelanepermute_b32	$s28, $s39, $s12, 0, 0;
	add_u32	$s12, $s58, 26;
	and_b32	$s13, $s12, 63;
	add_u32	$s12, $s34, 10;
	and_b32	$s12, $s12, 63;
	activelanepermute_b32	$s44, $s17, $s12, 0, 0;
	activelanepermute_b32	$s45, $s25, $s12, 0, 0;
	ld_global_align(4)_f32	$s12, [$d1+12];
	activelanepermute_b32	$s29, $s25, $s13, 0, 0;
	ld_global_align(4)_f32	$s19, [$d1+12];
	activelanepermute_b32	$s31, $s39, $s13, 0, 0;
	add_u32	$s13, $s58, 27;
	and_b32	$s14, $s13, 63;
	add_u32	$s13, $s34, 11;
	and_b32	$s13, $s13, 63;
	activelanepermute_b32	$s62, $s17, $s13, 0, 0;
	activelanepermute_b32	$s46, $s25, $s13, 0, 0;
	ld_global_align(4)_f32	$s13, [$d1+16];
	activelanepermute_b32	$s30, $s25, $s14, 0, 0;
	ld_global_align(4)_f32	$s21, [$d1+16];
	activelanepermute_b32	$s36, $s39, $s14, 0, 0;
	add_u32	$s14, $s58, 28;
	and_b32	$s15, $s14, 63;
	add_u32	$s14, $s34, 12;
	and_b32	$s14, $s14, 63;
	activelanepermute_b32	$s51, $s17, $s14, 0, 0;
	activelanepermute_b32	$s52, $s25, $s14, 0, 0;
	ld_global_align(4)_f32	$s14, [$d1+20];
	activelanepermute_b32	$s32, $s25, $s15, 0, 0;
	ld_global_align(4)_f32	$s22, [$d1+20];
	activelanepermute_b32	$s37, $s39, $s15, 0, 0;
	add_u32	$s15, $s58, 36;
	and_b32	$s16, $s15, 63;
	add_u32	$s15, $s34, 20;
	and_b32	$s15, $s15, 63;
	activelanepermute_b32	$s55, $s17, $s15, 0, 0;
	activelanepermute_b32	$s57, $s25, $s15, 0, 0;
	ld_global_align(4)_f32	$s24, [$d1+24];
	activelanepermute_b32	$s2, $s25, $s16, 0, 0;
	ld_global_align(4)_f32	$s23, [$d1+24];
	activelanepermute_b32	$s43, $s39, $s16, 0, 0;
	add_u32	$s16, $s58, 37;
	and_b32	$s38, $s16, 63;
	add_u32	$s16, $s34, 21;
	and_b32	$s16, $s16, 63;
	activelanepermute_b32	$s56, $s17, $s16, 0, 0;
	activelanepermute_b32	$s59, $s25, $s16, 0, 0;
	ld_global_align(4)_f32	$s15, [$d1+28];
	activelanepermute_b32	$s35, $s25, $s38, 0, 0;
	ld_global_align(4)_f32	$s26, [$d1+28];
	activelanepermute_b32	$s38, $s39, $s38, 0, 0;
	add_u32	$s58, $s58, 38;
	and_b32	$s61, $s58, 63;
	add_u32	$s34, $s34, 22;
	and_b32	$s34, $s34, 63;
	activelanepermute_b32	$s58, $s17, $s34, 0, 0;
	activelanepermute_b32	$s60, $s25, $s34, 0, 0;
	ld_global_align(4)_f32	$s16, [$d1+32];
	activelanepermute_b32	$s17, $s25, $s61, 0, 0;
	ld_global_align(4)_f32	$s34, [$d1+32];
	activelanepermute_b32	$s25, $s39, $s61, 0, 0;
	add_u32	$s42, $s42, $s40;
	cmp_lt_b1_s32	$c0, $s33, $s42;
	add_u32	$s40, $s41, $s40;
	cmp_lt_b1_s32	$c1, $s3, $s40;
	and_b1	$c1, $c0, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	cbr_b1	$c1, @BB5_4;
	// BB#3:
	ld_global_align(4)_f32	$s41, [$d1];
	ld_global_align(4)_f32	$s42, [$d1];
	ld_global_align(4)_f32	$s61, [$d1+4];
	mov_b32	$s39, $s62;
	ld_global_align(4)_f32	$s62, [$d1+4];
	ld_global_align(4)_f32	$s63, [$d1+8];
	ld_global_align(4)_f32	$s64, [$d1+8];
	ld_global_align(4)_f32	$s65, [$d1+12];
	st_spill_align(4)_u32	$s65, [%__spillStack][32];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s66, [$d1+12];
	ld_global_align(4)_f32	$s67, [$d1+16];
	st_spill_align(4)_u32	$s24, [%__spillStack];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s15, [%__spillStack][4];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s4, [%__spillStack][8];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s4, [$d1+16];
	st_spill_align(4)_u32	$s34, [%__spillStack][12];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s5, [%__spillStack][16];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s5, [$d1+20];
	st_spill_align(4)_u32	$s22, [%__spillStack][24];
	// 4-byte Folded Spill
	mov_b32	$s34, $s31;
	st_spill_align(4)_u32	$s13, [%__spillStack][20];
	// 4-byte Folded Spill
	st_spill_align(4)_u32	$s6, [%__spillStack][28];
	// 4-byte Folded Spill
	ld_global_align(4)_f32	$s6, [$d1+20];
	mov_b32	$s31, $s17;
	mov_b32	$s24, $s35;
	mov_b32	$s35, $s27;
	mov_b32	$s27, $s7;
	ld_global_align(4)_f32	$s7, [$d1+24];
	mov_b32	$s22, $s14;
	mov_b32	$s17, $s8;
	ld_global_align(4)_f32	$s8, [$d1+24];
	mov_b32	$s14, $s2;
	mov_b32	$s15, $s9;
	ld_global_align(4)_f32	$s9, [$d1+28];
	mov_b32	$s13, $s10;
	ld_global_align(4)_f32	$s10, [$d1+28];
	mov_b32	$s2, $s18;
	mov_b32	$s18, $s11;
	ld_global_align(4)_f32	$s11, [$d1+32];
	mov_b32	$s65, $s45;
	mov_b32	$s45, $s12;
	ld_global_align(4)_f32	$s12, [$d1+32];
	mov_b32	$s54, $s54;
	mul_ftz_f32	$s54, $s64, $s54;
	mov_b32	$s54, $s54;
	mov_b32	$s53, $s53;
	mul_ftz_f32	$s53, $s63, $s53;
	mov_b32	$s53, $s53;
	cmp_lt_b1_s32	$c1, $s0, 50;
	cmov_b32	$s53, $c1, $s53, $s54;
	mov_b32	$s48, $s48;
	mul_ftz_f32	$s42, $s42, $s48;
	mov_b32	$s42, $s42;
	mov_b32	$s47, $s47;
	mul_ftz_f32	$s41, $s41, $s47;
	mov_b32	$s41, $s41;
	cmp_lt_b1_s32	$c1, $s0, 52;
	cmov_b32	$s41, $c1, $s41, $s42;
	mov_b32	$s42, $s50;
	mul_ftz_f32	$s42, $s62, $s42;
	mov_b32	$s42, $s42;
	mov_b32	$s47, $s49;
	mul_ftz_f32	$s47, $s61, $s47;
	mov_b32	$s47, $s47;
	cmp_lt_b1_s32	$c1, $s0, 51;
	cmov_b32	$s42, $c1, $s47, $s42;
	mov_b32	$s47, $s57;
	mul_ftz_f32	$s8, $s8, $s47;
	mov_b32	$s8, $s8;
	mov_b32	$s47, $s55;
	mul_ftz_f32	$s7, $s7, $s47;
	mov_b32	$s7, $s7;
	cmp_lt_b1_s32	$c1, $s0, 36;
	cmov_b32	$s7, $c1, $s7, $s8;
	mov_b32	$s8, $s59;
	mul_ftz_f32	$s8, $s10, $s8;
	mov_b32	$s8, $s8;
	mov_b32	$s10, $s56;
	mul_ftz_f32	$s9, $s9, $s10;
	mov_b32	$s9, $s9;
	cmp_lt_b1_s32	$c1, $s0, 35;
	cmov_b32	$s8, $c1, $s9, $s8;
	mov_b32	$s9, $s60;
	mul_ftz_f32	$s9, $s12, $s9;
	mov_b32	$s12, $s45;
	mov_b32	$s9, $s9;
	mov_b32	$s10, $s58;
	mul_ftz_f32	$s10, $s11, $s10;
	mov_b32	$s10, $s10;
	cmp_lt_b1_s32	$c1, $s0, 34;
	cmov_b32	$s9, $c1, $s10, $s9;
	mov_b32	$s10, $s52;
	mul_ftz_f32	$s6, $s6, $s10;
	mov_b32	$s6, $s6;
	mov_b32	$s10, $s51;
	mul_ftz_f32	$s5, $s5, $s10;
	mov_b32	$s5, $s5;
	cmp_lt_b1_s32	$c1, $s0, 42;
	cmov_b32	$s5, $c1, $s5, $s6;
	mov_b32	$s6, $s46;
	mul_ftz_f32	$s4, $s4, $s6;
	mov_b32	$s4, $s4;
	mov_b32	$s6, $s39;
	mul_ftz_f32	$s6, $s67, $s6;
	mov_b32	$s6, $s6;
	cmp_lt_b1_s32	$c1, $s0, 43;
	cmov_b32	$s4, $c1, $s6, $s4;
	mov_b32	$s6, $s65;
	mul_ftz_f32	$s6, $s66, $s6;
	mov_b32	$s6, $s6;
	mov_b32	$s10, $s44;
	ld_spill_align(4)_u32	$s11, [%__spillStack][32];
	// 4-byte Folded Reload
	mul_ftz_f32	$s10, $s11, $s10;
	mov_b32	$s10, $s10;
	cmp_lt_b1_s32	$c1, $s0, 44;
	cmov_b32	$s6, $c1, $s10, $s6;
	mov_b32	$s10, $s42;
	mov_b32	$s11, $s41;
	add_ftz_f32	$s11, $s11, 0F00000000;
	add_ftz_f32	$s10, $s11, $s10;
	mov_b32	$s11, $s53;
	add_ftz_f32	$s10, $s10, $s11;
	mov_b32	$s11, $s18;
	mov_b32	$s18, $s2;
	mov_b32	$s6, $s6;
	add_ftz_f32	$s6, $s10, $s6;
	mov_b32	$s10, $s13;
	mov_b32	$s4, $s4;
	add_ftz_f32	$s4, $s6, $s4;
	mov_b32	$s5, $s5;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s9;
	mov_b32	$s9, $s15;
	mov_b32	$s2, $s14;
	mov_b32	$s6, $s8;
	mov_b32	$s7, $s7;
	mul_u32	$s8, $s3, $s1;
	add_u32	$s8, $s8, $s33;
	cvt_s64_s32	$d1, $s8;
	mov_b32	$s8, $s17;
	mov_b32	$s14, $s22;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	add_ftz_f32	$s4, $s4, $s7;
	mov_b32	$s7, $s27;
	mov_b32	$s27, $s35;
	mov_b32	$s35, $s24;
	mov_b32	$s17, $s31;
	add_ftz_f32	$s4, $s4, $s6;
	ld_spill_align(4)_u32	$s6, [%__spillStack][28];
	// 4-byte Folded Reload
	ld_spill_align(4)_u32	$s13, [%__spillStack][20];
	// 4-byte Folded Reload
	mov_b32	$s31, $s34;
	ld_spill_align(4)_u32	$s22, [%__spillStack][24];
	// 4-byte Folded Reload
	add_ftz_f32	$s4, $s4, $s5;
	ld_spill_align(4)_u32	$s5, [%__spillStack][16];
	// 4-byte Folded Reload
	ld_spill_align(4)_u32	$s34, [%__spillStack][12];
	// 4-byte Folded Reload
	st_global_align(4)_f32	$s4, [$d1];
	ld_spill_align(4)_u32	$s4, [%__spillStack][8];
	// 4-byte Folded Reload
	ld_spill_align(4)_u32	$s15, [%__spillStack][4];
	// 4-byte Folded Reload
	ld_spill_align(4)_u32	$s24, [%__spillStack];
	// 4-byte Folded Reload

@BB5_4:
	add_u32	$s3, $s3, 8;
	cmp_lt_b1_s32	$c1, $s3, $s40;
	and_b1	$c0, $c0, $c1;
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB5_6;
	// BB#5:
	mov_b32	$s4, $s4;
	mov_b32	$s5, $s5;
	mul_ftz_f32	$s4, $s6, $s4;
	mul_ftz_f32	$s5, $s8, $s5;
	mov_b32	$s6, $s7;
	mov_b32	$s7, $s20;
	mov_b32	$s4, $s4;
	mov_b32	$s5, $s5;
	cmp_lt_b1_s32	$c0, $s0, 40;
	mul_ftz_f32	$s6, $s9, $s6;
	mul_ftz_f32	$s7, $s11, $s7;
	mov_b32	$s8, $s27;
	mov_b32	$s9, $s28;
	cmov_b32	$s4, $c0, $s4, $s5;
	mov_b32	$s5, $s6;
	mov_b32	$s6, $s7;
	cmp_lt_b1_s32	$c0, $s0, 39;
	mul_ftz_f32	$s7, $s10, $s8;
	mul_ftz_f32	$s8, $s18, $s9;
	mov_b32	$s9, $s31;
	mov_b32	$s10, $s29;
	cmov_b32	$s5, $c0, $s5, $s6;
	mov_b32	$s6, $s7;
	mov_b32	$s7, $s8;
	cmp_lt_b1_s32	$c0, $s0, 38;
	mul_ftz_f32	$s8, $s19, $s9;
	mul_ftz_f32	$s9, $s12, $s10;
	mov_b32	$s10, $s36;
	mov_b32	$s11, $s30;
	mov_b32	$s4, $s4;
	cmov_b32	$s6, $c0, $s6, $s7;
	mov_b32	$s7, $s9;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c0, $s0, 32;
	mul_ftz_f32	$s9, $s21, $s10;
	mul_ftz_f32	$s10, $s13, $s11;
	mov_b32	$s11, $s37;
	mov_b32	$s12, $s32;
	add_ftz_f32	$s4, $s4, 0F00000000;
	mov_b32	$s5, $s5;
	cmov_b32	$s7, $c0, $s7, $s8;
	mov_b32	$s8, $s10;
	mov_b32	$s9, $s9;
	cmp_lt_b1_s32	$c0, $s0, 31;
	mul_ftz_f32	$s10, $s22, $s11;
	mul_ftz_f32	$s11, $s14, $s12;
	mov_b32	$s12, $s43;
	mov_b32	$s13, $s2;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c0, $s8, $s9;
	mov_b32	$s8, $s11;
	mov_b32	$s9, $s10;
	cmp_lt_b1_s32	$c0, $s0, 30;
	mul_ftz_f32	$s10, $s23, $s12;
	mul_ftz_f32	$s11, $s24, $s13;
	mov_b32	$s12, $s38;
	mov_b32	$s13, $s35;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s7;
	cmov_b32	$s7, $c0, $s8, $s9;
	mov_b32	$s8, $s11;
	mov_b32	$s9, $s10;
	cmp_lt_b1_s32	$c0, $s0, 24;
	mul_ftz_f32	$s10, $s26, $s12;
	mul_ftz_f32	$s11, $s15, $s13;
	mov_b32	$s12, $s25;
	mov_b32	$s13, $s17;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s6, $c0, $s8, $s9;
	mov_b32	$s8, $s11;
	mov_b32	$s9, $s10;
	cmp_lt_b1_s32	$c0, $s0, 23;
	mul_ftz_f32	$s10, $s34, $s12;
	mul_ftz_f32	$s11, $s16, $s13;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s7;
	cmov_b32	$s7, $c0, $s8, $s9;
	cmp_lt_b1_s32	$c0, $s0, 22;
	mov_b32	$s0, $s11;
	mov_b32	$s8, $s10;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s6;
	cmov_b32	$s0, $c0, $s0, $s8;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s5, $s7;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s0, $s0;
	add_ftz_f32	$s0, $s4, $s0;
	mul_u32	$s1, $s3, $s1;
	add_u32	$s1, $s1, $s33;
	cvt_s64_s32	$d1, $s1;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	st_global_align(4)_f32	$s0, [$d0];

@BB5_6:
	// %_ZZ17Stencil_Hcc_Shfl2RN2hc5arrayIfLi1EEES2_S2_iiiENK3$_3clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ16Stencil_Hcc_ShflRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__219__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	// BB#0:
	laneid_u32	$s1;
	add_u32	$s2, $s1, 4;
	cvt_s64_s32	$d0, $s2;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s0, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s3, $d0;
	add_u32	$s3, $s3, $s0;
	workitemabsid_u32	$s9, 1;
	ld_kernarg_align(4)_width(all)_u32	$s5, [%__arg_p11];
	ld_kernarg_align(4)_width(all)_u32	$s4, [%__arg_p9];
	mul_u32	$s7, $s3, 10;
	shl_u32	$s8, $s4, 1;
	ld_kernarg_align(4)_width(all)_u32	$s6, [%__arg_p10];
	add_u32	$s0, $s8, $s6;
	add_u32	$s10, $s8, $s5;
	and_b32	$s11, $s9, -8;
	sub_u32	$s2, $s2, $s7;
	or_b32	$s8, $s11, 6;
	workitemabsid_u32	$s12, 0;
	and_b32	$s13, $s12, -8;
	add_u32	$s7, $s2, $s13;
	add_u32	$s8, $s3, $s8;
	cmp_lt_b1_s32	$c0, $s8, $s10;
	cmp_lt_b1_s32	$c1, $s7, $s0;
	and_b1	$c0, $c1, $c0;
	cvt_s64_s32	$d0, $s1;
	mul_u64	$d0, $d0, 0x66666667;
	shr_u64	$d1, $d0, 63;
	cvt_u32_u64	$s2, $d1;
	shr_s64	$d0, $d0, 34;
	cvt_u32_u64	$s3, $d0;
	add_u32	$s2, $s3, $s2;
	mul_u32	$s10, $s2, 10;
	add_u32	$s11, $s2, $s11;
	add_u32	$s3, $s9, $s4;
	add_u32	$s2, $s12, $s4;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p0];
	mul_u32	$s9, $s11, $s0;
	sub_u32	$s10, $s1, $s10;
	add_u32	$s10, $s10, $s13;
	add_u32	$s9, $s10, $s9;
	cvt_s64_s32	$d3, $s9;
	cmp_ne_b1_b1	$c0, $c0, 1;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_global_align(4)_u32	$s11, [$d3];
	// implicit-def: S13
	cbr_b1	$c0, @BB6_2;
	// BB#1:
	mul_u32	$s8, $s8, $s0;
	add_u32	$s7, $s8, $s7;
	cvt_s64_s32	$d3, $s7;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_global_align(4)_u32	$s13, [$d2];

@BB6_2:
	// %._crit_edge.i
	add_u32	$s5, $s5, $s4;
	add_u32	$s4, $s6, $s4;
	cmp_lt_b1_s32	$c0, $s2, $s4;
	cmp_lt_b1_s32	$c1, $s3, $s5;
	and_b1	$c0, $c1, $c0;
	shr_s32	$s4, $s1, 3;
	shl_u32	$s4, $s4, 1;
	add_u32	$s4, $s4, $s1;
	and_b32	$s5, $s4, 63;
	activelanepermute_b32	$s10, $s11, $s5, 0, 0;
	add_u32	$s6, $s4, 2;
	activelanepermute_b32	$s12, $s13, $s5, 0, 0;
	add_u32	$s5, $s4, 1;
	add_u32	$s7, $s4, 10;
	and_b32	$s6, $s6, 63;
	and_b32	$s5, $s5, 63;
	add_u32	$s8, $s4, 22;
	add_u32	$s9, $s4, 21;
	add_u32	$s14, $s4, 20;
	add_u32	$s15, $s4, 12;
	add_u32	$s4, $s4, 11;
	and_b32	$s7, $s7, 63;
	and_b32	$s16, $s4, 63;
	and_b32	$s17, $s15, 63;
	and_b32	$s14, $s14, 63;
	and_b32	$s22, $s9, 63;
	and_b32	$s23, $s8, 63;
	cmp_ne_b1_b1	$c0, $c0, 1;
	activelanepermute_b32	$s15, $s11, $s5, 0, 0;
	activelanepermute_b32	$s18, $s13, $s5, 0, 0;
	activelanepermute_b32	$s20, $s11, $s6, 0, 0;
	activelanepermute_b32	$s21, $s13, $s6, 0, 0;
	activelanepermute_b32	$s4, $s11, $s7, 0, 0;
	activelanepermute_b32	$s5, $s13, $s7, 0, 0;
	activelanepermute_b32	$s6, $s11, $s16, 0, 0;
	activelanepermute_b32	$s7, $s13, $s16, 0, 0;
	activelanepermute_b32	$s8, $s11, $s17, 0, 0;
	activelanepermute_b32	$s9, $s13, $s17, 0, 0;
	activelanepermute_b32	$s17, $s11, $s14, 0, 0;
	activelanepermute_b32	$s19, $s13, $s14, 0, 0;
	activelanepermute_b32	$s14, $s11, $s22, 0, 0;
	activelanepermute_b32	$s16, $s13, $s22, 0, 0;
	activelanepermute_b32	$s11, $s11, $s23, 0, 0;
	activelanepermute_b32	$s13, $s13, $s23, 0, 0;
	cbr_b1	$c0, @BB6_4;
	// BB#3:
	ld_global_align(4)_f32	$s22, [$d1];
	ld_global_align(4)_f32	$s23, [$d1];
	ld_global_align(4)_f32	$s24, [$d1+4];
	ld_global_align(4)_f32	$s25, [$d1+4];
	ld_global_align(4)_f32	$s26, [$d1+8];
	ld_global_align(4)_f32	$s27, [$d1+8];
	ld_global_align(4)_f32	$s28, [$d1+12];
	ld_global_align(4)_f32	$s29, [$d1+12];
	ld_global_align(4)_f32	$s30, [$d1+16];
	ld_global_align(4)_f32	$s31, [$d1+16];
	ld_global_align(4)_f32	$s32, [$d1+20];
	ld_global_align(4)_f32	$s33, [$d1+20];
	ld_global_align(4)_f32	$s34, [$d1+24];
	ld_global_align(4)_f32	$s35, [$d1+24];
	ld_global_align(4)_f32	$s36, [$d1+28];
	ld_global_align(4)_f32	$s37, [$d1+28];
	ld_global_align(4)_f32	$s38, [$d1+32];
	ld_global_align(4)_f32	$s39, [$d1+32];
	mov_b32	$s21, $s21;
	mul_ftz_f32	$s21, $s27, $s21;
	mov_b32	$s21, $s21;
	mov_b32	$s20, $s20;
	mul_ftz_f32	$s20, $s26, $s20;
	mov_b32	$s20, $s20;
	cmp_lt_b1_s32	$c0, $s1, 50;
	cmov_b32	$s20, $c0, $s20, $s21;
	mov_b32	$s12, $s12;
	mul_ftz_f32	$s12, $s23, $s12;
	mov_b32	$s12, $s12;
	mov_b32	$s10, $s10;
	mul_ftz_f32	$s10, $s22, $s10;
	mov_b32	$s10, $s10;
	cmp_lt_b1_s32	$c0, $s1, 52;
	cmov_b32	$s10, $c0, $s10, $s12;
	mov_b32	$s12, $s18;
	mul_ftz_f32	$s12, $s25, $s12;
	mov_b32	$s12, $s12;
	mov_b32	$s15, $s15;
	mul_ftz_f32	$s15, $s24, $s15;
	mov_b32	$s15, $s15;
	cmp_lt_b1_s32	$c0, $s1, 51;
	cmov_b32	$s12, $c0, $s15, $s12;
	mov_b32	$s15, $s19;
	mul_ftz_f32	$s15, $s35, $s15;
	mov_b32	$s15, $s15;
	mov_b32	$s17, $s17;
	mul_ftz_f32	$s17, $s34, $s17;
	mov_b32	$s17, $s17;
	cmp_lt_b1_s32	$c0, $s1, 36;
	cmov_b32	$s15, $c0, $s17, $s15;
	mov_b32	$s16, $s16;
	mul_ftz_f32	$s16, $s37, $s16;
	mov_b32	$s16, $s16;
	mov_b32	$s14, $s14;
	mul_ftz_f32	$s14, $s36, $s14;
	mov_b32	$s14, $s14;
	cmp_lt_b1_s32	$c0, $s1, 35;
	cmov_b32	$s14, $c0, $s14, $s16;
	mov_b32	$s13, $s13;
	mul_ftz_f32	$s13, $s39, $s13;
	mov_b32	$s13, $s13;
	mov_b32	$s11, $s11;
	mul_ftz_f32	$s11, $s38, $s11;
	mov_b32	$s11, $s11;
	cmp_lt_b1_s32	$c0, $s1, 34;
	cmov_b32	$s11, $c0, $s11, $s13;
	mov_b32	$s9, $s9;
	mul_ftz_f32	$s9, $s33, $s9;
	mov_b32	$s9, $s9;
	mov_b32	$s8, $s8;
	mul_ftz_f32	$s8, $s32, $s8;
	mov_b32	$s8, $s8;
	cmp_lt_b1_s32	$c0, $s1, 42;
	cmov_b32	$s8, $c0, $s8, $s9;
	mov_b32	$s7, $s7;
	mul_ftz_f32	$s7, $s31, $s7;
	mov_b32	$s7, $s7;
	mov_b32	$s6, $s6;
	mul_ftz_f32	$s6, $s30, $s6;
	mov_b32	$s6, $s6;
	cmp_lt_b1_s32	$c0, $s1, 43;
	cmov_b32	$s6, $c0, $s6, $s7;
	mov_b32	$s5, $s5;
	mul_ftz_f32	$s5, $s29, $s5;
	mov_b32	$s5, $s5;
	mov_b32	$s4, $s4;
	mul_ftz_f32	$s4, $s28, $s4;
	mov_b32	$s4, $s4;
	cmp_lt_b1_s32	$c0, $s1, 44;
	cmov_b32	$s1, $c0, $s4, $s5;
	mov_b32	$s4, $s12;
	mov_b32	$s5, $s10;
	add_ftz_f32	$s5, $s5, 0F00000000;
	add_ftz_f32	$s4, $s5, $s4;
	mov_b32	$s5, $s20;
	add_ftz_f32	$s4, $s4, $s5;
	mov_b32	$s1, $s1;
	add_ftz_f32	$s1, $s4, $s1;
	mov_b32	$s4, $s6;
	add_ftz_f32	$s1, $s1, $s4;
	mov_b32	$s4, $s8;
	add_ftz_f32	$s1, $s1, $s4;
	mov_b32	$s4, $s11;
	mov_b32	$s5, $s14;
	mov_b32	$s6, $s15;
	mul_u32	$s0, $s3, $s0;
	add_u32	$s0, $s0, $s2;
	cvt_s64_s32	$d1, $s0;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	add_ftz_f32	$s0, $s1, $s6;
	add_ftz_f32	$s0, $s0, $s5;
	add_ftz_f32	$s0, $s0, $s4;
	st_global_align(4)_f32	$s0, [$d0];

@BB6_4:
	// %_ZZ16Stencil_Hcc_ShflRN2hc5arrayIfLi1EEES2_S2_iiiENK3$_2clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	align(16) group_f32 %ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9[324];
	// BB#0:
	workitemid_u32	$s12, 0;
	ld_kernarg_align(4)_width(all)_u32	$s5, [%__arg_p9];
	workitemabsid_u32	$s0, 1;
	add_u32	$s2, $s0, $s5;
	shl_u32	$s0, $s5, 1;
	ld_kernarg_align(4)_width(all)_u32	$s6, [%__arg_p10];
	add_u32	$s9, $s0, $s6;
	mul_u32	$s13, $s2, $s9;
	workitemabsid_u32	$s1, 0;
	add_u32	$s7, $s1, $s5;
	add_u32	$s1, $s13, $s7;
	cvt_s64_s32	$d0, $s1;
	add_u32	$s1, $s12, $s5;
	workitemid_u32	$s11, 1;
	add_u32	$s3, $s11, $s5;
	add_u32	$s4, $s0, 16;
	mul_u32	$s0, $s3, $s4;
	shl_u64	$d1, $d0, 2;
	add_u32	$s8, $s0, $s1;
	ld_kernarg_align(4)_width(all)_u32	$s10, [%__arg_p11];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p6];
	ld_kernarg_align(8)_width(all)_u64	$d3, [%__arg_p0];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p3];
	shl_u32	$s8, $s8, 2;
	add_u64	$d4, $d3, $d1;
	ld_global_align(4)_f32	$s14, [$d4];
	st_group_align(4)_f32	$s14, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s8];
	cmp_ne_b1_s32	$c0, $s12, 0;
	cbr_b1	$c0, @BB7_2;
	// BB#1:
	add_u32	$s14, $s5, $s0;
	shl_u32	$s14, $s14, 2;
	add_u32	$s15, $s7, $s13;
	add_u32	$s15, $s15, -1;
	cvt_s64_s32	$d4, $s15;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s15, [$d4];
	st_group_align(4)_f32	$s15, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s14-4];

@BB7_2:
	cmp_ne_b1_s32	$c0, $s1, 16;
	cbr_b1	$c0, @BB7_4;
	// BB#3:
	add_u32	$s13, $s7, $s13;
	add_u32	$s13, $s13, 1;
	cvt_s64_s32	$d4, $s13;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s13, [$d4];
	shl_u32	$s14, $s0, 2;
	st_group_align(4)_f32	$s13, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s14+68];

@BB7_4:
	cmp_ne_b1_s32	$c0, $s11, 0;
	cbr_b1	$c0, @BB7_6;
	// BB#5:
	add_u32	$s13, $s5, -1;
	mul_u32	$s13, $s4, $s13;
	add_u32	$s13, $s1, $s13;
	add_u32	$s14, $s2, -1;
	shl_u32	$s13, $s13, 2;
	mul_u32	$s14, $s14, $s9;
	add_u32	$s14, $s14, $s7;
	cvt_s64_s32	$d4, $s14;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s14, [$d4];
	st_group_align(4)_f32	$s14, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s13];

@BB7_6:
	cmp_ne_b1_s32	$c0, $s3, 16;
	cbr_b1	$c0, @BB7_8;
	// BB#7:
	mul_u32	$s13, $s4, 17;
	add_u32	$s13, $s1, $s13;
	add_u32	$s14, $s2, 1;
	shl_u32	$s13, $s13, 2;
	mul_u32	$s14, $s14, $s9;
	add_u32	$s14, $s14, $s7;
	cvt_s64_s32	$d4, $s14;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s14, [$d4];
	st_group_align(4)_f32	$s14, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s13];

@BB7_8:
	cmp_eq_b1_s32	$c1, $s12, 0;
	cmp_eq_b1_s32	$c0, $s3, 16;
	or_b32	$s12, $s12, $s11;
	cmp_ne_b1_s32	$c2, $s12, 0;
	cbr_b1	$c2, @BB7_10;
	// BB#9:
	add_u32	$s12, $s2, -1;
	mul_u32	$s12, $s12, $s9;
	add_u32	$s13, $s5, -1;
	mul_u32	$s14, $s4, $s13;
	add_u32	$s13, $s14, $s13;
	shl_u32	$s13, $s13, 2;
	add_u32	$s12, $s7, $s12;
	add_u32	$s12, $s12, -1;
	cvt_s64_s32	$d4, $s12;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s12, [$d4];
	st_group_align(4)_f32	$s12, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s13];

@BB7_10:
	cmp_eq_b1_s32	$c2, $s1, 16;
	and_b1	$c1, $c1, $c0;
	cmp_ne_b1_b1	$c1, $c1, 1;
	cbr_b1	$c1, @BB7_12;
	// BB#11:
	add_u32	$s12, $s2, 1;
	mul_u32	$s12, $s12, $s9;
	mul_u32	$s13, $s4, 17;
	add_u32	$s13, $s5, $s13;
	shl_u32	$s13, $s13, 2;
	add_u32	$s12, $s7, $s12;
	add_u32	$s12, $s12, -1;
	cvt_s64_s32	$d4, $s12;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s12, [$d4];
	st_group_align(4)_f32	$s12, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s13-4];

@BB7_12:
	cmp_ne_b1_b1	$c1, $c2, 1;
	cbr_b1	$c1, @BB7_17;
	// BB#13:
	cmp_eq_b1_s32	$c1, $s11, 0;
	cmp_ne_b1_b1	$c1, $c1, 1;
	cbr_b1	$c1, @BB7_15;
	// BB#14:
	add_u32	$s11, $s2, -1;
	mul_u32	$s11, $s11, $s9;
	add_u32	$s12, $s5, -1;
	mul_u32	$s12, $s4, $s12;
	shl_u32	$s12, $s12, 2;
	add_u32	$s11, $s7, $s11;
	add_u32	$s11, $s11, 1;
	cvt_s64_s32	$d4, $s11;
	shl_u64	$d4, $d4, 2;
	add_u64	$d4, $d3, $d4;
	ld_global_align(4)_f32	$s11, [$d4];
	st_group_align(4)_f32	$s11, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s12+68];

@BB7_15:
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB7_17;
	// BB#16:
	add_u32	$s11, $s2, 1;
	mul_u32	$s9, $s11, $s9;
	add_u32	$s9, $s7, $s9;
	add_u32	$s9, $s9, 1;
	cvt_s64_s32	$d4, $s9;
	shl_u64	$d4, $d4, 2;
	add_u64	$d3, $d3, $d4;
	mul_u32	$s9, $s4, 17;
	ld_global_align(4)_f32	$s11, [$d3];
	shl_u32	$s9, $s9, 2;
	st_group_align(4)_f32	$s11, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s9+68];

@BB7_17:
	// %.thread.i
	add_u32	$s9, $s10, $s5;
	add_u32	$s5, $s6, $s5;
	cmp_lt_b1_s32	$c0, $s7, $s5;
	cmp_lt_b1_s32	$c1, $s2, $s9;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB7_19;
	// BB#18:
	lda_group_u32	$s2, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9];
	add_u32	$s2, $s2, $s8;
	add_u32	$s5, $s3, 1;
	mul_u32	$s5, $s5, $s4;
	add_u32	$s3, $s3, -1;
	mul_u32	$s3, $s3, $s4;
	add_u32	$s4, $s3, $s1;
	add_u32	$s6, $s1, -1;
	add_u32	$s7, $s1, 1;
	shl_u32	$s4, $s4, 2;
	add_u32	$s8, $s5, $s7;
	add_u32	$s9, $s3, $s6;
	add_u32	$s10, $s6, $s0;
	shl_u32	$s10, $s10, 2;
	ld_group_align(4)_f32	$s10, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s10];
	shl_u32	$s9, $s9, 2;
	add_u32	$s3, $s3, $s7;
	shl_u32	$s8, $s8, 2;
	ld_group_align(4)_f32	$s4, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s4];
	ld_group_align(4)_f32	$s9, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s9];
	ld_global_align(4)_f32	$s11, [$d0];
	ld_global_align(4)_f32	$s12, [$d0+4];
	mul_ftz_f32	$s10, $s12, $s10;
	mul_ftz_f32	$s9, $s11, $s9;
	ld_group_align(4)_f32	$s2, [$s2];
	ld_global_align(4)_f32	$s11, [$d0+16];
	ld_global_align(4)_f32	$s12, [$d0+12];
	mul_ftz_f32	$s4, $s12, $s4;
	ld_group_align(4)_f32	$s8, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s8];
	shl_u32	$s3, $s3, 2;
	add_u32	$s1, $s5, $s1;
	mul_ftz_f32	$s2, $s11, $s2;
	ld_global_align(4)_f32	$s11, [$d0+32];
	ld_global_align(4)_f32	$s12, [$d0+24];
	ld_global_align(4)_f32	$s13, [$d0+20];
	add_ftz_f32	$s9, $s9, $s10;
	add_u32	$s5, $s5, $s6;
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_f32	$s5, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s5];
	ld_global_align(4)_f32	$s6, [$d0+8];
	mul_ftz_f32	$s5, $s6, $s5;
	add_ftz_f32	$s5, $s9, $s5;
	add_u64	$d1, $d2, $d1;
	add_ftz_f32	$s4, $s5, $s4;
	add_ftz_f32	$s2, $s4, $s2;
	shl_u32	$s1, $s1, 2;
	ld_group_align(4)_f32	$s1, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s1];
	mul_ftz_f32	$s1, $s13, $s1;
	add_ftz_f32	$s1, $s2, $s1;
	ld_group_align(4)_f32	$s2, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s3];
	mul_ftz_f32	$s2, $s12, $s2;
	add_ftz_f32	$s1, $s1, $s2;
	mul_ftz_f32	$s2, $s11, $s8;
	add_u32	$s0, $s7, $s0;
	shl_u32	$s0, $s0, 2;
	ld_group_align(4)_f32	$s0, [%ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__119__cxxamp_trampolineEPfiiS4_iiS4_iiiii.ZZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3_EC__1clENS_11tiled_indexILi2EEEE5local9][$s0];
	ld_global_align(4)_f32	$s3, [$d0+28];
	mul_ftz_f32	$s0, $s3, $s0;
	add_ftz_f32	$s0, $s1, $s0;
	add_ftz_f32	$s0, $s0, $s2;
	st_global_align(4)_f32	$s0, [$d1];

@BB7_19:
	// %_ZZ14Stencil_Hcc_SmRN2hc5arrayIfLi1EEES2_S2_iiiENK3$_1clENS_11tiled_indexILi2EEE.exit
	ret;
};

prog kernel &ZZ11Stencil_HccRN2hc5arrayIfLi1EEES2_S2_iiiEN3_EC__019__cxxamp_trampolineEPfiiS4_iiS4_iiiii(
	kernarg_u64 %__arg_p0,
	kernarg_u32 %__arg_p1,
	kernarg_u32 %__arg_p2,
	kernarg_u64 %__arg_p3,
	kernarg_u32 %__arg_p4,
	kernarg_u32 %__arg_p5,
	kernarg_u64 %__arg_p6,
	kernarg_u32 %__arg_p7,
	kernarg_u32 %__arg_p8,
	kernarg_u32 %__arg_p9,
	kernarg_u32 %__arg_p10,
	kernarg_u32 %__arg_p11)
{
	// BB#0:
	ld_kernarg_align(4)_width(all)_u32	$s2, [%__arg_p9];
	ld_kernarg_align(4)_width(all)_u32	$s0, [%__arg_p10];
	add_u32	$s4, $s0, $s2;
	workitemabsid_u32	$s0, 1;
	add_u32	$s1, $s0, $s2;
	ld_kernarg_align(4)_width(all)_u32	$s3, [%__arg_p11];
	add_u32	$s5, $s3, $s2;
	workitemabsid_u32	$s0, 0;
	add_u32	$s0, $s0, $s2;
	cmp_lt_b1_s32	$c0, $s0, $s5;
	cmp_lt_b1_s32	$c1, $s1, $s4;
	and_b1	$c0, $c1, $c0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__arg_p6];
	cmp_ne_b1_b1	$c0, $c0, 1;
	ld_kernarg_align(8)_width(all)_u64	$d2, [%__arg_p3];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%__arg_p0];
	cbr_b1	$c0, @BB8_2;
	// BB#1:
	shl_u32	$s2, $s2, 1;
	add_u32	$s2, $s2, $s3;
	mul_u32	$s3, $s1, $s2;
	add_u32	$s4, $s1, 1;
	add_u32	$s5, $s3, $s0;
	mul_u32	$s4, $s4, $s2;
	add_u32	$s6, $s0, 1;
	add_u32	$s7, $s4, $s6;
	cvt_s64_s32	$d3, $s7;
	add_u32	$s1, $s1, -1;
	shl_u64	$d3, $d3, 2;
	mul_u32	$s1, $s1, $s2;
	add_u32	$s2, $s4, $s0;
	cvt_s64_s32	$d4, $s5;
	add_u32	$s5, $s0, -1;
	add_u32	$s7, $s5, $s3;
	cvt_s64_s32	$d5, $s7;
	shl_u64	$d5, $d5, 2;
	add_u64	$d5, $d0, $d5;
	shl_u64	$d4, $d4, 2;
	cvt_s64_s32	$d6, $s2;
	ld_global_align(4)_f32	$s2, [$d1];
	ld_global_align(4)_f32	$s7, [$d1+4];
	add_u32	$s0, $s1, $s0;
	add_u64	$d3, $d0, $d3;
	shl_u64	$d6, $d6, 2;
	ld_global_align(4)_f32	$s8, [$d1+20];
	ld_global_align(4)_f32	$s9, [$d1+16];
	ld_global_align(4)_f32	$s10, [$d1+12];
	ld_global_align(4)_f32	$s11, [$d1+8];
	ld_global_align(4)_f32	$s12, [$d1+24];
	add_u64	$d2, $d2, $d4;
	ld_global_align(4)_f32	$s13, [$d1+28];
	ld_global_align(4)_f32	$s14, [$d1+32];
	ld_global_align(4)_f32	$s15, [$d5];
	mul_ftz_f32	$s7, $s7, $s15;
	add_u32	$s15, $s1, $s5;
	cvt_s64_s32	$d1, $s15;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	ld_global_align(4)_f32	$s15, [$d1];
	mul_ftz_f32	$s2, $s2, $s15;
	add_ftz_f32	$s2, $s2, $s7;
	add_u32	$s4, $s4, $s5;
	cvt_s64_s32	$d1, $s4;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	ld_global_align(4)_f32	$s4, [$d1];
	mul_ftz_f32	$s4, $s11, $s4;
	add_ftz_f32	$s2, $s2, $s4;
	cvt_s64_s32	$d1, $s0;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	ld_global_align(4)_f32	$s0, [$d1];
	mul_ftz_f32	$s0, $s10, $s0;
	add_ftz_f32	$s0, $s2, $s0;
	add_u64	$d1, $d0, $d4;
	ld_global_align(4)_f32	$s2, [$d1];
	mul_ftz_f32	$s2, $s9, $s2;
	add_ftz_f32	$s0, $s0, $s2;
	add_u64	$d1, $d0, $d6;
	ld_global_align(4)_f32	$s2, [$d1];
	mul_ftz_f32	$s2, $s8, $s2;
	add_ftz_f32	$s0, $s0, $s2;
	ld_global_align(4)_f32	$s2, [$d3];
	add_u32	$s1, $s1, $s6;
	cvt_s64_s32	$d1, $s1;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	ld_global_align(4)_f32	$s1, [$d1];
	mul_ftz_f32	$s1, $s12, $s1;
	add_ftz_f32	$s0, $s0, $s1;
	mul_ftz_f32	$s1, $s14, $s2;
	add_u32	$s2, $s6, $s3;
	cvt_s64_s32	$d1, $s2;
	shl_u64	$d1, $d1, 2;
	add_u64	$d0, $d0, $d1;
	ld_global_align(4)_f32	$s2, [$d0];
	mul_ftz_f32	$s2, $s13, $s2;
	add_ftz_f32	$s0, $s0, $s2;
	add_ftz_f32	$s0, $s0, $s1;
	st_global_align(4)_f32	$s0, [$d2];

@BB8_2:
	// %_ZZ11Stencil_HccRN2hc5arrayIfLi1EEES2_S2_iiiENK3$_0clENS_11tiled_indexILi2EEE.exit
	ret;
};

function &amp_barrier()(arg_u32 %n)
{
	// BB#0:                                // %entry
	barrier;
	ret;
};
////////////////////////////////////////////////////////////
/// HSAIL builtin functions
////////////////////////////////////////////////////////////
/// get wavefront size

prog function &__wavesize(arg_u32 %dest)()
{
	st_arg_u32	WAVESIZE, [%dest];
	ret;
};
////////////////////////////////////////////////////////////
/// PRM 5.6 : Individual Bit Instructions
////////////////////////////////////////////////////////////
/// popcount_u32_b32

prog function &__popcount_u32_b32(arg_u32 %dest)(arg_u32 %src)
{
	ld_arg_u32	$s0, [%src];
	popcount_u32_b32	$s1, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
/// popcount_u32_b64

prog function &__popcount_u32_b64(arg_u32 %dest)(arg_u64 %src)
{
	ld_arg_u64	$d0, [%src];
	popcount_u32_b64	$s0, $d0;
	st_arg_u32	$s0, [%dest];
	ret;
};
////////////////////////////////////////////////////////////
/// PRM 5.7 : Bit String Instructions
////////////////////////////////////////////////////////////
////////////////////
/// bitextract
////////////////////
/// bitextract_u32

prog function &__bitextract_u32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	bitextract_u32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// bitextract_u64

prog function &__bitextract_u64(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	bitextract_u64	$d1, $d0, $s0, $s1;
	st_arg_u64	$d1, [%dest];
	ret;
};
/// bitextract_s32

prog function &__bitextract_s32(arg_s32 %dest)(
	arg_s32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	bitextract_s32	$s3, $s0, $s1, $s2;
	st_arg_s32	$s3, [%dest];
	ret;
};
/// bitextract_s64

prog function &__bitextract_s64(arg_s64 %dest)(
	arg_s64 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	bitextract_s64	$d1, $d0, $s0, $s1;
	st_arg_s64	$d1, [%dest];
	ret;
};
////////////////////
/// bitinsert
////////////////////
/// bitinsert_u32

prog function &__bitinsert_u32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2,
	arg_u32 %src3)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	ld_arg_u32	$s3, [%src3];
	bitinsert_u32	$s4, $s0, $s1, $s2, $s3;
	st_arg_u32	$s4, [%dest];
	ret;
};
/// bitinsert_u64

prog function &__bitinsert_u64(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1,
	arg_u32 %src2,
	arg_u32 %src3)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	ld_arg_u32	$s0, [%src2];
	ld_arg_u32	$s1, [%src3];
	bitinsert_u64	$d2, $d0, $d1, $s0, $s1;
	st_arg_u64	$d2, [%dest];
	ret;
};
/// bitinsert_s32

prog function &__bitinsert_s32(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1,
	arg_u32 %src2,
	arg_u32 %src3)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	ld_arg_u32	$s3, [%src3];
	bitinsert_u32	$s4, $s0, $s1, $s2, $s3;
	st_arg_s32	$s4, [%dest];
	ret;
};
/// bitinsert_s64

prog function &__bitinsert_s64(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1,
	arg_u32 %src2,
	arg_u32 %src3)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	ld_arg_u32	$s0, [%src2];
	ld_arg_u32	$s1, [%src3];
	bitinsert_s64	$d2, $d0, $d1, $s0, $s1;
	st_arg_s64	$d2, [%dest];
	ret;
};
////////////////////
/// bitmask
////////////////////
/// bitmask_b32

prog function &__bitmask_b32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	bitmask_b32	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
/// bitmask_b64

prog function &__bitmask_b64(arg_u64 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	bitmask_b64	$d0, $s0, $s1;
	st_arg_u64	$d0, [%dest];
	ret;
};
////////////////////
/// bitrev
////////////////////
/// bitrev_b32

prog function &__bitrev_b32(arg_u32 %dest)(arg_u32 %src0)
{
	ld_arg_u32	$s0, [%src0];
	bitrev_b32	$s1, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
/// bitrev_b64

prog function &__bitrev_b64(arg_u64 %dest)(arg_u64 %src0)
{
	ld_arg_u64	$d0, [%src0];
	bitrev_b64	$d1, $d0;
	st_arg_u64	$d1, [%dest];
	ret;
};
////////////////////
/// bitselect
////////////////////
/// bitselect_b32

prog function &__bitselect_b32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	bitselect_b32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// bitselect_b64

prog function &__bitselect_b64(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1,
	arg_u64 %src2)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	ld_arg_u64	$d2, [%src2];
	bitselect_b64	$d3, $d0, $d1, $d2;
	st_arg_u64	$d3, [%dest];
	ret;
};
////////////////////
/// firstbit
////////////////////
/// firstbit_u32_u32

prog function &__firstbit_u32_u32(arg_u32 %dest)(arg_u32 %src0)
{
	ld_arg_u32	$s0, [%src0];
	firstbit_u32_u32	$s1, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
/// firstbit_u32_u64

prog function &__firstbit_u32_u64(arg_u32 %dest)(arg_u64 %src0)
{
	ld_arg_u64	$d0, [%src0];
	firstbit_u32_u64	$s0, $d0;
	st_arg_u32	$s0, [%dest];
	ret;
};
/// firstbit_u32_s32

prog function &__firstbit_u32_s32(arg_u32 %dest)(arg_u32 %src0)
{
	ld_arg_s32	$s0, [%src0];
	firstbit_u32_s32	$s1, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
/// firstbit_u32_s64

prog function &__firstbit_u32_s64(arg_u32 %dest)(arg_u64 %src0)
{
	ld_arg_s64	$d0, [%src0];
	firstbit_u32_s64	$s0, $d0;
	st_arg_u32	$s0, [%dest];
	ret;
};
////////////////////
/// lastbit
////////////////////
/// lastbit_u32_u32

prog function &__lastbit_u32_u32(arg_u32 %dest)(arg_u32 %src0)
{
	ld_arg_u32	$s0, [%src0];
	lastbit_u32_u32	$s1, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
/// lastbit_u32_u64

prog function &__lastbit_u32_u64(arg_u32 %dest)(arg_u64 %src0)
{
	ld_arg_u64	$d0, [%src0];
	lastbit_u32_u64	$s0, $d0;
	st_arg_u32	$s0, [%dest];
	ret;
};
/// lastbit_u32_s32

prog function &__lastbit_u32_s32(arg_u32 %dest)(arg_u32 %src0)
{
	ld_arg_s32	$s0, [%src0];
	lastbit_u32_s32	$s1, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
/// lastbit_u32_s64

prog function &__lastbit_u32_s64(arg_u32 %dest)(arg_u64 %src0)
{
	ld_arg_s64	$d0, [%src0];
	lastbit_u32_s64	$s0, $d0;
	st_arg_u32	$s0, [%dest];
	ret;
};
////////////////////////////////////////////////////////////
/// PRM 5.9 : Packed Data Instructions
////////////////////////////////////////////////////////////
////////////////////
/// shuffle
////////////////////
/// NOTE: shuffle can not be implemented as of now because src2 operand must be a constant value
/// and it's not possible to do so as a library function.
/// shuffle_u8x4
/// shuffle_u8x8
/// shuffle_u16x2
/// shuffle_u16x4
/// shuffle_u32x2
/// shuffle_s8x4
/// shuffle_s8x8
/// shuffle_s16x2
/// shuffle_s16x4
/// shuffle_s32x2
/// shuffle_f8x4
/// shuffle_f8x8
/// shuffle_f16x2
/// shuffle_f16x4
/// shuffle_f32x2
////////////////////
/// unpacklo
////////////////////
// unpacklo_u8x4

prog function &__unpacklo_u8x4(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpacklo_u8x4	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
// unpacklo_u8x8

prog function &__unpacklo_u8x8(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	unpacklo_u8x8	$d2, $d0, $d1;
	st_arg_u64	$d2, [%dest];
	ret;
};
// unpacklo_u16x2

prog function &__unpacklo_u16x2(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpacklo_u16x2	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
// unpacklo_u16x4

prog function &__unpacklo_u16x4(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	unpacklo_u16x4	$d2, $d0, $d1;
	st_arg_u64	$d2, [%dest];
	ret;
};
// unpacklo_u32x2

prog function &__unpacklo_u32x2(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	unpacklo_u32x2	$d2, $d0, $d1;
	st_arg_u64	$d2, [%dest];
	ret;
};
// unpacklo_s8x4

prog function &__unpacklo_s8x4(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	unpacklo_s8x4	$s2, $s0, $s1;
	st_arg_s32	$s2, [%dest];
	ret;
};
// unpacklo_s8x8

prog function &__unpacklo_s8x8(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	unpacklo_s8x8	$d2, $d0, $d1;
	st_arg_s64	$d2, [%dest];
	ret;
};
// unpacklo_s16x2

prog function &__unpacklo_s16x2(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	unpacklo_s16x2	$s2, $s0, $s1;
	st_arg_s32	$s2, [%dest];
	ret;
};
// unpacklo_s16x4

prog function &__unpacklo_s16x4(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	unpacklo_s16x4	$d2, $d0, $d1;
	st_arg_s64	$d2, [%dest];
	ret;
};
// unpacklo_s32x2

prog function &__unpacklo_s32x2(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	unpacklo_s32x2	$d2, $d0, $d1;
	st_arg_s64	$d2, [%dest];
	ret;
};
////////////////////
/// unpackhi
////////////////////
// unpackhi_u8x4

prog function &__unpackhi_u8x4(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpackhi_u8x4	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
// unpackhi_u8x8

prog function &__unpackhi_u8x8(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	unpackhi_u8x8	$d2, $d0, $d1;
	st_arg_u64	$d2, [%dest];
	ret;
};
// unpackhi_u16x2

prog function &__unpackhi_u16x2(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpackhi_u16x2	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
// unpackhi_u16x4

prog function &__unpackhi_u16x4(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	unpackhi_u16x4	$d2, $d0, $d1;
	st_arg_u64	$d2, [%dest];
	ret;
};
// unpackhi_u32x2

prog function &__unpackhi_u32x2(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u64 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u64	$d1, [%src1];
	unpackhi_u32x2	$d2, $d0, $d1;
	st_arg_u64	$d2, [%dest];
	ret;
};
// unpackhi_s8x4

prog function &__unpackhi_s8x4(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	unpackhi_s8x4	$s2, $s0, $s1;
	st_arg_s32	$s2, [%dest];
	ret;
};
// unpackhi_s8x8

prog function &__unpackhi_s8x8(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	unpackhi_s8x8	$d2, $d0, $d1;
	st_arg_s64	$d2, [%dest];
	ret;
};
// unpackhi_s16x2

prog function &__unpackhi_s16x2(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	unpackhi_s16x2	$s2, $s0, $s1;
	st_arg_s32	$s2, [%dest];
	ret;
};
// unpackhi_s16x4

prog function &__unpackhi_s16x4(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	unpackhi_s16x4	$d2, $d0, $d1;
	st_arg_s64	$d2, [%dest];
	ret;
};
// unpackhi_s32x2

prog function &__unpackhi_s32x2(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s64 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s64	$d1, [%src1];
	unpackhi_s32x2	$d2, $d0, $d1;
	st_arg_s64	$d2, [%dest];
	ret;
};
////////////////////
/// pack
////////////////////
// pack_u8x4_u32

prog function &__pack_u8x4_u32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	pack_u8x4_u32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
// pack_u8x8_u32

prog function &__pack_u8x8_u32(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_u8x8_u32	$d1, $d0, $s0, $s1;
	st_arg_u64	$d1, [%dest];
	ret;
};
// pack_u8x16_u32 is not implemented as of now
// pack_u16x2_u32

prog function &__pack_u16x2_u32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	pack_u16x2_u32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
// pack_u16x4_u32

prog function &__pack_u16x4_u32(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_u16x4_u32	$d1, $d0, $s0, $s1;
	st_arg_u64	$d1, [%dest];
	ret;
};
// pack_u16x8_u32 is not implemented as of now
// pack_u32x2_u32

prog function &__pack_u32x2_u32(arg_u64 %dest)(
	arg_u64 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_u32x2_u32	$d1, $d0, $s0, $s1;
	st_arg_u64	$d1, [%dest];
	ret;
};
// pack_u32x4_u32 is not implemented as of now
// pack_u64x2_u32 is not implemented as of now
// pack_s8x4_s32

prog function &__pack_s8x4_s32(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1,
	arg_u32 %src2)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	pack_s8x4_s32	$s3, $s0, $s1, $s2;
	st_arg_s32	$s3, [%dest];
	ret;
};
// pack_s8x8_s32

prog function &__pack_s8x8_s32(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s32 %src1,
	arg_u32 %src2)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_s8x8_s32	$d1, $d0, $s0, $s1;
	st_arg_s64	$d1, [%dest];
	ret;
};
// pack_s8x16_s32 is not implemented as of now
// pack_s16x2_s32

prog function &__pack_s16x2_s32(arg_s32 %dest)(
	arg_s32 %src0,
	arg_s32 %src1,
	arg_u32 %src2)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_s32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	pack_s16x2_s32	$s3, $s0, $s1, $s2;
	st_arg_s32	$s3, [%dest];
	ret;
};
// pack_s16x4_s32

prog function &__pack_s16x4_s32(arg_u64 %dest)(
	arg_s64 %src0,
	arg_s32 %src1,
	arg_u32 %src2)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_s16x4_s32	$d1, $d0, $s0, $s1;
	st_arg_s64	$d1, [%dest];
	ret;
};
// pack_s16x8_s32 is not implemented as of now
// pack_s32x2_s32

prog function &__pack_s32x2_s32(arg_s64 %dest)(
	arg_s64 %src0,
	arg_s32 %src1,
	arg_u32 %src2)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_s32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_s32x2_s32	$d1, $d0, $s0, $s1;
	st_arg_s64	$d1, [%dest];
	ret;
};
// pack_s32x4_s32 is not implemented as of now
// pack_s64x2_s32 is not implemented as of now
// pack_f32x2_f32

prog function &__pack_f32x2_f32(arg_f64 %dest)(
	arg_f64 %src0,
	arg_f32 %src1,
	arg_u32 %src2)
{
	ld_arg_f64	$d0, [%src0];
	ld_arg_f32	$s0, [%src1];
	ld_arg_u32	$s1, [%src2];
	pack_f32x2_f32	$d1, $d0, $s0, $s1;
	st_arg_f64	$d1, [%dest];
	ret;
};
// pack_f32x4_f32 is not implemented as of now
// pack_f64x2_f32 is not implemented as of now
// pack_f16x2_f16 is not implemented as of now
// pack_f16x4_f16 is not implemented as of now
// pack_f16x8_f16 is not implemented as of now
////////////////////
/// unpack
////////////////////
// unpack_u32_u8x4

prog function &__unpack_u32_u8x4(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpack_u32_u8x4	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
// unpack_u32_u8x8

prog function &__unpack_u32_u8x8(arg_u32 %dest)(
	arg_u64 %src0,
	arg_u32 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_u32_u8x8	$s1, $d0, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
// unpack_u32_u8x16 is not implemented as of now
// unpack_u32_u16x2

prog function &__unpack_u32_u16x2(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpack_u32_u16x2	$s2, $s0, $s1;
	st_arg_u32	$s2, [%dest];
	ret;
};
// unpack_u32_u16x4

prog function &__unpack_u32_u16x4(arg_u32 %dest)(
	arg_u64 %src0,
	arg_u32 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_u32_u16x4	$s1, $d0, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
// unpack_u32_u16x8 is not implemented as of now
// unpack_u32_u32x2

prog function &__unpack_u32_u32x2(arg_u32 %dest)(
	arg_u64 %src0,
	arg_u32 %src1)
{
	ld_arg_u64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_u32_u32x2	$s1, $d0, $s0;
	st_arg_u32	$s1, [%dest];
	ret;
};
// unpack_u32_u32x4 is not implemented as of now
// unpack_u32_u64x2 is not implemented as of now
// unpack_s32_s8x4

prog function &__unpack_s32_s8x4(arg_s32 %dest)(
	arg_s32 %src0,
	arg_u32 %src1)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpack_s32_s8x4	$s2, $s0, $s1;
	st_arg_s32	$s2, [%dest];
	ret;
};
// unpack_s32_s8x8

prog function &__unpack_s32_s8x8(arg_s32 %dest)(
	arg_s64 %src0,
	arg_u32 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_s32_s8x8	$s1, $d0, $s0;
	st_arg_s32	$s1, [%dest];
	ret;
};
// unpack_s32_s8x16 is not implemented as of now
// unpack_s32_s16x2

prog function &__unpack_s32_s16x2(arg_s32 %dest)(
	arg_s32 %src0,
	arg_u32 %src1)
{
	ld_arg_s32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	unpack_s32_s16x2	$s2, $s0, $s1;
	st_arg_s32	$s2, [%dest];
	ret;
};
// unpack_s32_s16x4

prog function &__unpack_s32_s16x4(arg_s32 %dest)(
	arg_s64 %src0,
	arg_u32 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_s32_s16x4	$s1, $d0, $s0;
	st_arg_s32	$s1, [%dest];
	ret;
};
// unpack_s32_s16x8 is not implemented as of now
// unpack_s32_s32x2

prog function &__unpack_s32_s32x2(arg_s32 %dest)(
	arg_s64 %src0,
	arg_u32 %src1)
{
	ld_arg_s64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_s32_s32x2	$s1, $d0, $s0;
	st_arg_s32	$s1, [%dest];
	ret;
};
// unpack_s32_s32x4 is not implemented as of now
// unpack_s32_s64x2 is not implemented as of now
// unpack_f32_f32x2

prog function &__unpack_f32_f32x2(arg_f32 %dest)(
	arg_f64 %src0,
	arg_u32 %src1)
{
	ld_arg_f64	$d0, [%src0];
	ld_arg_u32	$s0, [%src1];
	unpack_f32_f32x2	$s1, $d0, $s0;
	st_arg_f32	$s1, [%dest];
	ret;
};
// unpack_f32_f32x4 is not implemented as of now
// unpack_f32_f64x2 is not implemented as of now
// unpack_f16_f16x2 is not implemented as of now
// unpack_f16_f16x4 is not implemented as of now
// unpack_f16_f16x8 is not implemented as of now
////////////////////////////////////////////////////////////
/// PRM 5.15 : Multimedia Instructions
////////////////////////////////////////////////////////////
/// bitalign_b32

prog function &__bitalign_b32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	bitalign_b32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// bytealign_b32

prog function &__bytealign_b32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	bytealign_b32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// lerp_u8x4

prog function &__lerp_u8x4(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	lerp_u8x4	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// packcvt_u8x4_f32

prog function &__packcvt_u8x4_f32(arg_u32 %dest)(
	arg_f32 %src0,
	arg_f32 %src1,
	arg_f32 %src2,
	arg_f32 %src3)
{
	ld_arg_f32	$s0, [%src0];
	ld_arg_f32	$s1, [%src1];
	ld_arg_f32	$s2, [%src2];
	ld_arg_f32	$s3, [%src3];
	packcvt_u8x4_f32	$s4, $s0, $s1, $s2, $s3;
	st_arg_u32	$s4, [%dest];
	ret;
};
/// unpackcvt_f32_u8x4
/// NOTE:
/// - if src1 is larger or equal to 3, than unpackcvt with src1 as 3 would be used

prog function &__unpackcvt_f32_u8x4(arg_f32 %dest)(
	arg_f32 %src0,
	arg_u32 %src1)
{
	ld_arg_f32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	cmp_eq_b1_u32	$c0, $s1, 0;
	cbr_b1	$c0, @unpackcvt_0;
	cmp_eq_b1_u32	$c0, $s1, 1;
	cbr_b1	$c0, @unpackcvt_1;
	cmp_eq_b1_u32	$c0, $s1, 2;
	cbr_b1	$c0, @unpackcvt_2;

@unpackcvt_3:
	unpackcvt_f32_u8x4	$s2, $s0, 3;
	br	@return;

@unpackcvt_0:
	unpackcvt_f32_u8x4	$s2, $s0, 0;
	br	@return;

@unpackcvt_1:
	unpackcvt_f32_u8x4	$s2, $s0, 1;
	br	@return;

@unpackcvt_2:
	unpackcvt_f32_u8x4	$s2, $s0, 2;

@return:
	st_arg_f32	$s2, [%dest];
	ret;
};
/// sad_u32_u32

prog function &__sad_u32_u32(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	sad_u32_u32	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// sad_u32_u16x2

prog function &__sad_u32_u16x2(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	sad_u32_u16x2	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// sad_u32_u8x4

prog function &__sad_u32_u8x4(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	sad_u32_u8x4	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// sadhi_u16x2_u8x4

prog function &__sadhi_u16x2_u8x4(arg_u32 %dest)(
	arg_u32 %src0,
	arg_u32 %src1,
	arg_u32 %src2)
{
	ld_arg_u32	$s0, [%src0];
	ld_arg_u32	$s1, [%src1];
	ld_arg_u32	$s2, [%src2];
	sadhi_u16x2_u8x4	$s3, $s0, $s1, $s2;
	st_arg_u32	$s3, [%dest];
	ret;
};
////////////////////////////////////////////////////////////
/// PRM 6.6 : Atomic Instructions
////////////////////////////////////////////////////////////
/// atomic_wrapinc (global)

prog function &__atomic_wrapinc_global(arg_u32 %old)(
	arg_u64 %addr,
	arg_u32 %val)
{
	ld_arg_u64	$d0, [%addr];
	ld_arg_u32	$s0, [%val];
	atomic_wrapinc_global_scar_system_u32	$s1, [$d0], $s0;
	st_arg_u32	$s1, [%old];
	ret;
};
/// atomic_wrapinc (local)

prog function &__atomic_wrapinc_local(arg_u32 %old)(
	arg_u32 %addr,
	arg_u32 %val)
{
	ld_arg_u32	$s0, [%addr];
	ld_arg_u32	$s1, [%val];
	atomic_wrapinc_group_scar_wg_u32	$s2, [$s0], $s1;
	st_arg_u32	$s2, [%old];
	ret;
};
/// atomic_wrapdec (global)

prog function &__atomic_wrapdec_global(arg_u32 %old)(
	arg_u64 %addr,
	arg_u32 %val)
{
	ld_arg_u64	$d0, [%addr];
	ld_arg_u32	$s0, [%val];
	atomic_wrapdec_global_scar_system_u32	$s1, [$d0], $s0;
	st_arg_u32	$s1, [%old];
	ret;
};
/// atomic_wrapdec (local)

prog function &__atomic_wrapdec_local(arg_u32 %old)(
	arg_u32 %addr,
	arg_u32 %val)
{
	ld_arg_u32	$s0, [%addr];
	ld_arg_u32	$s1, [%val];
	atomic_wrapdec_group_scar_wg_u32	$s2, [$s0], $s1;
	st_arg_u32	$s2, [%old];
	ret;
};
////////////////////////////////////////////////////////////
/// PRM 9.4 : Cross-Lane Instructions
////////////////////////////////////////////////////////////
/// activelanecount_width_u32_b1
/// NOTE:
/// - width is not in use as of now
/// - src is of type u32, and will be converted to b1

prog function &__activelanecount_u32_b1(arg_u32 %dest)(arg_u32 %src)
{
	ld_arg_u32	$s0, [%src];
	cmp_ne_b1_u32	$c0, $s0, 0;
	activelanecount_u32_b1	$s0, $c0;
	st_arg_u32	$s0, [%dest];
	ret;
};
/// activelaneid_width_u32
/// NOTE:
/// - width is not in use as of now

prog function &__activelaneid_u32(arg_u32 %dest)()
{
	activelaneid_u32	$s0;
	st_arg_u32	$s0, [%dest];
	ret;
};
/// activelanemask_v4_width_b64_b1
/// NOTE:
/// - width is not in use as of now
/// - only dest0 is returned as of now
/// - dest1, dest2, dest3 are not returned as of now
/// - input is of type u32, and will be converted to b1

prog function &__activelanemask_v4_b64_b1(arg_u64 %dest0)(arg_u32 %src)
{
	ld_arg_u32	$s0, [%src];
	cmp_ne_b1_u32	$c0, $s0, 0;
	activelanemask_v4_b64_b1	($d0, $d1, $d2, $d3), $c0;
	st_arg_u64	$d0, [%dest0];
	ret;
};
/// activelanepermute_width_b1 is not implemented as of now
/// activelanepermute_width_b128 is not implemented as of now
/// activelanepermute_width_b32
/// NOTE:
/// - width is not in use as of now
/// - useIdentity is of type u32, and will be converted to b1

prog function &__activelanepermute_b32(arg_u32 %dest)(
	arg_u32 %src,
	arg_u32 %laneId,
	arg_u32 %identity,
	arg_u32 %useIdentity)
{
	ld_arg_u32	$s0, [%src];
	ld_arg_u32	$s1, [%laneId];
	ld_arg_u32	$s2, [%identity];
	ld_arg_u32	$s3, [%useIdentity];
	cmp_ne_b1_u32	$c0, $s3, 0;
	activelanepermute_b32	$s3, $s0, $s1, $s2, $c0;
	st_arg_u32	$s3, [%dest];
	ret;
};
/// activelanepermute_width_b64
/// NOTE:
/// - width is not in use as of now
/// - useIdentity is of type b32, and will be converted to b1

prog function &__activelanepermute_b64(arg_u64 %dest)(
	arg_u64 %src,
	arg_u32 %laneId,
	arg_u64 %identity,
	arg_u32 %useIdentity)
{
	ld_arg_u64	$d0, [%src];
	ld_arg_u32	$s0, [%laneId];
	ld_arg_u64	$d1, [%identity];
	ld_arg_u32	$s1, [%useIdentity];
	cmp_ne_b1_u32	$c0, $s1, 0;
	activelanepermute_b64	$d2, $d0, $s0, $d1, $c0;
	st_arg_u64	$d2, [%dest];
	ret;
};
////////////////////////////////////////////////////////////
/// PRM 11.4 : Miscellaneous Instructions
////////////////////////////////////////////////////////////
/// get system timestamp

prog function &__clock_u64(arg_u64 %dest)()
{
	clock_u64	$d0;
	st_arg_u64	$d0, [%dest];
	ret;
};
/// get hardware cycle count
/// NOTE:
/// - There is no corresponding HSAIL instruction so we always return 0 here

prog function &__cycle_u64(arg_u64 %dest)()
{
	st_arg_u64	0, [%dest];
	ret;
};
////////////////////////////////////////////////////////////
/// Dynamic group segment
////////////////////////////////////////////////////////////
/// global variable to store the size of static group segment
/// the value would be set by Kalmar runtime prior to kernel dispatch
prog global_u32 &hcc_static_group_segment_size = 0;
/// global variable to store the size of dynamic group segment
/// the value would be set by Kalmar runtime prior to kernel dispatch
prog global_u32 &hcc_dynamic_group_segment_size = 0;
/// get_static_group_segment_size : return the size of static group segment

prog function &get_static_group_segment_size(arg_u32 %ret)()
{
	ld_global_u32	$s0, [&hcc_static_group_segment_size];
	st_arg_u32	$s0, [%ret];
	ret;
};
/// get_dynamic_group_segment_size : return the size of dynamic group segment

prog function &get_dynamic_group_segment_size(arg_u32 %ret)()
{
	ld_global_u32	$s0, [&hcc_dynamic_group_segment_size];
	st_arg_u32	$s0, [%ret];
	ret;
};
/// get_group_segment_addr : get arbitrary address location within group segment
///
/// This function would return an arbitrary location within group segment.
/// It takes an u32 offset argument and returns the given address in the
/// group segment.  The base of group segment is fetched with groupbaseptr_u32.
///

prog function &get_group_segment_addr(arg_u32 %ret)(arg_u32 %offset)
{
	ld_arg_u32	$s1, [%offset];
	groupbaseptr_u32	$s0;
	add_u32	$s0, $s0, $s1;
	st_arg_u32	$s0, [%ret];
	ret;
};
/// get_dynamic_group_segment : get the pointer to the beginning of dynamic
/// group segment
///

prog function &get_dynamic_group_segment(arg_u32 %ret)()
{
	// call get_static_group_segment_size
	{
		arg_u32 %res1;
		call	&get_static_group_segment_size (%res1) ();
		ld_arg_u32	$s0, [%res1];
	}
	// call get_group_segment_addr
	{
		arg_u32 %offset;
		arg_u32 %res2;
		// fill in the argument
		st_arg_u32	$s0, [%offset];
		call	&get_group_segment_addr (%res2) (%offset);
		ld_arg_u32	$s1, [%res2];
	}
	st_arg_u32	$s1, [%ret];
	ret;
};
////////////////////////////////////////////////////////////
/// New / delete within kernels
////////////////////////////////////////////////////////////
prog global_u64 &signal_Xmalloc = 0;
prog global_u64 &signal_malloc = 0;
prog global_u64 &ptr_a_address = 0;
prog global_u64 &ptr_b_address = 0;
prog global_u64 &ptr_c_address = 0;
prog global_u64 &ptr_x_address = 0;
prog global_u64 &ptr_y_address = 0;
prog global_u64 &ptr_z_address = 0;

prog function &_Z14putXmallocFlagm()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&signal_Xmalloc];
	ret;
};

prog function &_Z13putMallocFlagm()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&signal_malloc];
	ret;
};

prog function &_Z9put_ptr_aPv2()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&ptr_a_address];
	ret;
};

prog function &_Z9put_ptr_bPv3()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&ptr_b_address];
	ret;
};

prog function &_Z9put_ptr_cPv4()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&ptr_c_address];
	ret;
};

prog function &_Z9put_ptr_xPv5()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&ptr_x_address];
	ret;
};

prog function &_Z9put_ptr_yPv6()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&ptr_y_address];
	ret;
};

prog function &_Z9put_ptr_zPv7()(arg_u64 %arg_p0)
{
	ld_arg_u64	$d0, [%arg_p0];
	st_global_u64	$d0, [&ptr_z_address];
	ret;
};
//  Implementation of Xfree/free, observe the hsail and extract it.  
//
//  parallel_for_each(
//    Concurrency::extent<1>(vecSize).tile<tileSize>(),
//    [=](Concurrency::tiled_index<tileSize> tidx) restrict(amp) {
//
//    int global = tidx.global[0];
//    int local = tidx.local[0];
//    int tile = tidx.tile[0];
//
//    // store the parameter
//    (ptr_y + global)->store(address, std::memory_order_release);
//
//    // store the signal value
//    (ptr_x + global)->store(2, std::memory_order_release);
//
//    // wait until syscall returns
//    while ((ptr_x + global)->load(std::memory_order_acquire));
//  });
//
//
// Xfree/free

prog function &_ZdlPv()(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1PNSt3__16atomicIlEElPNS1_IiEE_entry:
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_add_screl_s64_sig64	$d10, 1;
	// BB#0:
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d0, $d0, $d1;
	shl_u64	$d0, $d0, 32;
	shr_s64	$d1, $d0, 32;
	shl_u64	$d0, $d1, 2;
	ld_global_u64	$d2, [&ptr_x_address];
	add_u64	$d0, $d2, $d0;
	shl_u64	$d1, $d1, 3;
	ld_global_u64	$d2, [&ptr_y_address];
	add_u64	$d1, $d2, $d1;
	mov_b32	$s0, 2;
	ld_arg_u64	$d2, [%arg_p0];
	atomicnoret_st_global_screl_system_b64	[$d1], $d2;
	atomicnoret_st_global_screl_system_b32	[$d0], $s0;

@BB0_1:
	atomic_ld_global_scacq_system_b32	$s0, [$d0];
	cmp_ne_b1_s32	$c0, $s0, 0;
	cbr_b1	$c0, @BB0_1;
	// BB#2:                                // %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_sub_screl_s64_sig64	$d10, 1;
	ret;
};

prog function &_ZdaPv()(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1PNSt3__16atomicIlEElPNS1_IiEE_entry:
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_add_screl_s64_sig64	$d10, 1;
	// BB#0:
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d0, $d0, $d1;
	shl_u64	$d0, $d0, 32;
	shr_s64	$d1, $d0, 32;
	shl_u64	$d0, $d1, 2;
	ld_global_u64	$d2, [&ptr_x_address];
	add_u64	$d0, $d2, $d0;
	shl_u64	$d1, $d1, 3;
	ld_global_u64	$d2, [&ptr_y_address];
	add_u64	$d1, $d2, $d1;
	mov_b32	$s0, 2;
	ld_arg_u64	$d2, [%arg_p0];
	atomicnoret_st_global_screl_system_b64	[$d1], $d2;
	atomicnoret_st_global_screl_system_b32	[$d0], $s0;

@BB0_1:
	atomic_ld_global_scacq_system_b32	$s0, [$d0];
	cmp_ne_b1_s32	$c0, $s0, 0;
	cbr_b1	$c0, @BB0_1;
	// BB#2:                                // %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_sub_screl_s64_sig64	$d10, 1;
	ret;
};
//  Implementation of malloc, observe the hsail and extract it.
//
//  parallel_for_each(
//    Concurrency::extent<1>(vecSize).tile<tileSize>(),
//    [=](Concurrency::tiled_index<tileSize> tidx) restrict(amp) {
//
//    int global = tidx.global[0];
//    int local = tidx.local[0];
//    int tile = tidx.tile[0];
//
//    (ptr_y + global)->store(n, std::memory_order_release);
//
//    (ptr_x + global)->store(1, std::memory_order_release);
//
//    while ((ptr_x + global)->load(std::memory_order_acquire));
//
//    // load result from CPU
//    long result = (ptr_y + global)->load(std::memory_order_acquire);
//
//    // test access the memory allocated
//    int *p_counter = (int *)result;
//    *p_counter = 1;
//    int header_offset = sizeof(int);
//    int *p_header = (int *)((char *)p_counter + header_offset);
//    *p_header = header_offset;
//    char *alloc = (char *)(p_header + 1);
//
//    long address = (long)alloc;
// 
//    // store result
//    (ptr_z + global)->store(address, std::memory_order_release);
//  });
//
//
// malloc version

prog function &_Znwm_malloc(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__119__cxxamp_trampolineE_0lPNSt3__16atomicIlEEPNS1_IiEES3__entry:
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_add_screl_s64_sig64	$d10, 1;
	// BB#0:
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d0, $d0, $d1;
	shl_u64	$d0, $d0, 32;
	shr_s64	$d1, $d0, 32;
	shl_u64	$d0, $d1, 3;
	shl_u64	$d1, $d1, 2;
	ld_arg_u64	$d2, [%arg_p0];
	shl_u64	$d4, $d2, 32;
	ld_global_u64	$d2, [&ptr_y_address];
	ld_global_u64	$d3, [&ptr_x_address];
	add_u64	$d3, $d3, $d1;
	add_u64	$d2, $d2, $d0;
	add_u64	$d1, $d4, 0x800000000;
	shr_s64	$d4, $d1, 32;
	ld_global_u64	$d1, [&ptr_z_address];
	mov_b32	$s0, 1;
	atomicnoret_st_global_screl_system_b64	[$d2], $d4;
	atomicnoret_st_global_screl_system_b32	[$d3], $s0;

@BB0_1:
	atomic_ld_global_scacq_system_b32	$s1, [$d3];
	cmp_ne_b1_s32	$c0, $s1, 0;
	cbr_b1	$c0, @BB0_1;
	// BB#2:                                // %_ZZ4mainENK3$_1clE_0N11Concurrency11tiled_indexILi1ELi0ELi0EEE.exit
	atomic_ld_global_scacq_system_b64	$d2, [$d2];
	st_global_align(4)_u32	$s0, [$d2];
	mov_b32	$s0, 4;
	st_global_align(4)_u32	$s0, [$d2+4];
	add_u64	$d0, $d1, $d0;
	add_u64	$d1, $d2, 8;
	// atomicnoret_st_global_screl_system_b64       [$d0], $d1;
	st_arg_u64	$d1, [%ret_r0];
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_sub_screl_s64_sig64	$d10, 1;
	ret;
};

prog function &_Znam_malloc(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__119__cxxamp_trampolineE_0lPNSt3__16atomicIlEEPNS1_IiEES3__entry:
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_add_screl_s64_sig64	$d10, 1;
	// BB#0:
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d0, $d0, $d1;
	shl_u64	$d0, $d0, 32;
	shr_s64	$d1, $d0, 32;
	shl_u64	$d0, $d1, 3;
	shl_u64	$d1, $d1, 2;
	ld_arg_u64	$d2, [%arg_p0];
	shl_u64	$d4, $d2, 32;
	ld_global_u64	$d2, [&ptr_y_address];
	ld_global_u64	$d3, [&ptr_x_address];
	add_u64	$d3, $d3, $d1;
	add_u64	$d2, $d2, $d0;
	add_u64	$d1, $d4, 0x800000000;
	shr_s64	$d4, $d1, 32;
	ld_global_u64	$d1, [&ptr_z_address];
	mov_b32	$s0, 1;
	atomicnoret_st_global_screl_system_b64	[$d2], $d4;
	atomicnoret_st_global_screl_system_b32	[$d3], $s0;

@BB0_1:
	atomic_ld_global_scacq_system_b32	$s1, [$d3];
	cmp_ne_b1_s32	$c0, $s1, 0;
	cbr_b1	$c0, @BB0_1;
	// BB#2:                                // %_ZZ4mainENK3$_1clE_0N11Concurrency11tiled_indexILi1ELi0ELi0EEE.exit
	atomic_ld_global_scacq_system_b64	$d2, [$d2];
	st_global_align(4)_u32	$s0, [$d2];
	mov_b32	$s0, 4;
	st_global_align(4)_u32	$s0, [$d2+4];
	add_u64	$d0, $d1, $d0;
	add_u64	$d1, $d2, 8;
	// atomicnoret_st_global_screl_system_b64       [$d0], $d1;
	st_arg_u64	$d1, [%ret_r0];
	ld_global_u64	$d10, [&signal_malloc];
	signalnoret_sub_screl_s64_sig64	$d10, 1;
	ret;
};
//  Implementation of Xmalloc, observe the hsail and extract it.
//
//  parallel_for_each(
//    Concurrency::extent<1>(vecSize).tile<tileSize>(),
//    [=](Concurrency::tiled_index<tileSize> tidx) restrict(amp) {
//
//    #define MAX_TILE_SIZE 256
//
//    int global = tidx.global[0];
//    int local = tidx.local[0];
//    int tile = tidx.tile[0];
//
//    tile_static long g_idata[MAX_TILE_SIZE * 2]; // g_idata only
//    g_idata[local] = n; // g_idata only
//
//    tile_static long temp[MAX_TILE_SIZE * 2];
//    int offset = 1;
//
//    tidx.barrier.wait();
//
//    temp[2 * local] = g_idata[2 * local];
//    temp[2 * local + 1] = g_idata[2 * local + 1];
//
//    for (int d = currentworkgroupsize >> 1; d > 0; d >>= 1) // build
//    {
//      tidx.barrier.wait();
//
//      if (local < d)
//      {
//        int ai = offset * (2 * local + 1) - 1;
//        int bi = offset * (2 * local + 2) - 1;
//
//        temp[bi] += temp[ai];
//      }
//      offset *= 2;
//    }
//
//    if (local == 0) { temp[currentworkgroupsize - 1] = 0; }
//
//    for (int d = 1; d < currentworkgroupsize; d *= 2)
//    {
//      offset >>= 1;
//      tidx.barrier.wait();
//
//      if (local < d)
//      {
//        int ai = offset * (2 * local + 1) - 1;
//        int bi = offset * (2 * local + 2) - 1;
//
//        long t = temp[ai];
//        temp[ai] = temp[bi];
//        temp[bi] += t;
//      }
//    }
//
//    tidx.barrier.wait();
//
//    if (local == 0) {
//      long amount = temp[currentworkgroupsize - 1] + g_idata[currentworkgroupsize - 1] + sizeof(int) * currentworkgroupsize + sizeof(int);
//
//      (ptr_b + tile)->store(amount, std::memory_order_release);
//
//      (ptr_a + tile)->store(1, std::memory_order_release);
//
//      while ((ptr_a + tile)->load(std::memory_order_acquire));
//    }
//
//    tidx.barrier.wait();
//
//    // load result from CPU
//    long result = (ptr_b + tile)->load(std::memory_order_acquire);
//
//    // test access the memory allocated
//    int *p_counter = (int *) result;
//
//    if (local == 0) {
//      *p_counter = currentworkgroupsize;
//    }
//
//    int header_offset = (int)temp[local] + sizeof(int) * local + sizeof(int);
//    int *p_header = (int *)((char *)p_counter + header_offset);
//    *p_header = header_offset;
//    char *alloc = (char *)(p_header + 1);
//    long address = (long)alloc;
//
//    // store result
//    (ptr_c + global)->store(address, std::memory_order_release);
//  });
//
//
// Xmalloc version

prog function &_Znwm(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{
	align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12[512];
	align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13[512];

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__entry:
	ld_global_u64	$d10, [&signal_Xmalloc];
	signalnoret_add_screl_s64_sig64	$d10, 1;
	// BB#0:
	workitemid_u32	$s2, 0;
	shl_u32	$s0, $s2, 3;
	ld_arg_u64	$d0, [%arg_p0];
	st_group_align(8)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s0];
	barrier;
	shl_u32	$s6, $s2, 1;
	or_b32	$s3, $s6, 1;
	shl_u32	$s1, $s6, 3;
	ld_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s1];
	st_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s1];
	shl_u32	$s5, $s3, 3;
	workitemabsid_u32	$s4, 0;
	ld_global_u64	$d0, [&ptr_c_address];
	ld_global_u64	$d2, [&ptr_a_address];
	ld_global_u64	$d1, [&ptr_b_address];
	currentworkgroupsize_u32	$s1, 0;
	ld_kernarg_align(8)_width(all)_u64	$d3, [0];
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s5];
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s5];
	shr_s32	$s7, $s1, 1;
	cmp_lt_b1_s32	$c0, $s7, 1;
	cbr_b1	$c0, @BB0_1;
	// BB#2:                                // %.lr.ph11.i
	add_u32	$s8, $s6, 2;
	mov_b32	$s5, 1;

@BB0_3:
	barrier;
	cmp_ge_b1_s32	$c0, $s2, $s7;
	cbr_b1	$c0, @BB0_5;
	// BB#4:
	mul_u32	$s9, $s5, $s3;
	shl_u32	$s9, $s9, 3;
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	mul_u32	$s9, $s5, $s8;
	shl_u32	$s9, $s9, 3;
	ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	add_u64	$d4, $d5, $d4;
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_5:
	shl_u32	$s5, $s5, 1;
	shr_s32	$s7, $s7, 1;
	cmp_gt_b1_s32	$c0, $s7, 0;
	cbr_b1	$c0, @BB0_3;
	br	@BB0_6;

@BB0_1:
	mov_b32	$s5, 1;

@BB0_6:
	// %._crit_edge12.i
	cmp_ne_b1_s32	$c0, $s2, 0;
	cbr_b1	$c0, @BB0_8;
	// BB#7:
	shl_u32	$s7, $s1, 3;
	mov_b64	$d4, 0;
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s7-8];

@BB0_8:
	// %.preheader.i
	cmp_lt_b1_s32	$c1, $s1, 2;
	cbr_b1	$c1, @BB0_13;
	// BB#9:                                // %.lr.ph.i
	add_u32	$s6, $s6, 2;
	mov_b32	$s7, 1;

@BB0_10:
	barrier;
	shr_s32	$s5, $s5, 1;
	cmp_ge_b1_s32	$c1, $s2, $s7;
	cbr_b1	$c1, @BB0_12;
	// BB#11:
	mul_u32	$s8, $s5, $s3;
	shl_u32	$s8, $s8, 3;
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
	mul_u32	$s9, $s5, $s6;
	shl_u32	$s9, $s9, 3;
	ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	st_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
	ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	add_u64	$d4, $d5, $d4;
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_12:
	shl_u32	$s7, $s7, 1;
	cmp_lt_b1_s32	$c1, $s7, $s1;
	cbr_b1	$c1, @BB0_10;

@BB0_13:
	// %._crit_edge.i
	cvt_u64_u32	$d4, $s4;
	workgroupid_u32	$s3, 0;
	add_u64	$d3, $d4, $d3;
	pack_u32x2_u32	$d4, u32x2(0,0), $s2, 1;
	barrier;
	cmp_eq_b1_s32	$c1, $s2, 0;
	cbr_b1	$c1, @BB0_15;
	// BB#14:                                // %._crit_edge13.i
	cvt_s64_s32	$d5, $s3;
	br	@BB0_17;

@BB0_15:
	add_u32	$s2, $s1, -1;
	shl_u32	$s2, $s2, 3;
	cvt_s64_s32	$d5, $s1;
	shl_u64	$d6, $d5, 2;
	ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s2];
	cvt_s64_s32	$d5, $s3;
	shl_u64	$d8, $d5, 3;
	shl_u64	$d9, $d5, 2;
	add_u64	$d2, $d2, $d9;
	add_u64	$d8, $d1, $d8;
	add_u64	$d6, $d6, $d7;
	ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s2];
	add_u64	$d6, $d6, $d7;
	mov_b32	$s2, 1;
	add_u64	$d6, $d6, 4;
	atomicnoret_st_global_screl_system_b64	[$d8], $d6;
	atomicnoret_st_global_screl_system_b32	[$d2], $s2;

@BB0_16:
	atomic_ld_global_scacq_system_b32	$s2, [$d2];
	cmp_ne_b1_s32	$c1, $s2, 0;
	cbr_b1	$c1, @BB0_16;

@BB0_17:
	// %.loopexit.i
	shl_u64	$d2, $d5, 3;
	add_u64	$d1, $d1, $d2;
	barrier;
	atomic_ld_global_scacq_system_b64	$d1, [$d1];
	cbr_b1	$c0, @BB0_19;
	// BB#18:
	st_global_align(4)_u32	$s1, [$d1];

@BB0_19:
	// %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit
	shr_s64	$d2, $d4, 30;
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s0];
	and_b64	$d4, $d4, 0xffffffff;
	add_u64	$d2, $d2, $d4;
	add_u64	$d2, $d2, 4;
	cvt_u32_u64	$s0, $d2;
	shl_u64	$d4, $d2, 32;
	shl_u64	$d2, $d3, 32;
	shr_s64	$d2, $d2, 32;
	shr_s64	$d3, $d4, 32;
	add_u64	$d1, $d1, $d3;
	st_global_align(4)_u32	$s0, [$d1];
	shl_u64	$d2, $d2, 3;
	add_u64	$d0, $d0, $d2;
	add_u64	$d1, $d1, 4;
	// atomicnoret_st_global_screl_system_b64   [$d0], $d1;
	st_arg_u64	$d1, [%ret_r0];
	ld_global_u64	$d10, [&signal_Xmalloc];
	signalnoret_sub_screl_s64_sig64	$d10, 1;
	ret;
};

prog function &_Znam(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{
	align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12[512];
	align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13[512];

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__entry:
	ld_global_u64	$d10, [&signal_Xmalloc];
	signalnoret_add_screl_s64_sig64	$d10, 1;
	// BB#0:
	workitemid_u32	$s2, 0;
	shl_u32	$s0, $s2, 3;
	ld_arg_u64	$d0, [%arg_p0];
	st_group_align(8)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s0];
	barrier;
	shl_u32	$s6, $s2, 1;
	or_b32	$s3, $s6, 1;
	shl_u32	$s1, $s6, 3;
	ld_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s1];
	st_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s1];
	shl_u32	$s5, $s3, 3;
	workitemabsid_u32	$s4, 0;
	ld_global_u64	$d0, [&ptr_c_address];
	ld_global_u64	$d2, [&ptr_a_address];
	ld_global_u64	$d1, [&ptr_b_address];
	currentworkgroupsize_u32	$s1, 0;
	ld_kernarg_align(8)_width(all)_u64	$d3, [0];
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s5];
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s5];
	shr_s32	$s7, $s1, 1;
	cmp_lt_b1_s32	$c0, $s7, 1;
	cbr_b1	$c0, @BB0_1;
	// BB#2:                                // %.lr.ph11.i
	add_u32	$s8, $s6, 2;
	mov_b32	$s5, 1;

@BB0_3:
	barrier;
	cmp_ge_b1_s32	$c0, $s2, $s7;
	cbr_b1	$c0, @BB0_5;
	// BB#4:
	mul_u32	$s9, $s5, $s3;
	shl_u32	$s9, $s9, 3;
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	mul_u32	$s9, $s5, $s8;
	shl_u32	$s9, $s9, 3;
	ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	add_u64	$d4, $d5, $d4;
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_5:
	shl_u32	$s5, $s5, 1;
	shr_s32	$s7, $s7, 1;
	cmp_gt_b1_s32	$c0, $s7, 0;
	cbr_b1	$c0, @BB0_3;
	br	@BB0_6;

@BB0_1:
	mov_b32	$s5, 1;

@BB0_6:
	// %._crit_edge12.i
	cmp_ne_b1_s32	$c0, $s2, 0;
	cbr_b1	$c0, @BB0_8;
	// BB#7:
	shl_u32	$s7, $s1, 3;
	mov_b64	$d4, 0;
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s7-8];

@BB0_8:
	// %.preheader.i
	cmp_lt_b1_s32	$c1, $s1, 2;
	cbr_b1	$c1, @BB0_13;
	// BB#9:                                // %.lr.ph.i
	add_u32	$s6, $s6, 2;
	mov_b32	$s7, 1;

@BB0_10:
	barrier;
	shr_s32	$s5, $s5, 1;
	cmp_ge_b1_s32	$c1, $s2, $s7;
	cbr_b1	$c1, @BB0_12;
	// BB#11:
	mul_u32	$s8, $s5, $s3;
	shl_u32	$s8, $s8, 3;
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
	mul_u32	$s9, $s5, $s6;
	shl_u32	$s9, $s9, 3;
	ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	st_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
	ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
	add_u64	$d4, $d5, $d4;
	st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_12:
	shl_u32	$s7, $s7, 1;
	cmp_lt_b1_s32	$c1, $s7, $s1;
	cbr_b1	$c1, @BB0_10;

@BB0_13:
	// %._crit_edge.i
	cvt_u64_u32	$d4, $s4;
	workgroupid_u32	$s3, 0;
	add_u64	$d3, $d4, $d3;
	pack_u32x2_u32	$d4, u32x2(0,0), $s2, 1;
	barrier;
	cmp_eq_b1_s32	$c1, $s2, 0;
	cbr_b1	$c1, @BB0_15;
	// BB#14:                                // %._crit_edge13.i
	cvt_s64_s32	$d5, $s3;
	br	@BB0_17;

@BB0_15:
	add_u32	$s2, $s1, -1;
	shl_u32	$s2, $s2, 3;
	cvt_s64_s32	$d5, $s1;
	shl_u64	$d6, $d5, 2;
	ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s2];
	cvt_s64_s32	$d5, $s3;
	shl_u64	$d8, $d5, 3;
	shl_u64	$d9, $d5, 2;
	add_u64	$d2, $d2, $d9;
	add_u64	$d8, $d1, $d8;
	add_u64	$d6, $d6, $d7;
	ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s2];
	add_u64	$d6, $d6, $d7;
	mov_b32	$s2, 1;
	add_u64	$d6, $d6, 4;
	atomicnoret_st_global_screl_system_b64	[$d8], $d6;
	atomicnoret_st_global_screl_system_b32	[$d2], $s2;

@BB0_16:
	atomic_ld_global_scacq_system_b32	$s2, [$d2];
	cmp_ne_b1_s32	$c1, $s2, 0;
	cbr_b1	$c1, @BB0_16;

@BB0_17:
	// %.loopexit.i
	shl_u64	$d2, $d5, 3;
	add_u64	$d1, $d1, $d2;
	barrier;
	atomic_ld_global_scacq_system_b64	$d1, [$d1];
	cbr_b1	$c0, @BB0_19;
	// BB#18:
	st_global_align(4)_u32	$s1, [$d1];

@BB0_19:
	// %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit
	shr_s64	$d2, $d4, 30;
	ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s0];
	and_b64	$d4, $d4, 0xffffffff;
	add_u64	$d2, $d2, $d4;
	add_u64	$d2, $d2, 4;
	cvt_u32_u64	$s0, $d2;
	shl_u64	$d4, $d2, 32;
	shl_u64	$d2, $d3, 32;
	shr_s64	$d2, $d2, 32;
	shr_s64	$d3, $d4, 32;
	add_u64	$d1, $d1, $d3;
	st_global_align(4)_u32	$s0, [$d1];
	shl_u64	$d2, $d2, 3;
	add_u64	$d0, $d0, $d2;
	add_u64	$d1, $d1, 4;
	// atomicnoret_st_global_screl_system_b64.[$d0], $d1;
	st_arg_u64	$d1, [%ret_r0];
	ld_global_u64	$d10, [&signal_Xmalloc];
	signalnoret_sub_screl_s64_sig64	$d10, 1;
	ret;
};
